{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd7fae65-c8a9-4633-94d6-8de7ec8997ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from datetime import datetime\n",
    "import sys, os\n",
    "import subprocess\n",
    "\n",
    "from astropy.coordinates import SkyCoord\n",
    "from ctapipe.io import read_table\n",
    "import tables\n",
    "\n",
    "# Other auxiliar scripts\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"../scripts/\"))\n",
    "import geometry as geom\n",
    "import lstpipeline\n",
    "import script_utils_scaling as utils\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2bac8d7-7058-41ae-a7aa-174b0861aca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Source name in order to just complete the results file, and\n",
    "in order to improve run organization.\"\"\"\n",
    "source_name = \"crab\"\n",
    "\n",
    "\"\"\" Fit parameters\n",
    "Chosen limits in intensity (p.e.) for applying the fit i.e. the\n",
    "power law will be fitted only with the points within this range.\"\"\"\n",
    "limits_intensity = [316, 562]\n",
    "\"\"\" For the positive scaling cases (most of them), we need to have a lower \n",
    "limit in intensity. Thi slimit is used for the subset of events that are \n",
    "scaled just to find which is the scaling value. We use a very low limit by\n",
    "default 60 p.e. compared to the lower limit of the fit 316 p.e. because in \n",
    "the worst cases we will have a very non-linear scaling that will displace \n",
    "significantly the events intensities.\"\"\"\n",
    "limits_intensity_extended = 60\n",
    "\n",
    "\"\"\" Power law parameters for the reference\n",
    "All these parameters are taken from a common analysis of the full dataset\n",
    "Where the period of end of 2022 and start 2023 is taken as reference for good \n",
    "runs. Then we take as reference the mean power law parameters in that period.\n",
    "p0 is the normalization factor and p1 is the slope.\"\"\"\n",
    "ref_p0 =  1.74 \n",
    "ref_p1 = -2.23\n",
    "\n",
    "\"\"\" Threshold in statistics for the last subrun\n",
    "The limit in number of events after cleaning that we need to consider the last\n",
    "subrun has enough statistics to perform the analysis over it. Otherwise the \n",
    "values of the scaling that will be applied to this last rubrun are the same \n",
    "that are applied to the last last subrun.\"\"\"\n",
    "statistics_threshold = 10000\n",
    "\n",
    "\"\"\" Parameters for the empyrical fits for Zenith Distance corrections\n",
    "Are simply two 2 degree polynomials for each variable of the power law.\"\"\"\n",
    "p0a, p0b, p0c = -0.44751321, 3.62502037, -1.43611437\n",
    "p1a, p1b, p1c = -2.89253919, 0.99443581, -0.34013068\n",
    "\n",
    "# Standard paths for data in the IT cluster ---------\n",
    "root_dl1 = \"/fefs/aswg/data/real/DL1/*/v0.*/tailcut84/\"\n",
    "root_rfs = \"/fefs/aswg/data/models/AllSky/20230927_v0.10.4_crab_tuned/\"\n",
    "root_mcs = \"/fefs/aswg/data/mc/DL2/AllSky/20230927_v0.10.4_crab_tuned/TestingDataset/\"\n",
    "\n",
    "# Root path of this script\n",
    "root = os.getcwd()\n",
    "# Path to store the configuration file we are going to use\n",
    "config_file = os.path.join(root, \"config/standard_config.json\")\n",
    "# Path to store objects\n",
    "root_objects = os.path.join(root, f\"objects/\")\n",
    "# Data main directory\n",
    "root_data = os.path.join(root, f\"../../data/cherenkov_transparency_corrections/{source_name}/\")\n",
    "# Sub-dl1 objects directory\n",
    "root_sub_dl1 = os.path.join(root_objects, \"sub_dl1/\")\n",
    "# Directory for the results of the fit of each run\n",
    "root_results = os.path.join(root_objects, \"results_fits/\")\n",
    "root_final_results = os.path.join(root_objects, \"final_results_fits/\")\n",
    "# Configuration file for the job launching\n",
    "file_job_config = os.path.join(root, \"config/job_config_runs.txt\")\n",
    "\n",
    "# Directories for the data\n",
    "dir_dl1b_scaled = os.path.join(root_data, \"dl1_scaled/\")\n",
    "dir_dl1m_scaled = os.path.join(root_data, \"dl1_merged_scaled/\")\n",
    "dir_dl2_scaled = os.path.join(root_data, \"dl2_scaled/\")\n",
    "dir_dl2 = os.path.join(root_data, \"dl2/\")\n",
    "dir_dl3_scaled_base = os.path.join(root_data, \"dl3_scaled/\")\n",
    "dir_dl3_base = os.path.join(root_data, \"dl3/\")\n",
    "dir_irfs = os.path.join(root_data, \"irfs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac0556c-39b4-4c5a-af1b-78b33779b9d9",
   "metadata": {},
   "source": [
    "### Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f320e0a-78b8-485d-b575-0bbc203558c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The run number that we are interested in apply the corrections.\n",
    "The process is done run-wise, so the input will be an individual run.\"\"\"\n",
    "input_str = \"2914\"\n",
    "\n",
    "flag_scaled_str = \"True\"\n",
    "\n",
    "simulated = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab7f716-817b-435e-a835-efdc80a77d2e",
   "metadata": {},
   "source": [
    "### First initial variables computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0337cf86-a4a9-4bad-8e46-b868554da7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Run numbers to analyze, in our case only one\n",
    "    run_number = int(input_str)\n",
    "\n",
    "    # Reading the scaled or not flag\n",
    "    if flag_scaled_str == \"True\":\n",
    "        flag_scaled = True\n",
    "    elif flag_scaled_str == \"False\":\n",
    "        flag_scaled = False\n",
    "    else:\n",
    "        logger.error(f\"Input string for scaling: {flag_scaled_str} not valid.\\nInput 'True' or 'False'\")\n",
    "    \n",
    "    # Number of subruns to analyze per run\n",
    "    subruns_num = None  # Specify the number of subruns you want to analyze, set subruns_num = None to analyze all subruns\n",
    "\n",
    "    dir_dl3        = os.path.join(dir_dl3_base, f\"{run_number:05}\")\n",
    "    dir_dl3_scaled = os.path.join(dir_dl3_scaled_base, f\"{run_number:05}\")\n",
    "\n",
    "    # Creating the directories in case they don't exist\n",
    "    for path in [os.path.dirname(config_file), dir_dl1b_scaled, dir_dl1m_scaled, dir_dl2, dir_dl2_scaled, dir_dl3_scaled, dir_dl3, dir_irfs]:\n",
    "        os.makedirs(os.path.join(path), exist_ok=True)\n",
    "    \n",
    "    # Creating and storing a configuration file for lstchain processes\n",
    "    utils.configure_lstchain(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b32e12",
   "metadata": {},
   "source": [
    "### Finding the files that interest us\n",
    "#### Extracting dl1 files and dl1 datachecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e95eb908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding dl1  data to dictionary (Run 2914)...\n",
      "...Finished adding dl1 data to dictionary\n",
      "\n",
      "Adding dl1 datacheck data to dictionary (Run 2914)...\n",
      "...Finished adding dl1 data to dictionary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 s, sys: 7.18 s, total: 20.3 s\n",
      "Wall time: 41.8 s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    # Getting coordinates of source\n",
    "    source_coords = SkyCoord.from_name(source_name)\n",
    "    \n",
    "    dict_source = {\n",
    "        \"name\"   : source_name,\n",
    "        \"coords\" : source_coords,\n",
    "        \"ra\"     : source_coords.ra.deg  * u.deg, # ra in degrees\n",
    "        \"dec\"    : source_coords.dec.deg * u.deg, # dec in degrees\n",
    "    }\n",
    "    \n",
    "    # We create a empty dictionary to store all the information needed inside\n",
    "    dict_dchecks = {}\n",
    "    for run in [run_number]:\n",
    "        dict_dchecks[run] = {\n",
    "            \"run_num\" : run,\n",
    "        }\n",
    "    # Then we add the paths to the files and the datachecks\n",
    "    dict_dchecks = lstpipeline.add_dl1_paths_to_dict(dict_dchecks, root_dl1)\n",
    "    dict_dchecks = lstpipeline.add_dl1_paths_to_dict(dict_dchecks, root_dl1, dchecking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac5fda0",
   "metadata": {},
   "source": [
    "#### Then we read the observations information and also the selected nodes for MC and RFs and we add it to the DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91f1e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "    dcheck_zd, dcheck_az = [], []\n",
    "    dcheck_tstart, dcheck_telapsed = [], []\n",
    "    \n",
    "    for srun in range(len(dict_dchecks[run_number][\"dchecks\"][\"srunwise\"])):\n",
    "        tab_dcheck_srun = read_table(dict_dchecks[run_number][\"dchecks\"][\"srunwise\"][srun], \"/dl1datacheck/cosmics\")\n",
    "        \n",
    "        # reading the variables\n",
    "        dcheck_zd.append(90 - np.rad2deg(tab_dcheck_srun[\"mean_alt_tel\"]))\n",
    "        dcheck_az.append(np.rad2deg(tab_dcheck_srun[\"mean_az_tel\"]))\n",
    "        \n",
    "        dcheck_tstart.append(tab_dcheck_srun[\"dragon_time\"])\n",
    "        dcheck_telapsed.append(tab_dcheck_srun[\"elapsed_time\"])\n",
    "    \n",
    "    dcheck_zd = np.array(dcheck_zd)\n",
    "    dcheck_az = np.array(dcheck_az)\n",
    "    dcheck_tstart = np.array(dcheck_tstart)\n",
    "    dcheck_telapsed = np.array(dcheck_telapsed)\n",
    "    \n",
    "    dict_dchecks[run_number][\"time\"] = {\n",
    "        \"tstart\"   : dcheck_tstart[0],         # datetime object\n",
    "        \"telapsed\" : np.sum(dcheck_telapsed),  # s\n",
    "        \"srunwise\" : {\n",
    "            \"telapsed\" : dcheck_telapsed,      # s      \n",
    "        },\n",
    "    }\n",
    "    dict_dchecks[run_number][\"pointing\"] = {\n",
    "        \"zd\" : np.mean(dcheck_zd),  # deg\n",
    "        \"az\" : np.mean(dcheck_az),  # deg\n",
    "        \"srunwise\" : {\n",
    "            \"zd\" : dcheck_zd, # deg\n",
    "            \"az\" : dcheck_az, # deg\n",
    "        },\n",
    "    }\n",
    "    # then we also select the RFs and MC files looking at the nodes available\n",
    "    dict_dchecks = lstpipeline.add_rf_node(dict_dchecks, root_rfs, dict_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7facb6",
   "metadata": {},
   "source": [
    "### Selecting DL1b files for the scaled case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab88c2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 693 µs, total: 693 µs\n",
      "Wall time: 705 µs\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    for ir, run in enumerate(dict_dchecks.keys()):\n",
    "    \n",
    "        dir_run = os.path.join(dir_dl1b_scaled, f\"{run:05}\")\n",
    "        \n",
    "        sruns = [int(path.split(\".\")[-2]) for path in dict_dchecks[run][\"dl1a\"][\"srunwise\"]]\n",
    "        \n",
    "        dict_dchecks[run][\"dl1b_scaled\"] = {\"srunwise\" : []}\n",
    "    \n",
    "        for i, srun in enumerate(sruns[:subruns_num]):\n",
    "    \n",
    "            input_fname  = dict_dchecks[run][\"dl1a\"][\"srunwise\"][i]\n",
    "            output_fname = os.path.join(dir_run, f\"dl1_LST-1.Run{run:05}.{srun:04}.h5\")\n",
    "    \n",
    "            dict_dchecks[run][\"dl1b_scaled\"][\"srunwise\"].append(output_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a2e52d-c0ab-4215-aa2f-2a5201bc7ceb",
   "metadata": {},
   "source": [
    "### DL1 merging run-wise for the scaled case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18677346-a4ab-4ba4-9c74-eac08e02fe86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File already exists, deleting and re-computing:\n",
      "-->/fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/dl1_merged_scaled/dl1_LST-1.Run02914.h5\n",
      "lstchain_merge_hdf5_files --input-dir /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/dl1_scaled/02914 --output-file /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/dl1_merged_scaled/dl1_LST-1.Run02914.h5 --no-image\n",
      "100%|██████████| 115/115 [02:21<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 157 ms, sys: 848 ms, total: 1.01 s\n",
      "Wall time: 2min 46s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    for ir, run in enumerate(dict_dchecks.keys()):\n",
    "    \n",
    "        dir_run = os.path.join(dir_dl1b_scaled, f\"{run:05}\")\n",
    "        output_fname = os.path.join(dir_dl1m_scaled, f\"dl1_LST-1.Run{run:05}.h5\")\n",
    "    \n",
    "        if (not simulated) and (flag_scaled):\n",
    "            if os.path.exists(output_fname):\n",
    "                logger.info(f\"File already exists, deleting and re-computing:\\n-->{output_fname}\")\n",
    "                os.remove(output_fname)\n",
    "    \n",
    "            command_mergehdf5 = f\"lstchain_merge_hdf5_files --input-dir {dir_run} --output-file {output_fname} --no-image\"\n",
    "            logger.info(command_mergehdf5)\n",
    "            \n",
    "            subprocess.run(command_mergehdf5, shell=True)\n",
    "        \n",
    "        dict_dchecks[run][\"dl1b_scaled\"][\"runwise\"] = output_fname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4483a6",
   "metadata": {},
   "source": [
    "### DL1b to DL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab771e57-c8c6-4468-b84b-186ecf296e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing dl2 for Run  2914 (Scaled=True)\n",
      "--> /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/dl2_scaled/dl2_LST-1.Run02914.h5\n",
      "\n",
      "lstchain_dl1_to_dl2 --input-files /fefs/aswg/data/real/DL1/20201117/v0.9/tailcut84/dl1_LST-1.Run02914.h5 --path-models /fefs/aswg/data/models/AllSky/20230927_v0.10.4_crab_tuned/dec_2276 --output-dir /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/dl2_scaled/ --config /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/config/standard_config.json\n",
      "/fefs/aswg/workspace/juan.jimenez/softs/cta-lstchain/lstchain/reco/utils.py:536: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "Data contains not-predictable events.\n",
      "Column | Number of non finite values\n",
      "skewness : 1165728\n",
      "kurtosis : 1165728\n",
      "wl : 1165728\n",
      "y : 1165728\n",
      "width : 1165728\n",
      "length : 1165728\n",
      "log_intensity : 1165728\n",
      "time_gradient : 1176514\n",
      "leakage_intensity_width_2 : 1165728\n",
      "x : 1165728\n",
      "/fefs/aswg/workspace/juan.jimenez/softs/cta-lstchain/lstchain/reco/utils.py:536: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'layout.__table_column_meta__'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'optics.__table_column_meta__'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'geometry_0.__table_column_meta__'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'readout_0.__table_column_meta__'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 74.9 ms, sys: 711 ms, total: 786 ms\n",
      "Wall time: 15min 19s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    for ir, run in enumerate(dict_dchecks.keys()):\n",
    "    \n",
    "        input_fname         = dict_dchecks[run][\"dl1a\"][\"runwise\"]\n",
    "        input_fname_scaled  = dict_dchecks[run][\"dl1b_scaled\"][\"runwise\"]\n",
    "        output_fname        = os.path.join(dir_dl2,        os.path.basename(input_fname       ).replace(\"dl1\", \"dl2\", 1))\n",
    "        output_fname_scaled = os.path.join(dir_dl2_scaled, os.path.basename(input_fname_scaled).replace(\"dl1\", \"dl2\", 1))\n",
    "        rf_node             = dict_dchecks[run][\"simulations\"][\"rf\"]\n",
    "\n",
    "        # Not scaled case\n",
    "        if (not simulated) and (not flag_scaled):\n",
    "            _dir_dl2 = dir_dl2\n",
    "            _input_fname = input_fname\n",
    "            _output_fname = output_fname\n",
    "        # Scaled case\n",
    "        elif (not simulated) and (flag_scaled):\n",
    "            _dir_dl2 = dir_dl2_scaled\n",
    "            _input_fname = input_fname\n",
    "            _output_fname = output_fname_scaled\n",
    "\n",
    "        if (not simulated):\n",
    "            # Check if the file exists and delete if exists (may be empty or half filled)\n",
    "            if os.path.exists(_output_fname):\n",
    "                logger.info(f\"File already exists, deleting and re-computing:\\n-->{_output_fname}\")\n",
    "                os.remove(_output_fname)\n",
    "                \n",
    "            logger.info(f\"\\nComputing dl2 for Run {run:5} (Scaled={str(flag_scaled)})\")\n",
    "            logger.info(f\"--> {_output_fname}\\n\")\n",
    "            command_dl1dl2 = f\"lstchain_dl1_to_dl2 --input-files {_input_fname} --path-models {rf_node} \"\n",
    "            command_dl1dl2 = command_dl1dl2 + f\"--output-dir {_dir_dl2} --config {config_file}\"\n",
    "            logger.info(command_dl1dl2)\n",
    "            \n",
    "            subprocess.run(command_dl1dl2, shell=True)\n",
    "    \n",
    "        dict_dchecks[run][\"dl2\"] = output_fname\n",
    "        dict_dchecks[run][\"dl2_scaled\"] = output_fname_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427be0b9",
   "metadata": {},
   "source": [
    "### DL2 to DL3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9bef92ad-fc26-4790-9d1e-9aa6fa778df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/dl3/02914/dl3_LST-1.Run02914.fits\n",
      "--> /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/dl3_scaled/02914/dl3_LST-1.Run02914.fits\n",
      "\n",
      "lstchain_create_dl3_file --input-dl2 /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/dl2_scaled/dl2_LST-1.Run02914.h5 --input-irf-path /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/irfs/ --output-dl3-path /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/dl3_scaled/02914 --source-name crab --source-ra 83.6287deg --source-dec 22.0147deg --config /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/config/standard_config.json --overwrite\n",
      "2024-03-12 11:41:13,425 \u001b[1;35mCRITICAL\u001b[0m [lstchain.DataReductionFITSWriter] (application.inner): Bad config encountered during initialization: Path \"/fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/dl2_scaled/dl2_LST-1.Run02914.h5\" does not exist\n"
     ]
    }
   ],
   "source": [
    "    ra_str  = \"{}\".format(dict_source[\"ra\"]).replace(\" \", \"\")\n",
    "    dec_str = \"{}\".format(dict_source[\"dec\"]).replace(\" \", \"\")\n",
    "    \n",
    "    \n",
    "    for ir, run in enumerate(dict_dchecks.keys()):\n",
    "     \n",
    "        dl2_fname         = dict_dchecks[run][\"dl2\"]\n",
    "        dl2_fname_scaled  = dict_dchecks[run][\"dl2_scaled\"]\n",
    "        output_dl3        = os.path.join(dir_dl3,        f\"dl3_LST-1.Run{run:05}.fits\")\n",
    "        output_dl3_scaled = os.path.join(dir_dl3_scaled, f\"dl3_LST-1.Run{run:05}.fits\")\n",
    "        \n",
    "        \n",
    "        # Not scaled case\n",
    "        if (not simulated) and (not flag_scaled):\n",
    "            logger.info(f\"\\nConverting dl2 for {run:5}\")\n",
    "            command_dl3 = f\"lstchain_create_dl3_file --input-dl2 {dl2_fname} --input-irf-path {dir_irfs} \"\n",
    "            command_dl3 = command_dl3 + f\"--output-dl3-path {dir_dl3}--source-name {source_name} --source-ra {ra_str} \"\n",
    "            command_dl3 = command_dl3 + f\"--source-dec {dec_str} --config {config_file} --overwrite\"\n",
    "            logger.info(command_dl3)\n",
    "            \n",
    "            subprocess.run(command_dl3, shell=True)\n",
    "    \n",
    "        # Scaled case\n",
    "        elif (not simulated) and (flag_scaled):\n",
    "            logger.info(f\"--> {output_dl3}\\n--> {output_dl3_scaled}\\n\")\n",
    "            command_dl3 = f\"lstchain_create_dl3_file --input-dl2 {dl2_fname_scaled} --input-irf-path {dir_irfs} \"\n",
    "            command_dl3 = command_dl3 + f\"--output-dl3-path {dir_dl3_scaled} --source-name {source_name} --source-ra {ra_str} \"\n",
    "            command_dl3 = command_dl3 + f\"--source-dec {dec_str} --config {config_file} --overwrite\"\n",
    "            logger.info(command_dl3)\n",
    "            \n",
    "            subprocess.run(command_dl3, shell=True)\n",
    "            \n",
    "        dict_dchecks[run][\"dl3\"] = output_dl3\n",
    "        dict_dchecks[run][\"dl3_scaled\"] = output_dl3_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc55dd74-bc8b-4854-9c83-983130e613e0",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b856212-4d0b-4598-965b-6bf7390ef6de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
