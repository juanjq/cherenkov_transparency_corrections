{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd7fae65-c8a9-4633-94d6-8de7ec8997ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from datetime import datetime\n",
    "import pickle, json, sys, os, glob\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import chi2\n",
    "from scipy import optimize\n",
    "import subprocess\n",
    "\n",
    "from astropy.coordinates import SkyCoord\n",
    "from lstchain.io.config  import get_standard_config\n",
    "from ctapipe.io          import read_table\n",
    "import tables\n",
    "\n",
    "# Other auxiliar scripts\n",
    "sys.path.insert(0, os.getcwd() + \"/../scripts/\")\n",
    "import auxiliar as aux\n",
    "import geometry as geom\n",
    "import lstpipeline\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2bac8d7-7058-41ae-a7aa-174b0861aca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Source name in order to just complete the results file, and\n",
    "in order to improve run organization.\"\"\"\n",
    "source_name = \"crab\"\n",
    "\n",
    "# Standard paths for data in the IT cluster ---------\n",
    "root_dl1 = \"/fefs/aswg/data/real/DL1/*/v0.*/tailcut84/\"\n",
    "# root_rfs = \"/fefs/aswg/data/models/AllSky/20240131_allsky_v0.10.5_all_dec_base/\"\n",
    "root_rfs = \"/fefs/aswg/data/models/AllSky/20230927_v0.10.4_crab_tuned/\"\n",
    "# root_mcs = \"/fefs/aswg/data/mc/DL2/AllSky/20240131_allsky_v0.10.5_all_dec_base/TestingDataset/\"\n",
    "root_mcs = \"/fefs/aswg/data/mc/DL2/AllSky/20230927_v0.10.4_crab_tuned/TestingDataset/\"\n",
    "\n",
    "# Root path of this script\n",
    "root = os.getcwd() + \"/\"\n",
    "# Path to store the configuration file we are going to use\n",
    "config_file = root + \"config/standard_config.json\"\n",
    "# Path to store objects\n",
    "root_objects = root + f\"objects/\"\n",
    "# Data main directory\n",
    "root_data = root + f\"../../data/cherenkov_transparency_corrections/{source_name}/\"\n",
    "\n",
    "\n",
    "# Directories for the data\n",
    "dir_dl1b_scaled = root_data + \"dl1_scaled/\"\n",
    "dir_dl1m_scaled = root_data + \"dl1_merged_scaled/\"\n",
    "dir_dl2_scaled  = root_data + \"dl2_scaled/\"\n",
    "dir_dl2         = root_data + \"dl2/\"\n",
    "dir_dl3_scaled_base  = root_data + \"dl3_scaled/\"\n",
    "dir_dl3_base         = root_data + \"dl3/\"\n",
    "dir_irfs        = root_data + \"irfs/\"\n",
    "\n",
    "\n",
    "def configure_lstchain():\n",
    "    \"\"\"Creates a file of standard configuration for the lstchain analysis. \n",
    "    It can be changed inside this function\"\"\"\n",
    "    dict_config = get_standard_config()\n",
    "    # We select the heuristic flatfield option in the standard configuration\n",
    "    dict_config[\"source_config\"][\"LSTEventSource\"][\"use_flatfield_heuristic\"] = True\n",
    "    with open(config_file, \"w\") as json_file:\n",
    "        json.dump(dict_config, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac0556c-39b4-4c5a-af1b-78b33779b9d9",
   "metadata": {},
   "source": [
    "### Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f320e0a-78b8-485d-b575-0bbc203558c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The run number that we are interested in apply the corrections.\n",
    "The process is done run-wise, so the input will be an individual run.\"\"\"\n",
    "input_str = \"3095\"\n",
    "\n",
    "flag_scaled_str = \"True\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab7f716-817b-435e-a835-efdc80a77d2e",
   "metadata": {},
   "source": [
    "### First initial variables computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0337cf86-a4a9-4bad-8e46-b868554da7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run numbers to analyze, in our case only one\n",
    "run_number = int(input_str)\n",
    "\n",
    "# Reading the scaled or not flag\n",
    "if flag_scaled == \"True\":\n",
    "    flag_scaled = True\n",
    "elif flag_scaled_str == \"False\":\n",
    "    flag_scaled = False\n",
    "else:\n",
    "    logger.error(f\"Input string for scaling: {flag_scaled_str} not valid.\\nInput 'True' or 'False'\")\n",
    "        \n",
    "# Number of subruns to analyze per run\n",
    "subruns_num = None  # Specify the number of subruns you want to analyze, set subruns_num = None to analyze all subruns\n",
    "\n",
    "dir_dl3_scaled = dir_dl3_scaled_base + f\"{run_number:05}/\"\n",
    "dir_dl3        = dir_dl3_base        + f\"{run_number:05}/\"\n",
    "\n",
    "# Creating the directories in case they don't exist\n",
    "for path in [os.path.dirname(config_file), dir_dl1b_scaled, dir_dl1m_scaled, dir_dl2, dir_dl2_scaled, dir_dl3_scaled, dir_dl3, dir_irfs]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(os.path.join(path), exist_ok=True)\n",
    "\n",
    "# Creating and storing a configuration file for lstchain processes\n",
    "configure_lstchain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b32e12",
   "metadata": {},
   "source": [
    "### Finding the files that interest us\n",
    "#### Extracting dl1 files and dl1 datachecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e95eb908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding dl1  data to dictionary (Run 3095)...\n",
      "...Finished adding dl1 data to dictionary\n",
      "\n",
      "Adding dl1 datacheck data to dictionary (Run 3095)...\n",
      "...Finished adding dl1 data to dictionary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.5 s, sys: 6.2 s, total: 17.7 s\n",
      "Wall time: 28.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Getting coordinates of source\n",
    "source_coords = SkyCoord.from_name(source_name)\n",
    "\n",
    "dict_source = {\n",
    "    \"name\"   : source_name,\n",
    "    \"coords\" : source_coords,\n",
    "    \"ra\"     : source_coords.ra.deg  * u.deg, # ra in degrees\n",
    "    \"dec\"    : source_coords.dec.deg * u.deg, # dec in degrees\n",
    "}\n",
    "\n",
    "# We create a empty dictionary to store all the information needed inside\n",
    "dict_dchecks = {}\n",
    "for run in [run_number]:\n",
    "    dict_dchecks[run] = {\n",
    "        \"run_num\" : run,\n",
    "    }\n",
    "# Then we add the paths to the files and the datachecks\n",
    "dict_dchecks = lstpipeline.add_dl1_paths_to_dict(dict_dchecks, root_dl1)\n",
    "dict_dchecks = lstpipeline.add_dl1_paths_to_dict(dict_dchecks, root_dl1, dchecking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac5fda0",
   "metadata": {},
   "source": [
    "#### Then we read the observations information and also the selected nodes for MC and RFs and we add it to the DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91f1e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcheck_zd, dcheck_az = [], []\n",
    "dcheck_tstart, dcheck_telapsed = [], []\n",
    "\n",
    "for srun in range(len(dict_dchecks[run_number][\"dchecks\"][\"srunwise\"])):\n",
    "    tab_dcheck_srun = read_table(dict_dchecks[run_number][\"dchecks\"][\"srunwise\"][srun], \"/dl1datacheck/cosmics\")\n",
    "    \n",
    "    # reading the variables\n",
    "    dcheck_zd.append(90 - np.rad2deg(tab_dcheck_srun[\"mean_alt_tel\"]))\n",
    "    dcheck_az.append(np.rad2deg(tab_dcheck_srun[\"mean_az_tel\"]))\n",
    "    \n",
    "    dcheck_tstart.append(tab_dcheck_srun[\"dragon_time\"])\n",
    "    dcheck_telapsed.append(tab_dcheck_srun[\"elapsed_time\"])\n",
    "\n",
    "dcheck_zd = np.array(dcheck_zd)\n",
    "dcheck_az = np.array(dcheck_az)\n",
    "dcheck_tstart = np.array(dcheck_tstart)\n",
    "dcheck_telapsed = np.array(dcheck_telapsed)\n",
    "\n",
    "dict_dchecks[run_number][\"time\"] = {\n",
    "    \"tstart\"   : dcheck_tstart[0],            # datetime object\n",
    "    \"telapsed\" : np.sum(dcheck_telapsed),  # s\n",
    "    \"srunwise\" : {\n",
    "        \"telapsed\" : dcheck_telapsed,      # s      \n",
    "    },\n",
    "}\n",
    "dict_dchecks[run_number][\"pointing\"] = {\n",
    "    \"zd\" : np.mean(dcheck_zd),  # deg\n",
    "    \"az\" : np.mean(dcheck_az),  # deg\n",
    "    \"srunwise\" : {\n",
    "        \"zd\" : dcheck_zd, # deg\n",
    "        \"az\" : dcheck_az, # deg\n",
    "    },\n",
    "}\n",
    "# then we also select the RFs and MC files looking at the nodes available\n",
    "dict_dchecks, dict_nodes = lstpipeline.add_mc_and_rfs_nodes(dict_dchecks, root_rfs, root_mcs, dict_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7facb6",
   "metadata": {},
   "source": [
    "### Selecting DL1b files for the scaled case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab88c2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 422 µs, sys: 0 ns, total: 422 µs\n",
      "Wall time: 429 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for ir, run in enumerate(dict_dchecks.keys()):\n",
    "\n",
    "    dir_run = dir_dl1b_scaled + f\"{run:05}\" + \"/\"\n",
    "    \n",
    "    sruns = [int(path.split(\".\")[-2]) for path in dict_dchecks[run][\"dl1a\"][\"srunwise\"]]\n",
    "    \n",
    "    dict_dchecks[run][\"dl1b_scaled\"] = {\"srunwise\" : []}\n",
    "\n",
    "    for i, srun in enumerate(sruns[:subruns_num]):\n",
    "\n",
    "        input_fname  = dict_dchecks[run][\"dl1a\"][\"srunwise\"][i]\n",
    "        output_fname = dir_run + f\"dl1_LST-1.Run{run:05}.{srun:04}.h5\"\n",
    "\n",
    "        dict_dchecks[run][\"dl1b_scaled\"][\"srunwise\"].append(output_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a2e52d-c0ab-4215-aa2f-2a5201bc7ceb",
   "metadata": {},
   "source": [
    "### DL1 merging run-wise for the scaled case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18677346-a4ab-4ba4-9c74-eac08e02fe86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lstchain_merge_hdf5_files --input-dir /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/dl1_scaled/03095/ --output-file /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/dl1_merged_scaled/dl1_LST-1.Run03095.h5 --run-number 3095 --no-image\n",
      "100%|██████████| 217/217 [02:02<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 266 ms, sys: 1.13 s, total: 1.39 s\n",
      "Wall time: 2min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for ir, run in enumerate(dict_dchecks.keys()):\n",
    "\n",
    "    dir_run = dir_dl1b_scaled + f\"{run:05}\" + \"/\"\n",
    "    output_fname = dir_dl1m_scaled + f\"dl1_LST-1.Run{run:05}.h5\"\n",
    "\n",
    "    if flag_scaled:\n",
    "        command = f\"lstchain_merge_hdf5_files --input-dir {dir_run} --output-file {output_fname} --run-number {run} --no-image\"\n",
    "        logger.info(command)\n",
    "        \n",
    "        subprocess.run(command, shell=True)\n",
    "    \n",
    "    dict_dchecks[run][\"dl1b_scaled\"][\"runwise\"] = output_fname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4483a6",
   "metadata": {},
   "source": [
    "### DL1b to DL2 \n",
    "#### - For the scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab771e57-c8c6-4468-b84b-186ecf296e18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File already exists, deleting and re-computing:\n",
      "-->/fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/dl2_scaled/dl2_LST-1.Run03095.h5\n",
      "\n",
      "Computing dl2 for Run  3095 (scaled data)\n",
      "--> /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/dl2_scaled/dl2_LST-1.Run03095.h5\n",
      "\n",
      "lstchain_dl1_to_dl2 --input-files /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/dl1_merged_scaled/dl1_LST-1.Run03095.h5 --path-models /fefs/aswg/data/models/AllSky/20230927_v0.10.4_crab_tuned/dec_2276 --output-dir /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/dl2_scaled/ --config /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/config/standard_config.json\n",
      "/fefs/aswg/workspace/juan.jimenez/softs/cta-lstchain/lstchain/reco/utils.py:536: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "Data contains not-predictable events.\n",
      "Column | Number of non finite values\n",
      "y : 2517599\n",
      "leakage_intensity_width_2 : 2517599\n",
      "time_gradient : 2536325\n",
      "skewness : 2517603\n",
      "length : 2517599\n",
      "wl : 2517603\n",
      "kurtosis : 2517603\n",
      "log_intensity : 2517599\n",
      "width : 2517599\n",
      "x : 2517599\n",
      "/fefs/aswg/workspace/juan.jimenez/softs/cta-lstchain/lstchain/reco/utils.py:536: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.3 ms, sys: 868 ms, total: 927 ms\n",
      "Wall time: 16min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for ir, run in enumerate(dict_dchecks.keys()):\n",
    "\n",
    "    input_fname  = dict_dchecks[run][\"dl1b_scaled\"][\"runwise\"]\n",
    "    output_fname = dir_dl2_scaled + input_fname.split(\"/\")[-1].replace(\"dl1\", \"dl2\", 1)\n",
    "    rf_node      = dict_dchecks[run][\"simulations\"][\"rf\"]\n",
    "\n",
    "    \n",
    "\n",
    "    if flag_scaled:\n",
    "        # Check if the file exists and delete if exists (may be empty or half filled)\n",
    "        if os.path.exists(output_fname):\n",
    "            logger.info(f\"File already exists, deleting and re-computing:\\n-->{output_fname}\")\n",
    "            os.remove(output_fname)\n",
    "        \n",
    "        logger.info(f\"\\nComputing dl2 for Run {run:5} (scaled data)\")\n",
    "        logger.info(f\"--> {output_fname}\\n\")\n",
    "        command = f\"lstchain_dl1_to_dl2 --input-files {input_fname} --path-models {rf_node} --output-dir {dir_dl2_scaled} --config {config_file}\"\n",
    "        logger.info(command)\n",
    "        \n",
    "        subprocess.run(command, shell=True)\n",
    "\n",
    "    dict_dchecks[run][\"dl2_scaled\"] = output_fname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88830280-e33f-48db-928a-c41313160cd6",
   "metadata": {},
   "source": [
    "#### - For the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e073c23-28ff-4619-976e-66fe96e859d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing dl2 for Run  3095 (original data)\n",
      "--> /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/dl2/dl2_LST-1.Run03095.h5\n",
      "\n",
      "lstchain_dl1_to_dl2 --input-files /fefs/aswg/data/real/DL1/20201207/v0.10/tailcut84/dl1_LST-1.Run03095.h5 --path-models /fefs/aswg/data/models/AllSky/20230927_v0.10.4_crab_tuned/dec_2276 --output-dir /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/dl2/ --config /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/config/standard_config.json\n",
      "/fefs/aswg/workspace/juan.jimenez/softs/cta-lstchain/lstchain/reco/utils.py:536: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "Data contains not-predictable events.\n",
      "Column | Number of non finite values\n",
      "kurtosis : 2481593\n",
      "time_gradient : 2500701\n",
      "width : 2481589\n",
      "length : 2481589\n",
      "log_intensity : 2481589\n",
      "leakage_intensity_width_2 : 2481589\n",
      "skewness : 2481593\n",
      "x : 2481589\n",
      "wl : 2481593\n",
      "y : 2481589\n",
      "/fefs/aswg/workspace/juan.jimenez/softs/cta-lstchain/lstchain/reco/utils.py:536: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 82.6 ms, sys: 860 ms, total: 943 ms\n",
      "Wall time: 17min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for ir, run in enumerate(dict_dchecks.keys()):\n",
    "\n",
    "    input_fname  = dict_dchecks[run][\"dl1a\"][\"runwise\"]\n",
    "    output_fname = dir_dl2 + input_fname.split(\"/\")[-1].replace(\"dl1\", \"dl2\", 1)\n",
    "    rf_node      = dict_dchecks[run][\"simulations\"][\"rf\"]\n",
    "\n",
    "    \n",
    "\n",
    "    if not flag_scaled:\n",
    "        # Check if the file exists and delete if exists (may be empty or half filled)\n",
    "        if os.path.exists(output_fname):\n",
    "            logger.info(f\"File already exists, deleting and re-computing:\\n-->{output_fname}\")\n",
    "            os.remove(output_fname)\n",
    "            \n",
    "        logger.info(f\"\\nComputing dl2 for Run {run:5} (original data)\")\n",
    "        logger.info(f\"--> {output_fname}\\n\")    \n",
    "        command = f\"lstchain_dl1_to_dl2 --input-files {input_fname} --path-models {rf_node} --output-dir {dir_dl2} --config {config_file}\"\n",
    "        logger.info(command)\n",
    "        \n",
    "        subprocess.run(command, shell=True)\n",
    "\n",
    "    dict_dchecks[run][\"dl2\"] = output_fname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b96c170",
   "metadata": {},
   "source": [
    "### MCs to IRFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6d8ad8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "IRF dec_2276_node_theta_73.142_az_331.979_ already computed\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.28 ms, sys: 2.82 ms, total: 4.1 ms\n",
      "Wall time: 2.44 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Already computed IRFs\n",
    "computed_irfs = glob.glob(dir_irfs + \"*\")\n",
    "\n",
    "for ir, run in enumerate(dict_dchecks.keys()):\n",
    "    \n",
    "    input_mc = dict_dchecks[run][\"simulations\"][\"mc\"]\n",
    "\n",
    "    output_irf = dir_irfs + \"irf_{}_{}.fits.gz\".format(input_mc.split(\"/\")[-3], input_mc.split(\"/\")[-2])\n",
    "\n",
    "    # we don't compute the IRF if it has been already done\n",
    "    if output_irf not in computed_irfs:\n",
    "        \n",
    "        logger.info(f\"\\nComputing IRF for Run {run:5}\")\n",
    "        logger.info(f\"--> {output_irf}\\n\")\n",
    "        \n",
    "        command = f\"lstchain_create_irf_files --input-gamma-dl2 {input_mc} --output-irf-file {output_irf} --point-like\"\n",
    "        command = command + f\" --energy-dependent-gh --energy-dependent-theta --overwrite\"\n",
    "        logger.info(command)\n",
    "    \n",
    "        subprocess.run(command, shell=True)\n",
    "    \n",
    "    else:\n",
    "        logger.info(\"\\nIRF {}_{} already computed\\n\".format(input_mc.split(\"/\")[-3], input_mc.split(\"/\")[-2]))\n",
    "    dict_dchecks[run][\"irf\"] = output_irf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427be0b9",
   "metadata": {},
   "source": [
    "### DL2 to DL3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bef92ad-fc26-4790-9d1e-9aa6fa778df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting dl2 for  3095\n",
      "--> /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/dl3/03095/dl3_LST-1.Run03095.fits\n",
      "--> /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/dl3_scaled/03095/dl3_LST-1.Run03095.fits\n",
      "\n",
      "lstchain_create_dl3_file --input-dl2 /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/dl2/dl2_LST-1.Run03095.h5 --input-irf-path /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/irfs/ --output-dl3-path /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/dl3/03095/ --source-name crab --source-ra 83.6287deg --source-dec 22.0147deg --config /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/config/standard_config.json --overwrite\n",
      "2024-03-05 08:05:10,334 \u001b[1;33mWARNING\u001b[0m [lstchain.DataReductionFITSWriter] (lstchain_create_dl3_file.setup): Overwriting /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/dl3/03095/dl3_LST-1.Run03095.fits\n",
      "2024-03-05 08:05:10,335 \u001b[1;33mWARNING\u001b[0m [lstchain.DataReductionFITSWriter] (lstchain_create_dl3_file.setup): Overwriting /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/dl3/03095/final_irf.fits.gz\n",
      "/fefs/aswg/workspace/juan.jimenez/softs/cta-lstchain/lstchain/reco/utils.py:536: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The metadata are comparable\n",
      "The other parameter axes data are comparable\n",
      "Target value is outside interpolation. Using the nearest IRF.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lstchain_create_dl3_file --input-dl2 /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/dl2_scaled/dl2_LST-1.Run03095.h5 --input-irf-path /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/irfs/ --output-dl3-path /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/dl3_scaled/03095/ --source-name crab --source-ra 83.6287deg --source-dec 22.0147deg --config /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/config/standard_config.json --overwrite\n",
      "2024-03-05 08:06:46,330 \u001b[1;33mWARNING\u001b[0m [lstchain.DataReductionFITSWriter] (lstchain_create_dl3_file.setup): Overwriting /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/../../data/cherenkov_transparency_corrections/crab/dl3_scaled/03095/final_irf.fits.gz\n",
      "/fefs/aswg/workspace/juan.jimenez/softs/cta-lstchain/lstchain/reco/utils.py:536: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The metadata are comparable\n",
      "The other parameter axes data are comparable\n",
      "Target value is outside interpolation. Using the nearest IRF.\n",
      "CPU times: user 23.3 ms, sys: 189 ms, total: 213 ms\n",
      "Wall time: 3min 14s\n"
     ]
    }
   ],
   "source": [
    "ra_str  = \"{}\".format(dict_source[\"ra\"]).replace(\" \", \"\")\n",
    "dec_str = \"{}\".format(dict_source[\"dec\"]).replace(\" \", \"\")\n",
    "\n",
    "\n",
    "for ir, run in enumerate(dict_dchecks.keys()):\n",
    "\n",
    "    # dir_run = dir_dl3 + f\"{run:05}\" + \"/\"    \n",
    "    dl2_fname = dict_dchecks[run][\"dl2\"]\n",
    "    dl2_fname_scaled = dict_dchecks[run][\"dl2_scaled\"]\n",
    "    output_dl3 = dir_dl3 + f\"dl3_LST-1.Run{run:05}.fits\"\n",
    "    output_dl3_scaled = dir_dl3_scaled + f\"dl3_LST-1.Run{run:05}.fits\"\n",
    "    \n",
    "    \n",
    "    if not flag_scaled:\n",
    "        logger.info(f\"\\nConverting dl2 for {run:5}\")\n",
    "        command = f\"lstchain_create_dl3_file --input-dl2 {dl2_fname} --input-irf-path {dir_irfs} --output-dl3-path {dir_dl3} \"\n",
    "        command = command + f\"--source-name {source_name} --source-ra {ra_str} --source-dec {dec_str} --config {config_file} --overwrite\"\n",
    "        logger.info(command)\n",
    "        \n",
    "        subprocess.run(command, shell=True)\n",
    "\n",
    "    if flag_scaled:\n",
    "        logger.info(f\"--> {output_dl3}\\n--> {output_dl3_scaled}\\n\")\n",
    "        command = f\"lstchain_create_dl3_file --input-dl2 {dl2_fname_scaled} --input-irf-path {dir_irfs} --output-dl3-path {dir_dl3_scaled} \"\n",
    "        command = command + f\"--source-name {source_name} --source-ra {ra_str} --source-dec {dec_str} --config {config_file} --overwrite\"\n",
    "        logger.info(command)\n",
    "        \n",
    "        subprocess.run(command, shell=True)\n",
    "        \n",
    "    dict_dchecks[run][\"dl3\"] = output_dl3\n",
    "    dict_dchecks[run][\"dl3_scaled\"] = output_dl3_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1be4be3",
   "metadata": {},
   "source": [
    "### Add DL3 index files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2719e761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger.info(f\"All dl3 files created 100%\\n\\n\\nCreating index files...\")\n",
    "\n",
    "# command = f\"lstchain_create_dl3_index_files --input-dl3-dir {dir_dl3} --overwrite\"\n",
    "# logger.info(command)\n",
    "# subprocess.run(command, shell=True)\n",
    "\n",
    "# command = f\"lstchain_create_dl3_index_files --input-dl3-dir {dir_dl3_scaled} --overwrite\"\n",
    "# logger.info(command)\n",
    "# subprocess.run(command, shell=True)\n",
    "\n",
    "# logger.info(f\"\\nFinished with the dl3 process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc55dd74-bc8b-4854-9c83-983130e613e0",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b856212-4d0b-4598-965b-6bf7390ef6de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
