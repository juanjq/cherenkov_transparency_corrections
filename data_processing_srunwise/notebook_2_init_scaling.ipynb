{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e70776f7-e2e1-4057-b7ef-878b2be0556d",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b3b4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from datetime import datetime\n",
    "import pickle, json, sys, os, glob\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import chi2\n",
    "import subprocess\n",
    "\n",
    "from astropy.coordinates import SkyCoord\n",
    "from lstchain.io.config  import get_standard_config\n",
    "from ctapipe.io          import read_table\n",
    "import tables\n",
    "\n",
    "# Other auxiliar scripts\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"../scripts/\"))\n",
    "import geometry as geom\n",
    "import lstpipeline\n",
    "import script_utils_scaling as utils\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8c35e8-27a1-4e74-b5fe-82d5cb4aee4f",
   "metadata": {},
   "source": [
    "### Configuration and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0db13a5-8449-4e5c-8281-8a21053606d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Source name in order to just complete the results file, and\n",
    "in order to improve run organization.\"\"\"\n",
    "source_name = \"crab\"\n",
    "\n",
    "\"\"\" Fit parameters\n",
    "Chosen limits in intensity (p.e.) for applying the fit i.e. the\n",
    "power law will be fitted only with the points within this range.\"\"\"\n",
    "limits_intensity = [316, 562]\n",
    "\"\"\" For the positive scaling cases (most of them), we need to have a lower \n",
    "limit in intensity. Thi slimit is used for the subset of events that are \n",
    "scaled just to find which is the scaling value. We use a very low limit by\n",
    "default 60 p.e. compared to the lower limit of the fit 316 p.e. because in \n",
    "the worst cases we will have a very non-linear scaling that will displace \n",
    "significantly the events intensities.\"\"\"\n",
    "limits_intensity_extended = 60\n",
    "\n",
    "\"\"\" Power law parameters for the reference\n",
    "All these parameters are taken from a common analysis of the full dataset\n",
    "Where the period of end of 2022 and start 2023 is taken as reference for good \n",
    "runs. Then we take as reference the mean power law parameters in that period.\n",
    "p0 is the normalization factor and p1 is the slope.\"\"\"\n",
    "ref_p0 =  1.74 \n",
    "ref_p1 = -2.23\n",
    "\n",
    "\"\"\" Threshold in statistics for the last subrun\n",
    "The limit in number of events after cleaning that we need to consider the last\n",
    "subrun has enough statistics to perform the analysis over it. Otherwise the \n",
    "values of the scaling that will be applied to this last rubrun are the same \n",
    "that are applied to the last last subrun.\"\"\"\n",
    "statistics_threshold = 10000\n",
    "\n",
    "\"\"\" Parameters for the empyrical fits for Zenith Distance corrections\n",
    "Are simply two 2 degree polynomials for each variable of the power law.\"\"\"\n",
    "p0a, p0b, p0c = -0.44751321, 3.62502037, -1.43611437\n",
    "p1a, p1b, p1c = -2.89253919, 0.99443581, -0.34013068\n",
    "\n",
    "# Standard paths for data in the IT cluster ---------\n",
    "root_dl1 = \"/fefs/aswg/data/real/DL1/*/v0.*/tailcut84/\"\n",
    "root_rfs = \"/fefs/aswg/data/models/AllSky/20230927_v0.10.4_crab_tuned/\"\n",
    "root_mcs = \"/fefs/aswg/data/mc/DL2/AllSky/20230927_v0.10.4_crab_tuned/TestingDataset/\"\n",
    "\n",
    "# Root path of this script\n",
    "root = os.getcwd()\n",
    "# Path to store the configuration file we are going to use\n",
    "config_file = os.path.join(root, \"config/standard_config.json\")\n",
    "# Path to store objects\n",
    "root_objects = os.path.join(root, f\"objects/\")\n",
    "# Data main directory\n",
    "root_data = os.path.join(root, f\"../../data/cherenkov_transparency_corrections/{source_name}/\")\n",
    "# Sub-dl1 objects directory\n",
    "root_sub_dl1 = os.path.join(root_objects, \"sub_dl1/\")\n",
    "# Directory for the results of the fit of each run\n",
    "root_results = os.path.join(root_objects, \"results_fits/\")\n",
    "root_final_results = os.path.join(root_objects, \"final_results_fits/\")\n",
    "# Configuration file for the job launching\n",
    "file_job_config = os.path.join(root, \"config/job_config_runs.txt\")\n",
    "\n",
    "# Directories for the data\n",
    "dir_dl1b_scaled = os.path.join(root_data, \"dl1_scaled/\")\n",
    "dir_dl1m_scaled = os.path.join(root_data, \"dl1_merged_scaled/\")\n",
    "dir_dl2_scaled = os.path.join(root_data, \"dl2_scaled/\")\n",
    "dir_dl2 = os.path.join(root_data, \"dl2/\")\n",
    "dir_dl3_scaled_base = os.path.join(root_data, \"dl3_scaled/\")\n",
    "dir_dl3_base = os.path.join(root_data, \"dl3/\")\n",
    "dir_irfs = os.path.join(root_data, \"irfs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cff6f04",
   "metadata": {},
   "source": [
    "### Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d56ff173",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The run number that we are interested in apply the corrections.\n",
    "The process is done run-wise, so the input will be an individual run.\"\"\"\n",
    "input_str = \"3708_0\"\n",
    "simulate_data = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6b85d0-8470-47bf-97c0-69acffe92c59",
   "metadata": {},
   "source": [
    "### First initial variables computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c300d18-b522-41e6-bc4a-e131f8d46b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "    ########################################\n",
    "    # Initial configuring and paths creation\n",
    "    # Extracting the run number from the input string\n",
    "    run_number   = int(input_str.split(\"_\")[0])\n",
    "    first_last_srun = [int(s) for s in input_str.split(\"_\")[1:]]\n",
    "    if len(first_last_srun) == 1:\n",
    "        srun_numbers = np.array(first_last_srun)\n",
    "    else:\n",
    "        srun_numbers = np.arange(first_last_srun[0], first_last_srun[1] + 1)\n",
    "    \"\"\" Empty dictionary to store all the results of one run.\"\"\"\n",
    "    dict_results_empty = { \n",
    "        \"run\": run_number, \"filenames\": {}, \"statistics\": {}, \"flag_error\" : {},\n",
    "        \"scaled\" :           {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"p0\":                {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"delta_p0\":          {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"p1\":                {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"delta_p1\":          {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"chi2\":              {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"pvalue\":            {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"light_yield\":       {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"delta_light_yield\": {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"scaling\":           {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"delta_scaling\":     {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"scaling_percent\":       {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"delta_scaling_percent\": {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"final_scaling\": {}, \"delta_final_scaling\": {}, \"final_scaling_interpolated\": {}, \"interpolation\" : {},\n",
    "    }\n",
    "    # Create the paths that do not exist\n",
    "    for path in [os.path.dirname(config_file), root_data, root_objects, root_results, root_final_results, root_sub_dl1]:\n",
    "        os.makedirs(os.path.join(path), exist_ok=True)\n",
    "    # Creating and storing a configuration file for lstchain processes\n",
    "    utils.configure_lstchain(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b32e12",
   "metadata": {},
   "source": [
    "### Finding the files that interest us\n",
    "#### Extracting dl1 files and dl1 datachecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e95eb908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding dl1  data to dictionary (Run 3708)...\n",
      "...Finished adding dl1 data to dictionary\n",
      "\n",
      "Adding dl1 datacheck data to dictionary (Run 3708)...\n",
      "...Finished adding dl1 data to dictionary\n"
     ]
    }
   ],
   "source": [
    "    ################################################################\n",
    "    # Generating a dictionary with the information of all datachecks\n",
    "    # Getting coordinates of source\n",
    "    source_coords = SkyCoord.from_name(source_name)\n",
    "    \n",
    "    dict_source = {\n",
    "        \"name\"   : source_name,\n",
    "        \"coords\" : source_coords,\n",
    "        \"ra\"     : source_coords.ra.deg  * u.deg, # ra in degrees\n",
    "        \"dec\"    : source_coords.dec.deg * u.deg, # dec in degrees\n",
    "    }\n",
    "    \n",
    "    # We create a empty dictionary to store all the information needed inside\n",
    "    dict_dchecks = {}\n",
    "    for run in [run_number]:\n",
    "        dict_dchecks[run] = {\n",
    "            \"run_num\" : run,\n",
    "        }\n",
    "    # Then we add the paths to the files and the datachecks\n",
    "    dict_dchecks = lstpipeline.add_dl1_paths_to_dict(dict_dchecks, root_dl1)\n",
    "    dict_dchecks = lstpipeline.add_dl1_paths_to_dict(dict_dchecks, root_dl1, dchecking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac5fda0",
   "metadata": {},
   "source": [
    "#### Then we read the observations information and also the selected nodes for MC and RFs and we add it to the DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91f1e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "    dcheck_zd, dcheck_az = [], []\n",
    "    dcheck_tstart, dcheck_telapsed = [], []\n",
    "    \n",
    "    for srun in range(len(dict_dchecks[run_number][\"dchecks\"][\"srunwise\"])):\n",
    "        tab_dcheck_srun = read_table(dict_dchecks[run_number][\"dchecks\"][\"srunwise\"][srun], \"/dl1datacheck/cosmics\")\n",
    "        \n",
    "        # reading the variables\n",
    "        dcheck_zd.append(90 - np.rad2deg(tab_dcheck_srun[\"mean_alt_tel\"]))\n",
    "        dcheck_az.append(np.rad2deg(tab_dcheck_srun[\"mean_az_tel\"]))\n",
    "        \n",
    "        dcheck_tstart.append(tab_dcheck_srun[\"dragon_time\"])\n",
    "        dcheck_telapsed.append(tab_dcheck_srun[\"elapsed_time\"])\n",
    "    \n",
    "    dcheck_zd = np.array(dcheck_zd)\n",
    "    dcheck_az = np.array(dcheck_az)\n",
    "    dcheck_tstart = np.array(dcheck_tstart)\n",
    "    dcheck_telapsed = np.array(dcheck_telapsed)\n",
    "    \n",
    "    dict_dchecks[run_number][\"time\"] = {\n",
    "        \"tstart\"   : dcheck_tstart[0],            # datetime object\n",
    "        \"telapsed\" : np.sum(dcheck_telapsed),  # s\n",
    "        \"srunwise\" : {\n",
    "            \"telapsed\" : dcheck_telapsed,      # s      \n",
    "        },\n",
    "    }\n",
    "    dict_dchecks[run_number][\"pointing\"] = {\n",
    "        \"zd\" : np.mean(dcheck_zd),  # deg\n",
    "        \"az\" : np.mean(dcheck_az),  # deg\n",
    "        \"srunwise\" : {\n",
    "            \"zd\" : dcheck_zd, # deg\n",
    "            \"az\" : dcheck_az, # deg\n",
    "        },\n",
    "    }\n",
    "    # then we also select the RFs and MC files looking at the nodes available\n",
    "    dict_dchecks = lstpipeline.add_rf_node(dict_dchecks, root_rfs, dict_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261c0e68-ee3e-40b1-a11f-0eeaed2ca94b",
   "metadata": {},
   "source": [
    "### Read datacheck\n",
    "#### - The binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89f3cc7f-95f6-405c-a8ad-19b8cbc8f26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The intensity in the middle of the intensity range is 421.4 p.e.\n"
     ]
    }
   ],
   "source": [
    "    # Pivot intensity for decorrelation\n",
    "    ref_intensity = (limits_intensity[0] * limits_intensity[1]) ** 0.5\n",
    "    logger.info(f\"The intensity in the middle of the intensity range is {ref_intensity:.1f} p.e.\")\n",
    "    \n",
    "    ########################################################\n",
    "    # Reading the binning from the datacheck ---------------\n",
    "    # Opening the corresponding datacheck\n",
    "    fname_dcheck = dict_dchecks[run_number][\"dchecks\"][\"runwise\"]\n",
    "    tab_dcheck_run = tables.open_file(fname_dcheck)\n",
    "    \n",
    "    # Read the binning from the datacheck of the first subrun\n",
    "    dcheck_intensity_binning = np.array(tab_dcheck_run.root.dl1datacheck.histogram_binning.col(\"hist_intensity\")[0])\n",
    "    # Calculating the logarithmic center of each bin\n",
    "    dcheck_intensity_binning_centers = (dcheck_intensity_binning[:-1] * dcheck_intensity_binning[1:]) ** 0.5\n",
    "    # Calculating the width of each bin\n",
    "    dcheck_intensity_binning_widths = np.diff(dcheck_intensity_binning)\n",
    "    tab_dcheck_run.close()\n",
    "    \n",
    "    # Mask for the fitting region in the fits\n",
    "    mask_dcheck_bins_fit = (\n",
    "        (dcheck_intensity_binning_centers >= limits_intensity[0]) &\n",
    "        (dcheck_intensity_binning_centers <= limits_intensity[1])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eca996a-e892-4e46-9a04-7b18fb9411cf",
   "metadata": {},
   "source": [
    "#### - The intensity data from the datacheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7716a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    ##########################################################\n",
    "    # Reading the histogram from the datacheck ---------------\n",
    "    # Opening the corresponding datacheck\n",
    "    dcheck_hist_intensities = []\n",
    "    for fname_dcheck_srun in dict_dchecks[run_number][\"dchecks\"][\"srunwise\"]:\n",
    "        tab_dcheck_srun = tables.open_file(fname_dcheck_srun)\n",
    "        dcheck_hist_intensities.append(np.array(tab_dcheck_srun.root.dl1datacheck.cosmics.col(\"hist_intensity\")))\n",
    "        tab_dcheck_srun.close()\n",
    "    \n",
    "    # Converting from counts to rate per intensity unit (non-binning dependent quantity)\n",
    "    dcheck_rates       = [] # Array of histogram of rates for each subrun\n",
    "    dcheck_delta_rates = [] # The statistical error\n",
    "    for srun, dcheck_hist_intensity in enumerate(dcheck_hist_intensities):\n",
    "    \n",
    "        effective_time_srun = dict_dchecks[run_number][\"time\"][\"srunwise\"][\"telapsed\"][srun]\n",
    "        \n",
    "        dcheck_rates.append(              dcheck_hist_intensity  / effective_time_srun / dcheck_intensity_binning_widths)\n",
    "    \n",
    "        dcheck_delta_rates.append(np.sqrt(dcheck_hist_intensity) / effective_time_srun / dcheck_intensity_binning_widths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1da3d41-9c8c-43a7-807f-d0a9df93f8b4",
   "metadata": {},
   "source": [
    "### Correction factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dfdc14ca-4311-439a-bfdb-e447ae9c4735",
   "metadata": {},
   "outputs": [],
   "source": [
    "    ####################################\n",
    "    # Zenith correction factors to apply\n",
    "    corr_factor_p0 = geom.pol2(1, p0a, p0b, p0c) / geom.pol2(np.cos(np.deg2rad(dict_dchecks[run_number][\"pointing\"][\"zd\"])), p0a, p0b, p0c)\n",
    "    corr_factor_p1 = geom.pol2(1, p1a, p1b, p1c) - geom.pol2(np.cos(np.deg2rad(dict_dchecks[run_number][\"pointing\"][\"zd\"])), p1a, p1b, p1c)\n",
    "    \n",
    "    # Zenith correction of the reference (putting the reference in the zenith of the determined subrun)\n",
    "    corr_ref_p0 = ref_p0 / corr_factor_p0\n",
    "    corr_ref_p1 = ref_p1 - corr_factor_p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6c93261-d9f7-427c-8c6c-bd53930d0ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    ####################################################\n",
    "    # Parameters to input to the find_scaling() function\n",
    "    other_parameters = {\n",
    "        \"srun_numbers\" : srun_numbers,\n",
    "        \"dict_dchecks\" : dict_dchecks,\n",
    "        \"ref_intensity\" : ref_intensity,\n",
    "        \"dcheck_intensity_binning\" : dcheck_intensity_binning,\n",
    "        \"dcheck_intensity_binning_widths\" : dcheck_intensity_binning_widths,\n",
    "        \"dcheck_intensity_binning_centers\" : dcheck_intensity_binning_centers,\n",
    "        \"mask_dcheck_bins_fit\" : mask_dcheck_bins_fit,\n",
    "        \"corr_factor_p0\" : corr_factor_p0, \"corr_factor_p1\" : corr_factor_p1,\n",
    "        \"root_sub_dl1\" : root_sub_dl1,\n",
    "        \"dir_dl1b_scaled\" : dir_dl1b_scaled,\n",
    "        \"limits_intensity\" : limits_intensity,\n",
    "        \"limits_intensity_extended\" : limits_intensity_extended,\n",
    "        \"config_file\" : config_file,\n",
    "        \"ref_p0\" : ref_p0, \"ref_p1\" : ref_p1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00956a5-5068-4af3-86be-765e4dcd4bb8",
   "metadata": {},
   "source": [
    "### Applying the function over the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e46e595f-0944-44e9-b0ab-708f7153723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #######################\n",
    "    # Reading original data\n",
    "    # The main results dictionary\n",
    "    dict_results = dict_results_empty.copy()\n",
    "    # First filling the dictionary with ones in the scaled values\n",
    "    # and saving the number of events stored in each subrun.\n",
    "    for srun in srun_numbers:\n",
    "        dict_results[\"scaled\"][\"original\"][srun] = 1.0\n",
    "        dict_results[\"statistics\"][srun] = int(np.sum(dcheck_hist_intensities[srun]))\n",
    "    \n",
    "    # Then we read these files and perform the fits\n",
    "    dict_results = utils.find_scaling(\n",
    "        iteration_step=\"original\", dict_results=dict_results, other_parameters=other_parameters, simulated=simulate_data\n",
    "    )\n",
    "    \n",
    "    # Then filling the next step \"scaled\" with the calculated one\n",
    "    for srun in srun_numbers:\n",
    "        dict_results[\"scaled\"][\"upper\"][srun] = dict_results[\"scaling\"][\"original\"][srun]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36993d84",
   "metadata": {},
   "source": [
    "### Performing the first scaling for first run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a356f9a4-8203-4cb9-a50a-1d14e6b5630d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing subrun 0\n",
      "Running lstchain_dl1ab... scale: 1.00\n",
      "lstchain_dl1ab --input-file /fefs/aswg/data/real/DL1/20210216/v0.9/tailcut84/dl1_LST-1.Run03708.0000.h5 --output-file /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/objects/sub_dl1/tmp_dl1_srunwise_run3708_srun0_upper_scaled.h5 --config /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/config/standard_config.json --no-image --light-scaling 1.0036045707865267 --intensity-range 60.00,562.00\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "``/fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/objects/sub_dl1/tmp_dl1_srunwise_run3708_srun0_upper_scaled.h5`` does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m###################################\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Then performing the upper scaling\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m dict_results \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_scaling\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43miteration_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdict_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mother_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimulated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimulate_data\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/script_utils_scaling.py:262\u001b[0m, in \u001b[0;36mfind_scaling\u001b[0;34m(iteration_step, dict_results, other_parameters, simulated)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m#################################################################\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# Reading the dl1 file\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m#################################################################\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m simulated:\n\u001b[0;32m--> 262\u001b[0m     table_data \u001b[38;5;241m=\u001b[39m \u001b[43mtables\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_output_fname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m     data_counts_intensity, _ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhistogram(\n\u001b[1;32m    264\u001b[0m         table_data\u001b[38;5;241m.\u001b[39mroot\u001b[38;5;241m.\u001b[39mdl1\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mtelescope\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mLST_LSTCam\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintensity\u001b[39m\u001b[38;5;124m\"\u001b[39m), \n\u001b[1;32m    265\u001b[0m         bins\u001b[38;5;241m=\u001b[39mdcheck_intensity_binning\n\u001b[1;32m    266\u001b[0m     )\n\u001b[1;32m    267\u001b[0m     table_data\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/tables/file.py:294\u001b[0m, in \u001b[0;36mopen_file\u001b[0;34m(filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    290\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is already opened.  Please \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose it before reopening in write mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m filename)\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# Finally, create the File instance, and return it\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_uep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/tables/file.py:744\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams \u001b[38;5;241m=\u001b[39m params\n\u001b[1;32m    743\u001b[0m \u001b[38;5;66;03m# Now, it is time to initialize the File extension\u001b[39;00m\n\u001b[0;32m--> 744\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_g_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;66;03m# Check filters and set PyTables format version for new files.\u001b[39;00m\n\u001b[1;32m    747\u001b[0m new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_v_new\n",
      "File \u001b[0;32m/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/tables/hdf5extension.pyx:397\u001b[0m, in \u001b[0;36mtables.hdf5extension.File._g_new\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/tables/utils.py:146\u001b[0m, in \u001b[0;36mcheck_file_access\u001b[0;34m(filename, mode)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# The file should be readable.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39maccess(path, os\u001b[38;5;241m.\u001b[39mF_OK):\n\u001b[0;32m--> 146\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m``\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`` does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIsADirectoryError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m``\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`` is not a regular file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: ``/fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/objects/sub_dl1/tmp_dl1_srunwise_run3708_srun0_upper_scaled.h5`` does not exist"
     ]
    }
   ],
   "source": [
    "    ###################################\n",
    "    # Then performing the upper scaling\n",
    "    dict_results = utils.find_scaling(\n",
    "        iteration_step=\"upper\", dict_results=dict_results, other_parameters=other_parameters, simulated=simulate_data\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d17364d-986d-4c6b-b5ff-00fb03672928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36a97809-5f00-4aef-8d13-bf5a4e8665d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pedestal cleaning\n",
      "Fraction of Cat_A pixel cleaning thresholds above Cat_A picture thr.:0.030\n",
      "Tailcut clean with pedestal threshold config used:{'picture_thresh': 8, 'boundary_thresh': 4, 'sigma': 2.5, 'keep_isolated_pixels': False, 'min_number_picture_neighbors': 2, 'use_only_main_island': False, 'delta_time': 2}\n",
      "Using dynamic cleaning for events with average size of the 3 most brighest pixels > 267 p.e\n",
      "Remove from image pixels which have charge below = 0.03 * average size\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!lstchain_dl1ab \\\n",
    "--input-file /fefs/aswg/data/real/DL1/20210216/v0.9/tailcut84/dl1_LST-1.Run03708.0000.h5 \\\n",
    "--output-file /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/objects/sub_dl1/tmp_dl1_srunwise_run3708_srun0_upper_scaled.h5 \\\n",
    "--config /fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/config/standard_config.json \\\n",
    "--no-image \\\n",
    "--light-scaling 1.0036045707865267 \\\n",
    "--intensity-range 60.00,562.00\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c051af8a-3649-416c-824f-3c3e284a2647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94acdce6-579d-49c4-850a-6c3350199fdf",
   "metadata": {},
   "source": [
    "### Performing linear interpolation to calculate the new scaling factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cfdcc8-1785-4637-9b25-f03e8c048dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #####################################################\n",
    "    # Calculating the linear factor to the linear scaling\n",
    "    for srun in srun_numbers:\n",
    "    \n",
    "        # Now putting all together, upper and half\n",
    "        points_scaling     = np.array([1, dict_results[\"scaling\"][\"original\"][srun]])\n",
    "        points_light_yield = np.array([dict_results[\"light_yield\"][\"original\"][srun], dict_results[\"light_yield\"][\"upper\"][srun]])\n",
    "    \n",
    "        # Finding the final scaling as a line that pass trogh the two points we have\n",
    "        # Then we calculate where the light yield will be 1 in linear approximation\n",
    "        slope = (points_light_yield[1] - points_light_yield[0]) / (points_scaling[1] - points_scaling[0])\n",
    "        intercept = points_light_yield[0] - slope * points_scaling[0]\n",
    "        linear_scale_factor = 1 / slope - points_light_yield[0] / slope + points_scaling[0]\n",
    "    \n",
    "        dict_results[\"scaled\"][\"linear\"][srun] = linear_scale_factor\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e722a6f6-a225-4aeb-b4da-b9bc74991b58",
   "metadata": {},
   "source": [
    "### Applying the linear factor to the data and then re-calculating the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7d80bf-ee73-4212-83aa-fc644462f171",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    ##########################################\n",
    "    # Then applying this linear scaling factor\n",
    "    dict_results = utils.find_scaling(\n",
    "        iteration_step=\"linear\", dict_results=dict_results, other_parameters=other_parameters, simulated=simulate_data\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe1b52-20ad-4f90-a4f8-b9ac57bac4b3",
   "metadata": {},
   "source": [
    "### Then calculate the final light yield and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f4aa2c-6e14-4fd8-bd75-5417457b1c40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    ###################################################\n",
    "    # And finally calculating the final scaling factors\n",
    "    for srun in srun_numbers:\n",
    "    \n",
    "        # Only calculating for the cases with no flag errors:\n",
    "        if not dict_results[\"flag_error\"][srun]:\n",
    "    \n",
    "            # Now putting all together, upper and half\n",
    "            points_scaling           = np.array([dict_results[\"scaled\"][key][srun]            for key in [\"original\", \"linear\", \"upper\"]])\n",
    "            points_light_yield       = np.array([dict_results[\"light_yield\"][key][srun]       for key in [\"original\", \"linear\", \"upper\"]])\n",
    "            points_delta_light_yield = np.array([dict_results[\"delta_light_yield\"][key][srun] for key in [\"original\", \"linear\", \"upper\"]])\n",
    "            \n",
    "            if simulate_data:\n",
    "                points_scaling           = np.array([1, 1.2, 1.4])       + np.random.rand(3) * 0.1\n",
    "                points_light_yield       = np.array([0.7, 0.9, 1.2])     + np.random.rand(3) * 0.1\n",
    "                points_delta_light_yield = np.array([0.05, 0.05, 0.05])  + np.random.rand(3) * 0.01        \n",
    "                \n",
    "            srun_a, srun_b, srun_c, srun_delta_a, srun_delta_b, srun_delta_c = geom.parabola_3points(\n",
    "                *points_scaling, *points_light_yield, *points_delta_light_yield\n",
    "            )\n",
    "            \n",
    "            range_avg_point = np.mean(points_scaling)\n",
    "            x0, delta_x0 = geom.get_roots_pol2(\n",
    "                range_avg_point, 1,*points_scaling, *points_light_yield, *points_delta_light_yield\n",
    "            )\n",
    "    \n",
    "            final_scale_factor = x0\n",
    "            delta_final_scale_factor = delta_x0\n",
    "    \n",
    "        else:\n",
    "            final_scale_factor = np.nan\n",
    "            delta_final_scale_factor = np.nan\n",
    "            \n",
    "        dict_results[\"final_scaling\"][srun] = final_scale_factor\n",
    "        dict_results[\"delta_final_scaling\"][srun] = delta_final_scale_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00db2838-e2fb-464b-a6de-f6db10d3b44c",
   "metadata": {},
   "source": [
    "## Storing the dictionary with the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8696f0c6-e740-4d20-837e-4af94cdf8981",
   "metadata": {},
   "outputs": [],
   "source": [
    "    ##############################\n",
    "    # Storing data in a pkl object\n",
    "    dict_fname = os.path.join(root_results, f\"results_job_{input_str}.pkl\")\n",
    "    \n",
    "    # Saving the objects\n",
    "    with open(dict_fname, \"wb\") as f:\n",
    "        pickle.dump(dict_results, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# # Reading the object\n",
    "# with open(dict_fname, 'rb') as f:\n",
    "#     dict_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ce59b4-27d4-4aee-b00c-0e62654e7340",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
