{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e70776f7-e2e1-4057-b7ef-878b2be0556d",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b3b4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from datetime import datetime\n",
    "import pickle, json, sys, os, glob\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import chi2\n",
    "from scipy import optimize\n",
    "import subprocess\n",
    "\n",
    "from astropy.coordinates     import SkyCoord\n",
    "from lstchain.io.config      import get_standard_config\n",
    "from ctapipe.io              import read_table\n",
    "import tables\n",
    "\n",
    "# Other auxiliar scripts\n",
    "sys.path.insert(0, os.getcwd() + \"/../scripts/\")\n",
    "import auxiliar as aux\n",
    "import geometry as geom\n",
    "import lstpipeline\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cff6f04",
   "metadata": {},
   "source": [
    "### Configuration and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d56ff173",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Source name in order to just complete the results file, and\n",
    "in order to improve run organization.\"\"\"\n",
    "source_name = \"crab\"\n",
    "\n",
    "\"\"\" Fit parameters\n",
    "Chosen limits in intensity (p.e.) for applying the fit i.e. the\n",
    "power law will be fitted only with the points within this range.\"\"\"\n",
    "limits_intensity = [316, 562]\n",
    "\"\"\" For the positive scaling cases (most of them), we need to have a lower \n",
    "limit in intensity. Thi slimit is used for the subset of events that are \n",
    "scaled just to find which is the scaling value. We use a very low limit by\n",
    "default 60 p.e. compared to the lower limit of the fit 316 p.e. because in \n",
    "the worst cases we will have a very non-linear scaling that will displace \n",
    "significantly the events intensities.\"\"\"\n",
    "limits_intensity_extended = 60\n",
    "\n",
    "\"\"\" Power law parameters for the reference\n",
    "All these parameters are taken from a common analysis of the full dataset\n",
    "Where the period of end of 2022 and start 2023 is taken as reference for good \n",
    "runs. Then we take as reference the mean power law parameters in that period.\n",
    "p0 is the normalization factor and p1 is the slope.\"\"\"\n",
    "ref_p0 =  1.74 \n",
    "ref_p1 = -2.23\n",
    "\n",
    "\"\"\" Threshold in statistics for the last subrun\n",
    "The limit in number of events after cleaning that we need to consider the last\n",
    "subrun has enough statistics to perform the analysis over it. Otherwise the \n",
    "values of the scaling that will be applied to this last rubrun are the same \n",
    "that are applied to the last last subrun.\"\"\"\n",
    "statistics_threshold = 10000\n",
    "\n",
    "\"\"\" Parameters for the empyrical fits for Zenith Distance corrections\n",
    "Are simply two 2 degree polynomials for each variable of the power law.\"\"\"\n",
    "p0a, p0b, p0c = -0.44751321, 3.62502037, -1.43611437\n",
    "p1a, p1b, p1c = -2.89253919, 0.99443581, -0.34013068\n",
    "\n",
    "# Standard paths for data in the IT cluster ---------\n",
    "root_dl1 = \"/fefs/aswg/data/real/DL1/*/v0.*/tailcut84/\"\n",
    "root_rfs = \"/fefs/aswg/data/models/AllSky/20230901_v0.10.4_allsky_base_prod/\"\n",
    "root_mcs = \"/fefs/aswg/data/mc/DL2/AllSky/20230901_v0.10.4_allsky_base_prod/TestingDataset/\"\n",
    "\n",
    "# Root path of this script\n",
    "root = os.getcwd() + \"/\"\n",
    "# Path to store the configuration file we are going to use\n",
    "config_file = root + \"config/standard_config.json\"\n",
    "# Path to store objects\n",
    "root_objects = root + f\"objects/\"\n",
    "# Data main directory\n",
    "root_data = root + f\"../../data/cherenkov_transparency_corrections/{source_name}/\"\n",
    "# Sub-dl1 objects directory\n",
    "root_sub_dl1 = root_objects + \"sub_dl1/\"\n",
    "# Directory for the results of the fit of each run\n",
    "root_results = root_objects + \"results_fits/\"\n",
    "root_final_results = root_objects + \"final_results_fits/\"\n",
    "\n",
    "def configure_lstchain():\n",
    "    \"\"\"Creates a file of standard configuration for the lstchain analysis. \n",
    "    It can be changed inside this function\"\"\"\n",
    "    dict_config = get_standard_config()\n",
    "    # We select the heuristic flatfield option in the standard configuration\n",
    "    dict_config[\"source_config\"][\"LSTEventSource\"][\"use_flatfield_heuristic\"] = True\n",
    "    with open(config_file, \"w\") as json_file:\n",
    "        json.dump(dict_config, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9097b0-d17b-4cee-bfa7-7eef02a43982",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d78b4c0f-acbd-4740-ae6c-75e0a5d1789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the stored dictionaries that are inside the results folder\n",
    "dict_files = np.sort(glob.glob(root_results + \"*.pkl\"))\n",
    "\n",
    "# Storing all the run numbers and all the separate dictionaries\n",
    "total_runs, dictionaries = [], []\n",
    "for file in dict_files:\n",
    "\n",
    "    # Reading the dictionaries using pickle\n",
    "    with open(file, 'rb') as f:\n",
    "        tmp_dict = pickle.load(f)\n",
    "\n",
    "    total_runs.append(int(file.split(\"/\")[-1].split(\"_\")[2]))\n",
    "    dictionaries.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86f40d2-3c68-4de8-85ab-31b0721ec105",
   "metadata": {},
   "source": [
    "### Cleaning some of the directories, that have been filled with not tmp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d2e9f3d-493a-46c3-90ef-610dd64b474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all the entries in the directory\n",
    "for dir_to_delete in [root_sub_dl1]:\n",
    "    for entry in os.listdir(dir_to_delete):\n",
    "        entry_path = os.path.join(dir_to_delete, entry)\n",
    "    \n",
    "        # Check if it's a file and delete it\n",
    "        if os.path.isfile(entry_path):\n",
    "            os.remove(entry_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3238eac9-15ea-4dae-bd31-2f6711b6b5c3",
   "metadata": {},
   "source": [
    "### Merging the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "096c9b61-c2f7-4d2d-9e99-6cdea446d9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only non-repeated runs\n",
    "total_runs = np.unique(total_runs)\n",
    "\n",
    "# We create a empty dict for each run we have information\n",
    "dict_runs = {}\n",
    "for run in total_runs:\n",
    "    tmp = { \n",
    "        \"run\": run, \"filenames\": {}, \"statistics\": {}, \"flag_error\" : {},\n",
    "        \"scaled\" :           {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"p0\":                {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"delta_p0\":          {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"p1\":                {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"delta_p1\":          {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"chi2\":              {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"pvalue\":            {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"light_yield\":       {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"delta_light_yield\": {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"scaling\":           {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"delta_scaling\":     {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"scaling_percent\":       {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"delta_scaling_percent\": {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"final_scaling\": {}, \"final_scaling_interpolated\": {}, \"interpolation\" : {},\n",
    "    }\n",
    "    dict_runs[run] = tmp\n",
    "\n",
    "def merge_dicts(dict1, dict2):\n",
    "    \"\"\"\n",
    "    Recursively merge two dictionaries with nested structures.\n",
    "\n",
    "    Args:\n",
    "    - dict1: First dictionary\n",
    "    - dict2: Second dictionary\n",
    "\n",
    "    Returns:\n",
    "    - Merged dictionary\n",
    "    \"\"\"\n",
    "    merged = dict1.copy()\n",
    "\n",
    "    for key, value in dict2.items():\n",
    "        if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):\n",
    "            # If both values are dictionaries, recursively merge them\n",
    "            merged[key] = merge_dicts(merged[key], value)\n",
    "        else:\n",
    "            # Otherwise, just update or add the key-value pair\n",
    "            merged[key] = value\n",
    "    return merged\n",
    "    \n",
    "# Now we fill this dicts one by one with the empty one\n",
    "for d in dictionaries:\n",
    "    run = d[\"run\"]\n",
    "    dict_runs[run] = merge_dicts(dict_runs[run], d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e12f88a-5773-4bdc-8990-994674337646",
   "metadata": {},
   "source": [
    "### Checking statistics subrunwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6a07a95-539c-4bb3-ab1b-40e4a6798f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "For run 2932 subrun 65:\n",
      "N_events = 3731\n",
      "Flag_error = False\n",
      "So interpolating neighbors.\n",
      "\n",
      "For run 2932 subrun 67:\n",
      "N_events = 8073\n",
      "Flag_error = False\n",
      "So interpolating neighbors.\n",
      "\n",
      "For run 2932 subrun 68:\n",
      "N_events = 3801\n",
      "Flag_error = False\n",
      "So interpolating neighbors.\n",
      "\n",
      "For run 2932 subrun 70:\n",
      "N_events = 5963\n",
      "Flag_error = False\n",
      "So interpolating neighbors.\n",
      "\n",
      "For run 2932 subrun 71:\n",
      "N_events = 4606\n",
      "Flag_error = False\n",
      "So interpolating neighbors.\n",
      "\n",
      "For run 2932 subrun 75:\n",
      "N_events = 4362\n",
      "Flag_error = False\n",
      "So interpolating neighbors.\n",
      "\n",
      "For run 2932 subrun 76:\n",
      "N_events = 64\n",
      "Flag_error = True\n",
      "So interpolating neighbors.\n",
      "\n",
      "For run 2932 subrun 77:\n",
      "N_events = 2998\n",
      "Flag_error = True\n",
      "So interpolating neighbors.\n",
      "\n",
      "For run 2932 subrun 78:\n",
      "N_events = 7007\n",
      "Flag_error = True\n",
      "So interpolating neighbors.\n",
      "\n",
      "For run 2932 subrun 79:\n",
      "N_events = 8918\n",
      "Flag_error = False\n",
      "So interpolating neighbors.\n",
      "\n",
      "For run 2932 subrun 81:\n",
      "N_events = 8059\n",
      "Flag_error = False\n",
      "So interpolating neighbors.\n"
     ]
    }
   ],
   "source": [
    "# We don't trust the fit for subruns with too few events or in which the fit have not suceeded\n",
    "# In tose cases we will apply as the final scaling the average with the neighbors.\n",
    "for run in dict_runs.keys():\n",
    "    # The dictionary of one run\n",
    "    dict_run = dict_runs[run]\n",
    "    # Statistics object (dict)\n",
    "    dict_stats = dict_run[\"statistics\"]\n",
    "    last_srun = max(dict_stats.keys())\n",
    "\n",
    "    # We check subrun by subrun\n",
    "    for srun in np.sort(list(dict_stats.keys())):\n",
    "        stats = dict_stats[srun]\n",
    "        flag = dict_run[\"flag_error\"][srun]\n",
    "\n",
    "        if stats < statistics_threshold or flag:\n",
    "            logger.warning(f\"\\nFor run {run} subrun {srun}:\\nN_events = {stats}\\nFlag_error = {flag}\\nSo interpolating neighbors.\")\n",
    "\n",
    "            # Search for the right neighbor until a valid one is found\n",
    "            right_neighbor = srun + 1\n",
    "            while right_neighbor <= last_srun and (dict_run[\"statistics\"][right_neighbor] < statistics_threshold or dict_run[\"flag_error\"][right_neighbor]):\n",
    "                right_neighbor += 1\n",
    "\n",
    "            # Search for the left neighbor until a valid one is found\n",
    "            left_neighbor = srun - 1\n",
    "            while left_neighbor >= 0 and (dict_run[\"statistics\"][left_neighbor] < statistics_threshold or dict_run[\"flag_error\"][left_neighbor]):\n",
    "                left_neighbor -= 1\n",
    "\n",
    "            # Case of two invalid subruns in a row\n",
    "            if right_neighbor <= last_srun and left_neighbor >= 0:\n",
    "                dict_runs[run][\"final_scaling\"][srun] = (dict_runs[run][\"final_scaling\"][left_neighbor] + dict_runs[run][\"final_scaling\"][right_neighbor]) / 2\n",
    "            elif right_neighbor <= last_srun:\n",
    "                dict_runs[run][\"final_scaling\"][srun] = dict_runs[run][\"final_scaling\"][right_neighbor]\n",
    "            elif left_neighbor >= 0:\n",
    "                dict_runs[run][\"final_scaling\"][srun] = dict_runs[run][\"final_scaling\"][left_neighbor]\n",
    "            else:\n",
    "                logger.warning(f\"No valid neighbors found for run {run} subrun {srun}. Unable to interpolate.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b32e12",
   "metadata": {},
   "source": [
    "# Finding the files that interest us\n",
    "#### Extracting dl1 files and dl1 datachecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e95eb908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding dl1  data to dictionary (Run 2932)...\n",
      "...Finished adding dl1 data to dictionary\n",
      "\n",
      "Adding dl1 datacheck data to dictionary (Run 2932)...\n",
      "...Finished adding dl1 data to dictionary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 s, sys: 7.32 s, total: 19.2 s\n",
      "Wall time: 31.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Getting coordinates of source\n",
    "source_coords = SkyCoord.from_name(source_name)\n",
    "\n",
    "dict_source = {\n",
    "    \"name\"   : source_name,\n",
    "    \"coords\" : source_coords,\n",
    "    \"ra\"     : source_coords.ra.deg  * u.deg, # ra in degrees\n",
    "    \"dec\"    : source_coords.dec.deg * u.deg, # dec in degrees\n",
    "}\n",
    "\n",
    "# We create a empty dictionary to store all the information needed inside\n",
    "dict_dchecks = {}\n",
    "for run in total_runs:\n",
    "    dict_dchecks[run] = {\n",
    "        \"run_num\" : run,\n",
    "    }\n",
    "\n",
    "dict_dchecks = lstpipeline.add_dl1_paths_to_dict(dict_dchecks, root_dl1)\n",
    "dict_dchecks = lstpipeline.add_dl1_paths_to_dict(dict_dchecks, root_dl1, dchecking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac5fda0",
   "metadata": {},
   "source": [
    "#### Then we read the observations information and also the selected nodes for MC and RFs and we add it to the DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91f1e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for run_number in total_runs:\n",
    "    tab_dcheck_run = read_table(dict_dchecks[run_number][\"dchecks\"][\"runwise\"], \"/dl1datacheck/cosmics\")\n",
    "    \n",
    "    # reading the variables\n",
    "    dcheck_zd = 90 - np.rad2deg(np.array(tab_dcheck_run[\"mean_alt_tel\"]))\n",
    "    dcheck_az = np.rad2deg(np.array(tab_dcheck_run[\"mean_az_tel\"]))\n",
    "    dcheck_tstart = tab_dcheck_run[\"dragon_time\"][0][0]\n",
    "    dcheck_telapsed = np.array(tab_dcheck_run[\"elapsed_time\"])\n",
    "\n",
    "    dict_dchecks[run_number][\"time\"] = {\n",
    "        \"tstart\"   : dcheck_tstart,            # datetime object\n",
    "        \"telapsed\" : np.sum(dcheck_telapsed),  # s\n",
    "        \"srunwise\" : {\n",
    "            \"telapsed\" : dcheck_telapsed,      # s      \n",
    "        },\n",
    "    }\n",
    "    dict_dchecks[run_number][\"pointing\"] = {\n",
    "        \"zd\" : np.mean(dcheck_zd),  # deg\n",
    "        \"az\" : np.mean(dcheck_az),  # deg\n",
    "        \"srunwise\" : {\n",
    "            \"zd\" : dcheck_zd, # deg\n",
    "            \"az\" : dcheck_az, # deg\n",
    "        },\n",
    "    }\n",
    "    \n",
    "# then we also select the RFs and MC files looking at the nodes available\n",
    "dict_dchecks, dict_nodes = lstpipeline.add_mc_and_rfs_nodes(dict_dchecks, root_rfs, root_mcs, dict_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1d0e4d-a713-4a6d-b704-7c623de3c559",
   "metadata": {},
   "source": [
    "### Then we interpolate the lineal scalings with a linear behaviour trough the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf97dc25-4ea6-4495-9603-428e6718e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for run_number in total_runs:\n",
    "\n",
    "    dict_results = dict_runs[run_number]\n",
    "\n",
    "    x_fit = np.cumsum(dict_dchecks[run_number][\"time\"][\"srunwise\"][\"telapsed\"])\n",
    "    y_fit = np.array([dict_results[\"final_scaling\"][srun] for srun in np.sort(list(dict_results[\"final_scaling\"].keys()))])\n",
    "\n",
    "    # Performing the fit\n",
    "    params, pcov, info, _, _ = curve_fit(\n",
    "        f     = geom.straight_line,\n",
    "        xdata = x_fit,\n",
    "        ydata = y_fit,\n",
    "        p0    = [1, 0],\n",
    "        full_output = True,\n",
    "    )\n",
    "        \n",
    "    intercept       = params[0]\n",
    "    slope           = params[1]\n",
    "    delta_intercept = np.sqrt(pcov[0, 0])\n",
    "    delta_slope     = np.sqrt(pcov[1, 1])\n",
    "    _chi2           = np.sum(info['fvec'] ** 2)\n",
    "    pvalue          = 1 - chi2.cdf(_chi2, len(x_fit))\n",
    "    \n",
    "    dict_results[\"interpolation\"] = {\n",
    "        \"chi2\" : _chi2,      \n",
    "        \"p_value\" : pvalue,         \n",
    "        \"slope\": slope,      \n",
    "        \"delta_slope\" : delta_slope,     \n",
    "        \"intercept\" : intercept, \n",
    "        \"delta_intercept\" : delta_intercept,\n",
    "    }\n",
    "    \n",
    "    # #############################################\n",
    "    # #############################################\n",
    "    # fig, ax = plt.subplots(1, 1, figsize=(10, 2))\n",
    "    \n",
    "    # ax.plot(x_fit/60, ((intercept + x_fit * slope) - 1) * 100, color=\"k\", ls=\"--\", zorder=10, label=\"Interpolation\")\n",
    "    \n",
    "    # ax.plot(x_fit/60, (y_fit - 1) * 100, 'r', label='Data')\n",
    "    \n",
    "    # ax.set_xlabel(\"Time elapsed [min]\")\n",
    "    # ax.set_ylabel(\"Scaling factor [%]\")\n",
    "    # ax.legend(ncols=2, frameon=False)\n",
    "    \n",
    "    # plt.show()\n",
    "    # #############################################\n",
    "    # #############################################\n",
    "    \n",
    "    # Setting a interpolated scaling factor\n",
    "    for srun in dict_results[\"final_scaling\"].keys():\n",
    "        \n",
    "        scaling_interpolated = intercept + slope * x_fit[srun]\n",
    "        \n",
    "        dict_results[\"final_scaling_interpolated\"][srun] = scaling_interpolated\n",
    "        dict_results[\"scaled\"][\"final\"][srun]            = scaling_interpolated\n",
    "\n",
    "    dict_fname = root_final_results + f\"results_job_{run_number}.pkl\"\n",
    "    \n",
    "    # Saving the object\n",
    "    with open(dict_fname, 'wb') as f:\n",
    "        pickle.dump(dict_results, f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
