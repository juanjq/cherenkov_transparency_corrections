{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6885968a-e940-477e-988b-b6324d56935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle, sys, os, json\n",
    "\n",
    "# Other auxiliar scripts\n",
    "sys.path.insert(0, os.getcwd() + \"/../scripts/\")\n",
    "import lstpipeline\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\"\"\"Number of subruns in one job\"\"\"\n",
    "n_subruns_job = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973492b9-e9ac-4618-b7a7-14085fad75be",
   "metadata": {},
   "source": [
    "### Paths and filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996e1c08-8c6f-4ebe-8765-e1911abda839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root path of this script\n",
    "root = os.getcwd() + \"/\"\n",
    "# We need to create also a folder to store the slurm outputs\n",
    "root_slurm = root + \"objects/output_slurm\"\n",
    "# Path to store the configuration file we are going to use\n",
    "root_config = root + \"config/\"\n",
    "\n",
    "file_job_config = root_config + \"job_config_runs.txt\"\n",
    "\n",
    "# STANDARD paths ---------\n",
    "root_dl1 = \"/fefs/aswg/data/real/DL1/*/v0.*/tailcut84/\"\n",
    "root_rfs = \"/fefs/aswg/data/models/AllSky/20230901_v0.10.4_allsky_base_prod/\"\n",
    "root_mcs = \"/fefs/aswg/data/mc/DL2/AllSky/20230901_v0.10.4_allsky_base_prod/TestingDataset/\"\n",
    "\n",
    "# Create the paths that do not exist\n",
    "for path in [root_config, root_slurm]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(os.path.join(path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a16050c-b0c4-4fa9-b28e-3025f01aad16",
   "metadata": {},
   "source": [
    "### Run numbers we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "005b0c00-ec5e-4b43-a5f0-775efa84c1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing for 1 runs\n"
     ]
    }
   ],
   "source": [
    "runs_performance_paper = [\n",
    "       2914, 2929, 2930, 2931, 2932, 2933, 2934, 2949, 2950, 2967, 2968,\n",
    "       2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2988, 2989,\n",
    "       2990, 2991, 2992, 3004, 3005, 3006, 3007, 3008, 3093, 3094, 3095,\n",
    "       3096, 3231, 3232, 3243, 3270, 3271, 3272, 3273, 3274, 3275, 3276,\n",
    "       3277, 3278, 3279, 3318, 3319, 3320, 3321, 3328, 3329, 3330, 3338,\n",
    "       3339, 3340, 3355, 3356, 3373, 3598, 3599, 3600, 3601, 3615, 3632,\n",
    "       3633, 3634, 3635, 3672, 3673, 3674, 3675, 3676, 3677, 3706, 3707,\n",
    "       3708, 4067, 4068, 4086, 4087, 6045, 6073, 6304, 6872, 6873, 6874,\n",
    "       6875, 6892, 6893, 6894, 6895, 7097, 7098, 7099, 7133, 7136, 7161,\n",
    "       7195, 7196, 7197, 7199, 7200, 7227, 7228, 7231, 7232, 7233, 7253,\n",
    "       7254, 7255, 7256, 7274, 7275, 7276, 7277\n",
    "]\n",
    "runs_performance_paper = [\n",
    "       2914, 2929, 2930, 2931, 2932, 2933, 2934, 2949, 2950, 2967, 2968,\n",
    "       2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2988, 2989,\n",
    "       2990, 2991, 2992, 3004, 3005, 3006, 3007, 3008, 3093, 3094, 3095,\n",
    "       3096, 3231, 3232, 3243, 3270, 3271, 3272, 3273, 3274, 3275, 3276,\n",
    "       3277, 3278, 3279, 3318, 3319, 3320, 3321, 3328, 3329, 3330, 3338,\n",
    "       3339, 3340, 3355, 3356, 3373, 3598, 3599, 3600, 3601, 3615, 3632,\n",
    "       3633, 3634, 3635\n",
    "]\n",
    "\n",
    "# runs_performance_paper = [\n",
    "#        2914, 2929, 2930, 2931, 2932, 2933, 2934, 2949, 2950, 2967, 2968,\n",
    "#        2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2988, 2989,\n",
    "#        2990, 2991, 2992, \n",
    "# ]\n",
    "\n",
    "runs_good_period = [\n",
    "       10668, 10671, 10672, 10673, 10674, 10917, 10950, 11088, 11125,\n",
    "       11166, 11191, 11192, 11193, 11196, 11197, 11208, 11209, 11219,\n",
    "       11221, 11222, 11224, 11225, 11228, 11229, 11230, 11231, 11237,\n",
    "       11238, 11239, 11240, 11241, 11243, 11244, 11245, 11246, 11247,\n",
    "       11249, 11250, 11251, 11252, 11254, 11257, 11258, 11259, 11260,\n",
    "       11261, 11262, 11263, 11264, 11265, 11266, 11267, 11268, 11269,\n",
    "       11270, 11271, 11272, 11273, 11276, 11277, 11278, 11280, 11282,\n",
    "       11354, 11355, 11357, 11358, 11359, 11360, 11361, 11363, 11378,\n",
    "       11379, 11380, 11381, 11382, 11383, 11384, 11408, 11409, 11410,\n",
    "       11439, 11609, 11610, 11634, 11649, 11650, 11651, 11652, 11671,\n",
    "       11676, 11711, 11712, 11834, 11919, 11920, 11930\n",
    "]\n",
    "\n",
    "runs_other_good = [ \n",
    "        2758,  2759,  3088,  3098,  3584,  3586,  3683,  3705,  3725,\n",
    "        3894,  4010,  4011,  4015,  4129,  5955,  5957,  5958,  5992,\n",
    "        5993,  5995,  6011,  6282,  7084,  7087,  7140,  7142,  7143,\n",
    "        7169,  7170,  7172,  7174,  7201,  9252,  9274,  9434,  9436,\n",
    "        9596,  9687,  9689,  9996, 10034, 10035, 10083, 10084, 10085,\n",
    "       10089, 10090, 10590, 10591, 10592, 10593, 10595, 10596, 10597,\n",
    "       10599, 10600, 10601, 10602, 10603, 10604, 10630, 10633, 10634,\n",
    "       10635, 10636, 10637, 10638, 12048, 12077, 12291, 12669, 12742,\n",
    "       12767, 14629, 14670, 15570, 15571, 15572, 15641, 15727, 15780,\n",
    "       15969, 15971, 16001, 16052, 16111, 16237, 16238, 16286, 16337,\n",
    "       16409\n",
    "]\n",
    "\n",
    "runs_bad_rates_good_weather = [ 9686, 10258, 10260, 10262, 10263]\n",
    "\n",
    "runs_bad_rates_bad_weather  = [\n",
    "        2767,  5738,  5800,  6192,  6194,  6852,  6853,  6962,  6963,\n",
    "        6964,  6965,  6966,  6990,  9253,  9715,  9716,  9882, 10077,\n",
    "       10078, 10264, 10526, 10527, 10528\n",
    "]\n",
    "\n",
    "# Selecting the runs we want to analyse\n",
    "runs = runs_performance_paper[:1]\n",
    "\n",
    "print(f\"Computing for {len(runs)} runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fe0776-759e-4307-96a4-df344b013331",
   "metadata": {},
   "source": [
    "### Reading some of the information in the datachecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11606f13-cdbb-4c0c-ae4b-63037b25522c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding dl1  data to dictionary (Run 2914)...\n",
      "...Finished adding dl1 data to dictionary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.12 s, sys: 2.37 s, total: 8.49 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We create a empty dictionary to store all the information needed inside\n",
    "dict_dchecks = {}\n",
    "for run in runs:\n",
    "    dict_dchecks[run] = {\n",
    "        \"run_num\" : run,\n",
    "    }\n",
    "\n",
    "dict_dchecks = lstpipeline.add_dl1_paths_to_dict(dict_dchecks, root_dl1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cda4d48-d3e6-40b4-895c-9c46e29f96fc",
   "metadata": {},
   "source": [
    "### For each runs having a set of subruns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "222144d5-def3-4188-baf7-8874965a05b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_run_sruns = {}\n",
    "for run in runs:\n",
    "    fnames_dl1 = np.sort(dict_dchecks[run][\"dl1a\"][\"srunwise\"])\n",
    "    srun_numbers = [int(f.split(\".\")[-2]) for f in fnames_dl1]\n",
    "    dict_run_sruns[run] = srun_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344eba4b-68d5-4fa2-887a-99d1b0ef488c",
   "metadata": {},
   "source": [
    "### Storing the jobs in sets of certain amount of subruns inside the same job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67f077d7-3fd3-41fb-ac32-48d9eca6874c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final amount of jobs is 23\n"
     ]
    }
   ],
   "source": [
    "n_jobs = 0\n",
    "with open(file_job_config, \"w\") as file:\n",
    "\n",
    "    for run in runs:\n",
    "    \n",
    "        count_sruns = 0\n",
    "        sruns = np.sort(dict_run_sruns[run])\n",
    "    \n",
    "        tmp_str = \"\"\n",
    "        for srun in sruns:\n",
    "            \n",
    "            tmp_str = tmp_str + f\"_{srun}\"\n",
    "    \n",
    "            # Launching a certain amount of subruns together\n",
    "            if (count_sruns % n_subruns_job == 0 and srun != 0) or (srun == max(sruns)):\n",
    "\n",
    "                tmp_str_splitted = tmp_str.split(\"_\")\n",
    "                if len(tmp_str_splitted) != 2:\n",
    "                    tmp_str = \"_\" + tmp_str_splitted[1] + \"_\" + tmp_str_splitted[-1]\n",
    "                \n",
    "                file.write(f\"{run}{tmp_str}\\n\")\n",
    "                tmp_str = \"\"\n",
    "                n_jobs += 1\n",
    "    \n",
    "            count_sruns += 1\n",
    "print(f\"The final amount of jobs is {n_jobs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8a431b-c3bf-4d52-905f-6eb21ab44c67",
   "metadata": {},
   "source": [
    "# Generating the configuration file\n",
    "## Configure here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17746a9a-cb87-419a-8c6d-bc40272b7833",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_config_scaling = os.path.join(\"config\", \"config_scaling_parameters.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5df362-0e8b-4d98-b883-929c7b975a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Source name in order to just complete the results file, and in order to improve run organization.\"\"\"\n",
    "source_name = \"crab\"\n",
    "\n",
    "\"\"\" Fit parameters\n",
    "Chosen limits in intensity (p.e.) for applying the fit i.e. the power law will be fitted only with the points within this range.\"\"\"\n",
    "limits_intensity = [316, 562]\n",
    "\"\"\" For the positive scaling cases (most of them), we need to have a lower  limit in intensity. Thi slimit is used for the subset of \n",
    "events that are scaled just to find which is the scaling value. We use a very low limit by default 60 p.e. compared to the lower \n",
    "limit of the fit 316 p.e. because in the worst cases we will have a very non-linear scaling that will displace significantly the \n",
    "events intensities.\"\"\"\n",
    "limits_intensity_extended = 60\n",
    "\n",
    "\"\"\" Power law parameters for the reference\n",
    "All these parameters are taken from a common analysis of the full dataset Where the period of end of 2022 and start 2023 is \n",
    "taken as reference for good runs. Then we take as reference the mean power law parameters in that period. p0 is the \n",
    "normalization factor and p1 is the slope.\"\"\"\n",
    "ref_p0 =  1.74 \n",
    "ref_p1 = -2.23\n",
    "\n",
    "\"\"\" Threshold in statistics for the last subrun\n",
    "The limit in number of events after cleaning that we need to consider the last subrun has enough statistics to perform the \n",
    "analysis over it. Otherwise the values of the scaling that will be applied to this last rubrun are the same that are applied \n",
    "to the last last subrun.\"\"\"\n",
    "statistics_threshold = 4000\n",
    "\n",
    "\"\"\" Parameters for the empyrical fits for Zenith Distance corrections Are simply two 2 degree polynomials for each variable \n",
    "of the power law.\"\"\"\n",
    "p0a, p0b, p0c = -0.44751321, 3.62502037, -1.43611437\n",
    "p1a, p1b, p1c = -2.89253919, 0.99443581, -0.34013068\n",
    "\n",
    "# Standard paths for data in the IT cluster ---------\n",
    "root_dl1 = \"/fefs/aswg/data/real/DL1/*/v0.*/tailcut84/\"\n",
    "root_rfs = \"/fefs/aswg/data/models/AllSky/20230927_v0.10.4_crab_tuned/\"\n",
    "root_mcs = \"/fefs/aswg/data/mc/DL2/AllSky/20230927_v0.10.4_crab_tuned/TestingDataset/\"\n",
    "\n",
    "# Root path of this script\n",
    "root = os.getcwd()\n",
    "# Path to store the configuration file we are going to use\n",
    "config_file = os.path.join(root, \"config/standard_config.json\")\n",
    "# Path to store objects\n",
    "root_objects = os.path.join(root, f\"objects/\")\n",
    "# Data main directory\n",
    "root_data = os.path.join(root, f\"../../data/cherenkov_transparency_corrections/{source_name}/\")\n",
    "# Sub-dl1 objects directory\n",
    "root_sub_dl1 = os.path.join(root_objects, \"sub_dl1/\")\n",
    "# Directory for the results of the fit of each run\n",
    "root_results = os.path.join(root_objects, \"results_fits/\")\n",
    "root_final_results = os.path.join(root_objects, \"final_results_fits/\")\n",
    "# Configuration file for the job launching\n",
    "file_job_config = os.path.join(root, \"config\", \"job_config_runs.txt\")\n",
    "# File for temporal bash scripts\n",
    "file_temporal_bash = os.path.join(root, \"objects\", \"tmp_bash/\")\n",
    "\n",
    "# Directories for the data\n",
    "dir_dl1b_scaled = os.path.join(root_data, \"dl1_scaled/\")\n",
    "dir_dl1m_scaled = os.path.join(root_data, \"dl1_merged_scaled/\")\n",
    "dir_dl2_scaled = os.path.join(root_data, \"dl2_scaled/\")\n",
    "dir_dl2 = os.path.join(root_data, \"dl2/\")\n",
    "dir_dl3_scaled_base = os.path.join(root_data, \"dl3_scaled/\")\n",
    "dir_dl3_base = os.path.join(root_data, \"dl3/\")\n",
    "dir_irfs = os.path.join(root_data, \"irfs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4b5404-7389-43d6-9d54-bf5db2bd73bd",
   "metadata": {},
   "source": [
    "### Insert in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deda55f-9031-462b-b243-17426f855660",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration_dictionary = {\n",
    "  \"source_name\": source_name,\n",
    "  \"fit_parameters\": {\n",
    "    \"limits_intensity\": limits_intensity,\n",
    "    \"limits_intensity_extended\": limits_intensity_extended,\n",
    "    \"ref_p0\": ref_p0, \"ref_p1\": ref_p1,\n",
    "    \"statistics_threshold\": statistics_threshold,\n",
    "    \"p0a\": p0a, \"p0b\": p0b, \"p0c\": p0c,\n",
    "    \"p1a\": p1a, \"p1b\": p1b, \"p1c\": p1c\n",
    "  },\n",
    "  \"paths\": {\n",
    "    \"root_dl1\": root_dl1,\n",
    "    \"root_rfs\": root_rfs,\n",
    "    \"root_mcs\": root_mcs,\n",
    "    \"root\": root,\n",
    "    \"config_file\": config_file,\n",
    "    \"root_objects\": root_objects,\n",
    "    \"root_data\": root_data,\n",
    "    \"root_sub_dl1\": root_sub_dl1,\n",
    "    \"root_results\": root_results,\n",
    "    \"root_final_results\": root_final_results,\n",
    "    \"file_job_config\": file_job_config,\n",
    "    \"file_temporal_bash\": file_temporal_bash,\n",
    "    \"dir_dl1b_scaled\": dir_dl1b_scaled,\n",
    "    \"dir_dl1m_scaled\": dir_dl1m_scaled,\n",
    "    \"dir_dl2_scaled\": dir_dl2_scaled,\n",
    "    \"dir_dl2\": dir_dl2,\n",
    "    \"dir_dl3_scaled_base\": dir_dl3_scaled_base,\n",
    "    \"dir_dl3_base\": dir_dl3_base,\n",
    "    \"dir_irfs\": dir_irfs\n",
    "  }\n",
    "}\n",
    "\n",
    "# Store in a file\n",
    "# Open the file in write mode\n",
    "with open(fname_config_scaling, \"w\") as json_file:\n",
    "    json.dump(configuration_dictionary, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbb450e2-210e-4325-8ca0-0d12e2c8f739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code to read it\n",
    "# with open(fname_config_scaling, \"r\") as json_file:\n",
    "#     configuration_dictionary = json.load(json_file)\n",
    "\n",
    "# # Now we wxtract all the variables with the same name as in the dictionary\n",
    "# source_name = configuration_dictionary[\"source_name\"]\n",
    "# for superkey in [\"fit_parameters\", \"paths\"]:\n",
    "#     for key, value in configuration_dictionary[superkey].items():\n",
    "#         globals()[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc08d0e1-d4a2-4388-a201-776775a9e4bb",
   "metadata": {},
   "source": [
    "# Generate the IRFS\n",
    "### This will only need to be run one time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "607e5827-eab6-4597-897f-8b198eaf1ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_scripts  = \"/fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/\"\n",
    "# python_script = f\"{root_scripts}/script_1_scaling.py\"\n",
    "\n",
    "# ! python $python_script \"irfs\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
