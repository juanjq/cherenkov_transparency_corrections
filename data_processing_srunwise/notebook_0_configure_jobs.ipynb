{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e341ce7-17d0-4c8b-94e5-eff019971e13",
   "metadata": {},
   "source": [
    "# Notebook for configuring jobs and processing\n",
    "#### Fist we export the needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6885968a-e940-477e-988b-b6324d56935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle, sys, os, json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Other auxiliar scripts\n",
    "sys.path.insert(0, os.getcwd() + \"/../scripts/\")\n",
    "import lstpipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a169243c-97eb-4573-bc89-d7a6b9853aeb",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">1. Configuring file for each job to be sent</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "847b5874-4a45-40d1-a962-599134f22b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Number of subruns in one job\"\"\"\n",
    "n_subruns_job = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973492b9-e9ac-4618-b7a7-14085fad75be",
   "metadata": {},
   "source": [
    "### Paths and filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc344a61-3310-4951-b375-9c28127a2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname_config_scaling = os.path.join(\"config\", \"config_scaling_parameters.json\")\n",
    "\n",
    "# # Code to read it\n",
    "# with open(fname_config_scaling, \"r\") as json_file:\n",
    "#     configuration_dictionary = json.load(json_file)\n",
    "\n",
    "# # Now we wxtract all the variables with the same name as in the dictionary\n",
    "# source_name = configuration_dictionary[\"source_name\"]\n",
    "# for superkey in [\"fit_parameters\", \"paths\"]:\n",
    "#     for key, value in configuration_dictionary[superkey].items():\n",
    "#         globals()[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "996e1c08-8c6f-4ebe-8765-e1911abda839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root path of this script\n",
    "root = os.getcwd() + \"/\"\n",
    "# We need to create also a folder to store the slurm outputs\n",
    "root_slurm = root + \"objects/output_slurm\"\n",
    "# Path to store the configuration file we are going to use\n",
    "root_config = root + \"config/\"\n",
    "\n",
    "file_job_config = root_config + \"job_config_runs.txt\"\n",
    "\n",
    "# STANDARD paths ---------\n",
    "root_dl1 = \"/fefs/aswg/data/real/DL1/*/v0.*/tailcut84/\"\n",
    "root_rfs = \"/fefs/aswg/data/models/AllSky/20230901_v0.10.4_allsky_base_prod/\"\n",
    "root_mcs = \"/fefs/aswg/data/mc/DL2/AllSky/20230901_v0.10.4_allsky_base_prod/TestingDataset/\"\n",
    "\n",
    "# Create the paths that do not exist\n",
    "for path in [root_config, root_slurm]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(os.path.join(path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a16050c-b0c4-4fa9-b28e-3025f01aad16",
   "metadata": {},
   "source": [
    "### Run numbers we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "005b0c00-ec5e-4b43-a5f0-775efa84c1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing for 102 runs\n"
     ]
    }
   ],
   "source": [
    "runs = [2853, 2854, 2855, 2913, 2914, 2916, 2917, 2918, 2919, 2922, 2923,\n",
    "       2924, 2925, 2929, 2930, 2931, 2932, 2933, 2934, 2949, 2950, 2952,\n",
    "       2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2965, 2966,\n",
    "       2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977,\n",
    "       2988, 2989, 2990, 2991, 2992, 3004, 3005, 3006, 3007, 3008, 3009,\n",
    "       3010, 3011, 3012, 3087, 3088, 3089, 3090, 3093, 3094, 3095, 3096,\n",
    "       3097, 3098, 3099, 3100, 3101, 3143, 3144, 3169, 3170, 3171, 3231,\n",
    "       3232, 3233, 3234, 3235, 3236, 3237, 3243, 3244, 3245, 3264, 3265,\n",
    "       3266, 3267, 3268, 3269, 3270, 3271, 3272, 3273, 3274, 3275, 3276,\n",
    "       3277, 3278, 3279, 3318, 3319, 3320, 3321, 3328, 3329, 3330, 3338,\n",
    "       3339, 3340, 3355, 3356, 3373, 3382, 3383, 3384, 3385, 3514, 3515,\n",
    "       3516, 3583, 3584, 3585, 3586, 3598, 3599, 3600, 3601, 3615, 3616,\n",
    "       3631, 3632, 3633, 3634, 3635, 3636, 3637, 3638, 3639, 3649, 3650,\n",
    "       3651, 3652, 3653, 3655, 3657, 3659, 3672, 3673, 3674, 3675, 3676,\n",
    "       3677, 3678, 3679, 3680, 3681, 3682, 3683, 3684, 3703, 3704, 3705,\n",
    "       3706, 3707, 3708, 3709, 3710, 3711, 3712, 3725, 3729, 3730, 3894,\n",
    "       3895, 3925, 3953, 3954, 3955, 3956, 3957, 3959, 3960, 3961, 3962,\n",
    "       3973, 3974, 3975, 3976, 3977, 3979, 3980, 3981, 3982, 3983, 4007,\n",
    "       4008, 4009, 4010, 4011, 4012, 4013, 4014, 4015, 4027, 4028, 4029,\n",
    "       4030, 4031, 4032, 4033, 4034, 4035, 4036, 4037, 4067, 4068, 4069,\n",
    "       4070, 4071, 4073, 4074, 4075, 4086, 4087, 4088, 4089, 4090, 4091,\n",
    "       4092, 4093, 4094, 4095, 4125, 4126, 4127, 4128, 4129, 4145, 4146,\n",
    "       4147, 4148, 4149, 4395, 5737, 5738, 5758, 5759, 5775, 5776, 5777,\n",
    "       5798, 5799, 5800, 5955, 5956, 5957, 5958, 5992, 5993, 5994, 5995,\n",
    "       6011, 6013, 6014, 6016, 6039, 6040, 6041, 6042, 6043, 6044, 6045,\n",
    "       6068, 6069, 6070, 6071, 6072, 6073, 6146, 6147, 6148, 6166, 6167,\n",
    "       6168, 6169, 6170, 6171, 6172, 6192, 6193, 6194, 6195, 6239, 6240,\n",
    "       6241, 6242, 6279, 6281, 6282, 6301, 6302, 6303, 6304, 6323, 6324,\n",
    "       6851, 6852, 6853, 6854, 6855, 6871, 6872, 6873, 6874, 6875, 6887,\n",
    "       6888, 6889, 6890, 6891, 6892, 6893, 6894, 6895, 6896, 6897, 6898,\n",
    "       6935, 6936, 6943, 6944, 6945, 6962, 6963, 6964, 6965, 6966, 6968,\n",
    "       6969, 6990, 6992, 7084, 7086, 7087, 7088, 7097, 7098, 7099, 7100,\n",
    "       7101, 7102, 7105, 7106, 7133, 7134, 7135, 7136, 7137, 7138, 7139,\n",
    "       7140, 7141, 7142, 7143, 7161, 7168, 7169, 7170, 7171, 7172, 7173,\n",
    "       7174, 7195, 7196, 7197, 7198, 7199, 7200, 7201, 7202, 7227, 7228,\n",
    "       7229, 7230, 7231, 7232, 7233, 7234, 7235, 7236, 7237, 7238, 7253,\n",
    "       7254, 7255, 7256, 7274, 7275, 7276, 7277, 7278, 7279, 7280, 7281,\n",
    "       7282, 7301, 7302, 7303, 7304, 7305, 7434, 7465, 7466]\n",
    "\n",
    "\n",
    "runs = [2853, 2854, 2855, 2913, 2914, 2916, 2917, 2918, 2919, 2922, 2923,\n",
    "       2924, 2925, 2929, 2930, 2931, 2932, 2933, 2934, 2949, 2950, 2952,\n",
    "       2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2965, 2966,\n",
    "       2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977,\n",
    "       2988, 2989, 2990, 2991, 2992, 3004, 3005, 3006, 3007, 3008, 3009,\n",
    "       3010, 3011, 3012, 3087, 3088, 3089, 3090, 3093, 3094, 3095, 3096,\n",
    "       3097, 3098, 3099, 3100, 3101, 3143, 3144, 3169, 3170, 3171, 3231,\n",
    "       3232, 3233, 3234, 3235, 3236, 3237, 3243, 3244, 3245, 3264, 3265,\n",
    "       3266, 3267, 3268, 3269, 3270, 3271, 3272, 3273, 3274, 3275, 3276,\n",
    "       3277, 3278, 3279]\n",
    "\n",
    "# Selecting the runs we want to analyse\n",
    "runs = runs[:]\n",
    "\n",
    "print(f\"Computing for {len(runs)} runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fe0776-759e-4307-96a4-df344b013331",
   "metadata": {},
   "source": [
    "### Reading the number of subruns from the datachecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11606f13-cdbb-4c0c-ae4b-63037b25522c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding dl1  data to dictionary (Run 2853)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2854)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2855)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2913)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2914)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2916)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2917)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2918)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2919)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2922)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2923)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2924)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2925)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2929)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2930)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2931)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2932)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2933)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2934)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2949)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2950)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2952)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2953)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2954)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2955)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2956)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2957)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2958)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2959)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2960)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2961)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2965)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2966)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2967)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2968)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2969)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2970)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2971)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2972)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2973)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2974)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2975)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2976)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2977)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2988)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2989)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2990)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2991)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 2992)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3004)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3005)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3006)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3007)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3008)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3009)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3010)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3011)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3012)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3087)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3088)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3089)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3090)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3093)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3094)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3095)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3096)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3097)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3098)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3099)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3100)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3101)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3143)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3144)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3169)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3170)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3171)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3231)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3232)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3233)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3234)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3235)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3236)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3237)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3243)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3244)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3245)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3264)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3265)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3266)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3267)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3268)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3269)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3270)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3271)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3272)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3273)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3274)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3275)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3276)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3277)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3278)...\n",
      "\n",
      "Adding dl1  data to dictionary (Run 3279)...\n",
      "...Finished adding dl1 data to dictionary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35 s, sys: 3.43 s, total: 38.4 s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We create a empty dictionary to store all the information needed inside\n",
    "dict_dchecks = {}\n",
    "for run in runs:\n",
    "    dict_dchecks[run] = {\n",
    "        \"run_num\" : run,\n",
    "    }\n",
    "\n",
    "dict_dchecks = lstpipeline.add_dl1_paths_to_dict(dict_dchecks, root_dl1)\n",
    "\n",
    "dict_run_sruns = {}\n",
    "for run in runs:\n",
    "    fnames_dl1 = np.sort(dict_dchecks[run][\"dl1a\"][\"srunwise\"])\n",
    "    srun_numbers = [int(f.split(\".\")[-2]) for f in fnames_dl1]\n",
    "    dict_run_sruns[run] = srun_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344eba4b-68d5-4fa2-887a-99d1b0ef488c",
   "metadata": {},
   "source": [
    "### Storing the subrun numbers in sets of certain amount of subruns inside the same job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67f077d7-3fd3-41fb-ac32-48d9eca6874c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final amount of jobs is 2497\n"
     ]
    }
   ],
   "source": [
    "n_jobs = 0\n",
    "with open(file_job_config, \"w\") as file:\n",
    "    for run in runs:\n",
    "        count_sruns = 0\n",
    "        sruns = np.sort(dict_run_sruns[run])\n",
    "        tmp_str = \"\"\n",
    "        for srun in sruns:\n",
    "            tmp_str = tmp_str + f\"_{srun}\"\n",
    "            # Launching a certain amount of subruns together\n",
    "            if (count_sruns % n_subruns_job == 0 and srun != 0) or (srun == max(sruns)):\n",
    "                tmp_str_splitted = tmp_str.split(\"_\")\n",
    "                if len(tmp_str_splitted) != 2:\n",
    "                    tmp_str = \"_\" + tmp_str_splitted[1] + \"_\" + tmp_str_splitted[-1]\n",
    "                file.write(f\"{run}{tmp_str}\\n\")\n",
    "                tmp_str = \"\"\n",
    "                n_jobs += 1\n",
    "            count_sruns += 1\n",
    "print(f\"The final amount of jobs is {n_jobs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8a431b-c3bf-4d52-905f-6eb21ab44c67",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">2. Configuring file for the fit parameters and etc</span>\n",
    "#### Is stored permanently inside `config/config_scaling_parameters.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc5df362-0e8b-4d98-b883-929c7b975a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permanent loczation of the configuration file\n",
    "fname_config_scaling = os.path.join(\"config\", \"config_scaling_parameters.json\")\n",
    "\n",
    "\"\"\" Source name in order to just complete the results file, and in order to improve run organization.\"\"\"\n",
    "source_name = \"crab\"\n",
    "\n",
    "\"\"\" Fit parameters\n",
    "Chosen limits in intensity (p.e.) for applying the fit i.e. the power law will be fitted only with the points within this range.\"\"\"\n",
    "limits_intensity = [316, 562]\n",
    "\"\"\" For the positive scaling cases (most of them), we need to have a lower  limit in intensity. Thi slimit is used for the subset of \n",
    "events that are scaled just to find which is the scaling value. We use a very low limit by default 60 p.e. compared to the lower \n",
    "limit of the fit 316 p.e. because in the worst cases we will have a very non-linear scaling that will displace significantly the \n",
    "events intensities.\"\"\"\n",
    "limits_intensity_extended = 60\n",
    "\n",
    "\"\"\" Power law parameters for the reference\n",
    "All these parameters are taken from a common analysis of the full dataset Where the period of end of 2022 and start 2023 is \n",
    "taken as reference for good runs. Then we take as reference the mean power law parameters in that period. p0 is the \n",
    "normalization factor and p1 is the slope.\"\"\"\n",
    "ref_p0 =  1.74 \n",
    "ref_p1 = -2.23\n",
    "\n",
    "\"\"\" Threshold in statistics for the last subrun\n",
    "The limit in number of events after cleaning that we need to consider the last subrun has enough statistics to perform the \n",
    "analysis over it. Otherwise the values of the scaling that will be applied to this last rubrun are the same that are applied \n",
    "to the last last subrun.\"\"\"\n",
    "statistics_threshold = 4000\n",
    "\n",
    "\"\"\" The number of tries in the dl1 files creation. Due that dl1 file creation have some bug that causes the file to not be \n",
    "present but not notify with any error. We implemented a loop and a check to see if the file exists. \"\"\"\n",
    "number_tries_dl1 = 3\n",
    "\n",
    "\"\"\" Parameters for the empyrical fits for Zenith Distance corrections Are simply two 2 degree polynomials for each variable \n",
    "of the power law.\"\"\"\n",
    "p0a, p0b, p0c = -0.44751321, 3.62502037, -1.43611437\n",
    "p1a, p1b, p1c = -2.89253919, 0.99443581, -0.34013068\n",
    "\n",
    "# Standard paths for data in the IT cluster ---------\n",
    "root_dl1 = \"/fefs/aswg/data/real/DL1/*/v0.*/tailcut84/\"\n",
    "root_rfs = \"/fefs/aswg/data/models/AllSky/20230927_v0.10.4_crab_tuned/\"\n",
    "root_mcs = \"/fefs/aswg/data/mc/DL2/AllSky/20230927_v0.10.4_crab_tuned/TestingDataset/\"\n",
    "\n",
    "# Root path of this script\n",
    "root = os.getcwd()\n",
    "# Path to store the configuration file we are going to use\n",
    "config_file = os.path.join(root, \"config/standard_config.json\")\n",
    "# Path to store objects\n",
    "root_objects = os.path.join(root, f\"objects/\")\n",
    "# Data main directory\n",
    "root_data = os.path.join(root, f\"../../data/cherenkov_transparency_corrections/{source_name}/\")\n",
    "# Sub-dl1 objects directory\n",
    "root_sub_dl1 = os.path.join(root_objects, \"sub_dl1/\")\n",
    "# Directory for the results of the fit of each run\n",
    "root_results = os.path.join(root_objects, \"results_fits/\")\n",
    "root_final_results = os.path.join(root_objects, \"final_results_fits/\")\n",
    "# Configuration file for the job launching\n",
    "file_job_config = os.path.join(root, \"config\", \"job_config_runs.txt\")\n",
    "# File for temporal bash scripts\n",
    "file_temporal_bash = os.path.join(root, \"objects\", \"tmp_bash/\")\n",
    "\n",
    "# Directories for the data\n",
    "dir_dl1b_scaled = os.path.join(root_data, \"dl1_scaled/\")\n",
    "dir_dl1m_scaled = os.path.join(root_data, \"dl1_merged_scaled/\")\n",
    "dir_dl2_scaled = os.path.join(root_data, \"dl2_scaled/\")\n",
    "dir_dl2 = os.path.join(root_data, \"dl2/\")\n",
    "dir_dl3_scaled_base = os.path.join(root_data, \"dl3_scaled/\")\n",
    "dir_dl3_base = os.path.join(root_data, \"dl3/\")\n",
    "dir_irfs = os.path.join(root_data, \"irfs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4b5404-7389-43d6-9d54-bf5db2bd73bd",
   "metadata": {},
   "source": [
    "### Insert in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4deda55f-9031-462b-b243-17426f855660",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration_dictionary = {\n",
    "  \"source_name\": source_name,\n",
    "  \"fit_parameters\": {\n",
    "    \"limits_intensity\": limits_intensity,\n",
    "    \"limits_intensity_extended\": limits_intensity_extended,\n",
    "    \"ref_p0\": ref_p0, \"ref_p1\": ref_p1,\n",
    "    \"statistics_threshold\": statistics_threshold,\n",
    "    \"p0a\": p0a, \"p0b\": p0b, \"p0c\": p0c,\n",
    "    \"p1a\": p1a, \"p1b\": p1b, \"p1c\": p1c,\n",
    "    \"number_tries_dl1\": number_tries_dl1\n",
    "  },\n",
    "  \"paths\": {\n",
    "    \"root_dl1\": root_dl1,\n",
    "    \"root_rfs\": root_rfs,\n",
    "    \"root_mcs\": root_mcs,\n",
    "    \"root\": root,\n",
    "    \"config_file\": config_file,\n",
    "    \"root_objects\": root_objects,\n",
    "    \"root_data\": root_data,\n",
    "    \"root_sub_dl1\": root_sub_dl1,\n",
    "    \"root_results\": root_results,\n",
    "    \"root_final_results\": root_final_results,\n",
    "    \"file_job_config\": file_job_config,\n",
    "    \"file_temporal_bash\": file_temporal_bash,\n",
    "    \"dir_dl1b_scaled\": dir_dl1b_scaled,\n",
    "    \"dir_dl1m_scaled\": dir_dl1m_scaled,\n",
    "    \"dir_dl2_scaled\": dir_dl2_scaled,\n",
    "    \"dir_dl2\": dir_dl2,\n",
    "    \"dir_dl3_scaled_base\": dir_dl3_scaled_base,\n",
    "    \"dir_dl3_base\": dir_dl3_base,\n",
    "    \"dir_irfs\": dir_irfs\n",
    "  }\n",
    "}\n",
    "\n",
    "# Store in a file\n",
    "# Open the file in write mode\n",
    "with open(fname_config_scaling, \"w\") as json_file:\n",
    "    json.dump(configuration_dictionary, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dab550-8178-4d27-8a3e-d2ee141baf33",
   "metadata": {},
   "source": [
    "#### Copy this code to export it in another script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbb450e2-210e-4325-8ca0-0d12e2c8f739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code to read it\n",
    "# with open(fname_config_scaling, \"r\") as json_file:\n",
    "#     configuration_dictionary = json.load(json_file)\n",
    "\n",
    "# # Now we wxtract all the variables with the same name as in the dictionary\n",
    "# source_name = configuration_dictionary[\"source_name\"]\n",
    "# for superkey in [\"fit_parameters\", \"paths\"]:\n",
    "#     for key, value in configuration_dictionary[superkey].items():\n",
    "#         globals()[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc08d0e1-d4a2-4388-a201-776775a9e4bb",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">3. Generating the full grid of IRFs</span>\n",
    "#### It might take some time if was not already computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607e5827-eab6-4597-897f-8b198eaf1ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_scripts  = \"/fefs/aswg/workspace/juan.jimenez/cherenkov_transparency_corrections/data_processing_srunwise/\"\n",
    "# python_script = f\"{root_scripts}/script_1_scaling.py\"\n",
    "\n",
    "# ! python $python_script \"irfs\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
