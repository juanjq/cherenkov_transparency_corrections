{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e70776f7-e2e1-4057-b7ef-878b2be0556d",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from datetime import datetime\n",
    "import pickle, json, sys, os, glob\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import chi2\n",
    "from scipy import optimize\n",
    "import subprocess\n",
    "\n",
    "from astropy.coordinates     import SkyCoord\n",
    "from lstchain.io.config      import get_standard_config\n",
    "from ctapipe.io              import read_table\n",
    "import tables\n",
    "\n",
    "# Other auxiliar scripts\n",
    "sys.path.insert(0, os.getcwd() + \"/../scripts/\")\n",
    "import auxiliar as aux\n",
    "import geometry as geom\n",
    "import lstpipeline\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cff6f04",
   "metadata": {},
   "source": [
    "### Configuration and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e592063-e67e-4439-ab38-51f28955dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Source name in order to just complete the results file, and\n",
    "in order to improve run organization.\"\"\"\n",
    "source_name = \"crab\"\n",
    "\n",
    "\"\"\" Fit parameters\n",
    "Chosen limits in intensity (p.e.) for applying the fit i.e. the\n",
    "power law will be fitted only with the points within this range.\"\"\"\n",
    "limits_intensity = [316, 562]\n",
    "\"\"\" For the positive scaling cases (most of them), we need to have a lower \n",
    "limit in intensity. Thi slimit is used for the subset of events that are \n",
    "scaled just to find which is the scaling value. We use a very low limit by\n",
    "default 60 p.e. compared to the lower limit of the fit 316 p.e. because in \n",
    "the worst cases we will have a very non-linear scaling that will displace \n",
    "significantly the events intensities.\"\"\"\n",
    "limits_intensity_extended = 60\n",
    "\n",
    "\"\"\" Power law parameters for the reference\n",
    "All these parameters are taken from a common analysis of the full dataset\n",
    "Where the period of end of 2022 and start 2023 is taken as reference for good \n",
    "runs. Then we take as reference the mean power law parameters in that period.\n",
    "p0 is the normalization factor and p1 is the slope.\"\"\"\n",
    "ref_p0 =  1.74 \n",
    "ref_p1 = -2.23\n",
    "\n",
    "\"\"\" Threshold in statistics for the last subrun\n",
    "The limit in number of events after cleaning that we need to consider the last\n",
    "subrun has enough statistics to perform the analysis over it. Otherwise the \n",
    "values of the scaling that will be applied to this last rubrun are the same \n",
    "that are applied to the last last subrun.\"\"\"\n",
    "statistics_threshold = 15000\n",
    "\n",
    "\"\"\" Parameters for the empyrical fits for Zenith Distance corrections\n",
    "Are simply two 2 degree polynomials for each variable of the power law.\"\"\"\n",
    "p0a, p0b, p0c = -0.44751321, 3.62502037, -1.43611437\n",
    "p1a, p1b, p1c = -2.89253919, 0.99443581, -0.34013068\n",
    "\n",
    "# Standard paths for data in the IT cluster ---------\n",
    "root_dl1 = \"/fefs/aswg/data/real/DL1/*/v0.*/tailcut84/\"\n",
    "# root_rfs = \"/fefs/aswg/data/models/AllSky/20240131_allsky_v0.10.5_all_dec_base/\"\n",
    "root_rfs = \"/fefs/aswg/data/models/AllSky/20230927_v0.10.4_crab_tuned/\"\n",
    "# root_mcs = \"/fefs/aswg/data/mc/DL2/AllSky/20240131_allsky_v0.10.5_all_dec_base/TestingDataset/\"\n",
    "root_mcs = \"/fefs/aswg/data/mc/DL2/AllSky/20230927_v0.10.4_crab_tuned/TestingDataset/\"\n",
    "\n",
    "# Root path of this script\n",
    "root = os.getcwd() + \"/\"\n",
    "# Path to store the configuration file we are going to use\n",
    "config_file = root + \"config/standard_config.json\"\n",
    "# Path to store objects\n",
    "root_objects = root + f\"objects/\"\n",
    "# Data main directory\n",
    "root_data = root + f\"../../data/cherenkov_transparency_corrections/{source_name}/\"\n",
    "# Sub-dl1 objects directory\n",
    "root_sub_dl1 = root_objects + \"sub_dl1/\"\n",
    "# Directory for the results of the fit of each run\n",
    "root_results = root_objects + \"results_fits/\"\n",
    "root_final_results = root_objects + \"final_results_fits/\"\n",
    "# Slurm output folder\n",
    "root_slurm = root + \"objects/output_slurm\"\n",
    "# Configuration file for the job launching\n",
    "file_job_config = root_objects + \"config/job_config_runs.txt\"\n",
    "\n",
    "def configure_lstchain():\n",
    "    \"\"\"Creates a file of standard configuration for the lstchain analysis. \n",
    "    It can be changed inside this function\"\"\"\n",
    "    dict_config = get_standard_config()\n",
    "    # We select the heuristic flatfield option in the standard configuration\n",
    "    dict_config[\"source_config\"][\"LSTEventSource\"][\"use_flatfield_heuristic\"] = True\n",
    "    with open(config_file, \"w\") as json_file:\n",
    "        json.dump(dict_config, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731cdc8f-bec8-4b8c-b1f2-3701e3594749",
   "metadata": {},
   "source": [
    "### Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30530e0d-4701-44d9-871b-34e75a522541",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The run number that we are interested in apply the corrections.\n",
    "The process is done run-wise, so the input will be an individual run.\"\"\"\n",
    "input_str = \"3094_142\"\n",
    "simulate_data = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0781c631-a9f2-4891-b9f0-02d4a2cfd2dc",
   "metadata": {},
   "source": [
    "### First initial variables computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4351371c-f83f-49fe-bcc6-b7a1bdadbfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the run number from the input string\n",
    "run_number   = int(input_str.split(\"_\")[0])\n",
    "first_last_srun = [int(s) for s in input_str.split(\"_\")[1:]]\n",
    "if len(first_last_srun) == 1:\n",
    "    srun_numbers = np.array(first_last_srun)\n",
    "else:\n",
    "    srun_numbers = np.arange(first_last_srun[0], first_last_srun[1] + 1)\n",
    "\n",
    "# Creating and storing a configuration file for lstchain processes\n",
    "configure_lstchain()\n",
    "\n",
    "# Create the paths that do not exist\n",
    "for path in [root_data + f\"dl1_scaled/{run_number:05}/\"]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(os.path.join(path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b32e12",
   "metadata": {},
   "source": [
    "### Finding the files that interest us\n",
    "#### Extracting dl1 files and dl1 datachecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95eb908",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Getting coordinates of source\n",
    "source_coords = SkyCoord.from_name(source_name)\n",
    "\n",
    "dict_source = {\n",
    "    \"name\"   : source_name,\n",
    "    \"coords\" : source_coords,\n",
    "    \"ra\"     : source_coords.ra.deg  * u.deg, # ra in degrees\n",
    "    \"dec\"    : source_coords.dec.deg * u.deg, # dec in degrees\n",
    "}\n",
    "\n",
    "# We create a empty dictionary to store all the information needed inside\n",
    "dict_dchecks = {}\n",
    "for run in [run_number]:\n",
    "    dict_dchecks[run] = {\n",
    "        \"run_num\" : run,\n",
    "    }\n",
    "# Then we add the paths to the files and the datachecks\n",
    "dict_dchecks = lstpipeline.add_dl1_paths_to_dict(dict_dchecks, root_dl1)\n",
    "dict_dchecks = lstpipeline.add_dl1_paths_to_dict(dict_dchecks, root_dl1, dchecking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac5fda0",
   "metadata": {},
   "source": [
    "#### Then we read the observations information and also the selected nodes for MC and RFs and we add it to the DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f1e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcheck_zd, dcheck_az = [], []\n",
    "dcheck_tstart, dcheck_telapsed = [], []\n",
    "\n",
    "for srun in range(len(dict_dchecks[run_number][\"dchecks\"][\"srunwise\"])):\n",
    "    tab_dcheck_srun = read_table(dict_dchecks[run_number][\"dchecks\"][\"srunwise\"][srun], \"/dl1datacheck/cosmics\")\n",
    "    \n",
    "    # reading the variables\n",
    "    dcheck_zd.append(90 - np.rad2deg(tab_dcheck_srun[\"mean_alt_tel\"]))\n",
    "    dcheck_az.append(np.rad2deg(tab_dcheck_srun[\"mean_az_tel\"]))\n",
    "    \n",
    "    dcheck_tstart.append(tab_dcheck_srun[\"dragon_time\"])\n",
    "    dcheck_telapsed.append(tab_dcheck_srun[\"elapsed_time\"])\n",
    "\n",
    "dcheck_zd = np.array(dcheck_zd)\n",
    "dcheck_az = np.array(dcheck_az)\n",
    "dcheck_tstart = np.array(dcheck_tstart)\n",
    "dcheck_telapsed = np.array(dcheck_telapsed)\n",
    "\n",
    "dict_dchecks[run_number][\"time\"] = {\n",
    "    \"tstart\"   : dcheck_tstart[0],            # datetime object\n",
    "    \"telapsed\" : np.sum(dcheck_telapsed),  # s\n",
    "    \"srunwise\" : {\n",
    "        \"telapsed\" : dcheck_telapsed,      # s      \n",
    "    },\n",
    "}\n",
    "dict_dchecks[run_number][\"pointing\"] = {\n",
    "    \"zd\" : np.mean(dcheck_zd),  # deg\n",
    "    \"az\" : np.mean(dcheck_az),  # deg\n",
    "    \"srunwise\" : {\n",
    "        \"zd\" : dcheck_zd, # deg\n",
    "        \"az\" : dcheck_az, # deg\n",
    "    },\n",
    "}\n",
    "# then we also select the RFs and MC files looking at the nodes available\n",
    "dict_dchecks, dict_nodes = lstpipeline.add_mc_and_rfs_nodes(dict_dchecks, root_rfs, root_mcs, dict_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261c0e68-ee3e-40b1-a11f-0eeaed2ca94b",
   "metadata": {},
   "source": [
    "### Read datacheck\n",
    "#### - The binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f3cc7f-95f6-405c-a8ad-19b8cbc8f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot intensity for decorrelation\n",
    "ref_intensity = (limits_intensity[0] * limits_intensity[1]) ** 0.5\n",
    "logger.info(f\"The intensity in the middle of the intensity range is {ref_intensity:.1f} p.e.\")\n",
    "\n",
    "########################################################\n",
    "# Reading the binning from the datacheck ---------------\n",
    "# Opening the corresponding datacheck\n",
    "fname_dcheck = dict_dchecks[run_number][\"dchecks\"][\"runwise\"]\n",
    "tab_dcheck_run = tables.open_file(fname_dcheck)\n",
    "\n",
    "# Read the binning from the datacheck of the first subrun\n",
    "dcheck_intensity_binning = np.array(tab_dcheck_run.root.dl1datacheck.histogram_binning.col(\"hist_intensity\")[0])\n",
    "# Calculating the logarithmic center of each bin\n",
    "dcheck_intensity_binning_centers = (dcheck_intensity_binning[:-1] * dcheck_intensity_binning[1:]) ** 0.5\n",
    "# Calculating the width of each bin\n",
    "dcheck_intensity_binning_widths = np.diff(dcheck_intensity_binning)\n",
    "tab_dcheck_run.close()\n",
    "\n",
    "# Mask for the fitting region in the fits\n",
    "mask_dcheck_bins_fit = (\n",
    "    (dcheck_intensity_binning_centers >= limits_intensity[0]) &\n",
    "    (dcheck_intensity_binning_centers <= limits_intensity[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eca996a-e892-4e46-9a04-7b18fb9411cf",
   "metadata": {},
   "source": [
    "#### - The intensity data from the datacheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7716a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# Reading the histogram from the datacheck ---------------\n",
    "# Opening the corresponding datacheck\n",
    "dcheck_hist_intensities = []\n",
    "for fname_dcheck_srun in dict_dchecks[run_number][\"dchecks\"][\"srunwise\"]:\n",
    "    tab_dcheck_srun = tables.open_file(fname_dcheck_srun)\n",
    "    dcheck_hist_intensities.append(np.array(tab_dcheck_srun.root.dl1datacheck.cosmics.col(\"hist_intensity\")))\n",
    "    tab_dcheck_srun.close()\n",
    "\n",
    "# Converting from counts to rate per intensity unit (non-binning dependent quantity)\n",
    "dcheck_rates       = [] # Array of histogram of rates for each subrun\n",
    "dcheck_delta_rates = [] # The statistical error\n",
    "for srun, dcheck_hist_intensity in enumerate(dcheck_hist_intensities):\n",
    "\n",
    "    effective_time_srun = dict_dchecks[run_number][\"time\"][\"srunwise\"][\"telapsed\"][srun]\n",
    "    \n",
    "    dcheck_rates.append(              dcheck_hist_intensity  / effective_time_srun / dcheck_intensity_binning_widths)\n",
    "\n",
    "    dcheck_delta_rates.append(np.sqrt(dcheck_hist_intensity) / effective_time_srun / dcheck_intensity_binning_widths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1da3d41-9c8c-43a7-807f-d0a9df93f8b4",
   "metadata": {},
   "source": [
    "### Correction factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdc14ca-4311-439a-bfdb-e447ae9c4735",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_factor_p0 = geom.pol2(1, p0a, p0b, p0c) / geom.pol2(np.cos(np.deg2rad(dict_dchecks[run_number][\"pointing\"][\"zd\"])), p0a, p0b, p0c)\n",
    "corr_factor_p1 = geom.pol2(1, p1a, p1b, p1c) - geom.pol2(np.cos(np.deg2rad(dict_dchecks[run_number][\"pointing\"][\"zd\"])), p1a, p1b, p1c)\n",
    "\n",
    "# Zenith correction of the reference (putting the reference in the zenith of the determined subrun)\n",
    "corr_ref_p0 = ref_p0 / corr_factor_p0\n",
    "corr_ref_p1 = ref_p1 - corr_factor_p1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ff66a0-303c-4ee9-8461-33cb3b779791",
   "metadata": {},
   "source": [
    "## Function to perform all the scaling and then the reading\n",
    "#### - Zenith corrections and light yield\n",
    "#### - Defining scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c93261-d9f7-427c-8c6c-bd53930d0ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_scaling(iteration_step, dict_results, other_parameters, simulated=False):\n",
    "    \"\"\"\n",
    "    A function to perform scaling and evaluating the results. Returning everything in a updated dictionary\n",
    "\n",
    "    Input:\n",
    "    - iteration_step: (str) \n",
    "        The iteration step you are in, that can be \"original\" for the original data, \"upper\" for the upper\n",
    "        limit on the scale factor, \"linear\" for the linear intepolation factor and \"final\" for the final scaling \n",
    "        and results.\n",
    "        \n",
    "    - dict_results: (dict)\n",
    "        Dictionary with the results of the before step.\n",
    "        \n",
    "    - simulated: (bool)\n",
    "        If instead of scaling the data, random data is generated just to fill the values. Short to run tests.\n",
    "\n",
    "    - other_parameters (dict)\n",
    "        A dictionary with all other needed parameters:\n",
    "        * \"srun_numbers\"\n",
    "        * \"dict_dchecks\"\n",
    "        * \"ref_intensity\"\n",
    "        * \"dcheck_intensity_binning\"\n",
    "        * \"dcheck_intensity_binning_widths\"\n",
    "        * \"dcheck_intensity_binning_centers\"\n",
    "        * \"mask_dcheck_bins_fit\"\n",
    "        * \"corr_factor_p0\"\n",
    "        * \"corr_factor_p1\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Reading the other variables dictionary\n",
    "    srun_numbers = other_parameters[\"srun_numbers\"]\n",
    "    dict_dchecks = other_parameters[\"dict_dchecks\"]\n",
    "    ref_intensity = other_parameters[\"ref_intensity\"]\n",
    "    dcheck_intensity_binning = other_parameters[\"dcheck_intensity_binning\"]\n",
    "    dcheck_intensity_binning_widths = other_parameters[\"dcheck_intensity_binning_widths\"]\n",
    "    dcheck_intensity_binning_centers = other_parameters[\"dcheck_intensity_binning_centers\"]\n",
    "    mask_dcheck_bins_fit = other_parameters[\"mask_dcheck_bins_fit\"]\n",
    "    corr_factor_p0 = other_parameters[\"corr_factor_p0\"]\n",
    "    corr_factor_p1 = other_parameters[\"corr_factor_p1\"]\n",
    "    \n",
    "    # Creating a arrray of subruns looking at the datachecks and also extracting the run number\n",
    "    run_number  = dict_results[\"run\"]\n",
    "\n",
    "    # Empty arrays to store the fit information\n",
    "    data_p0, data_delta_p0 = [], []\n",
    "    data_p1, data_delta_p1 = [], []\n",
    "    data_chi2, data_pvalue = [], []\n",
    "    \n",
    "    # Processing subrun by subrun---------------------------------------------------------------\n",
    "    for srun in srun_numbers:    \n",
    "\n",
    "        # Reading dl1\n",
    "        #################################################\n",
    "        input_fname = dict_dchecks[run_number][\"dl1a\"][\"srunwise\"][srun]   # Input dl1a \n",
    "        data_scale_factor = dict_results[\"scaled\"][iteration_step][srun]   # Reading the scaling factor\n",
    "\n",
    "        # Here we do different things depending on the iteration step\n",
    "        # ////////////////////////////////////////////////////////////\n",
    "        # ////////////////////////////////////////////////////////////\n",
    "        # If is the first one i.e. == \"original\"\n",
    "        # We do not run lstchain_dl1ab because the data is already scaled\n",
    "        if iteration_step == \"original\":\n",
    "            data_output_fname = input_fname\n",
    "\n",
    "        # ////////////////////////////////////////////////////////////\n",
    "        # ////////////////////////////////////////////////////////////\n",
    "        # If is the second or third: \"upper\" or \"linear\"\n",
    "        # We perform lstchain_dl1ab but over a subset of the data only to keep it shorter\n",
    "        elif iteration_step in [\"upper\", \"linear\"]:\n",
    "\n",
    "            # Temporal dl1 file that will be overwritten in the next iteration / subrun\n",
    "            data_output_fname = root_sub_dl1 + f\"tmp_dl1_srunwise_run{run_number}_srun{srun}_{iteration_step}_scaled.h5\" \n",
    "\n",
    "            logger.info(f\"\\nProcessing subrun {srun}\")\n",
    "\n",
    "            # If scale is greater than 1 we select a range lower than the upper one\n",
    "            # otherwise we select a range higher than the upper one\n",
    "            if data_scale_factor > 1:\n",
    "                dl1_selected_range = f\"{limits_intensity_extended:.2f},{limits_intensity[1]:.2f}\"\n",
    "            else:\n",
    "                dl1_selected_range = f\"{limits_intensity[0]:.2f},inf\"\n",
    "\n",
    "            if not simulated:\n",
    "                logger.info(f\"Running lstchain_dl1ab... scale: {data_scale_factor:.2f}\")\n",
    "                # If the file already exists we delete it\n",
    "                if os.path.exists(data_output_fname):\n",
    "                    os.remove(data_output_fname)\n",
    "            \n",
    "                command = f\"lstchain_dl1ab --input-file {input_fname} --output-file {data_output_fname} --config {config_file}\"\n",
    "                command = command + f\" --no-image --light-scaling {data_scale_factor} --intensity-range {dl1_selected_range}\"\n",
    "                logger.info(command)\n",
    "\n",
    "                # We add an exception because sometimes can fail...\n",
    "                ntries = 3\n",
    "                while ntries > 0:\n",
    "                    try:\n",
    "                        ntries = ntries - 1\n",
    "                        subprocess.run(command, shell=True)\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"FLAG HERE! Failed to run {command} with error: {repr(e)}\")\n",
    "        # ////////////////////////////////////////////////////////////\n",
    "        # ////////////////////////////////////////////////////////////\n",
    "        # If is the last step i.e. \"final\"\n",
    "        # The lstchain_dl1ab script is run over all thedataset to generate the final file\n",
    "        elif iteration_step == \"final\":\n",
    "\n",
    "            data_output_fname = root_data + f\"dl1_scaled/{run_number:05}/\" + os.path.basename(dict_dchecks[run_number][\"dl1a\"][\"srunwise\"][srun])\n",
    "            logger.info(f\"\\nProcessing subrun {srun}\")\n",
    "\n",
    "            if not simulated:\n",
    "                logger.info(f\"Running lstchain_dl1ab... scale: {data_scale_factor:.2f}\")\n",
    "                # If the file already exists we delete it\n",
    "                if os.path.exists(data_output_fname):\n",
    "                    os.remove(data_output_fname)\n",
    "            \n",
    "                command = f\"lstchain_dl1ab --input-file {input_fname} --output-file {data_output_fname} --config {config_file}\"\n",
    "                command = command + f\" --no-image --light-scaling {data_scale_factor}\"\n",
    "                logger.info(command)\n",
    "                \n",
    "                # We add an exception because sometimes can fail...\n",
    "                ntries = 3\n",
    "                while ntries > 0:\n",
    "                    try:\n",
    "                        ntries = ntries - 1\n",
    "                        subprocess.run(command, shell=True)\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"FLAG HERE! Failed to run {command} with error: {repr(e)}\")\n",
    "    \n",
    "            # We store this info also in the dictionary in the final case\n",
    "            dict_results[\"filenames\"][srun] = data_output_fname\n",
    "\n",
    "        #################################################################\n",
    "        # Reading the dl1 file\n",
    "        #################################################################\n",
    "        if not simulated:\n",
    "            table_data = tables.open_file(data_output_fname)\n",
    "            data_counts_intensity, _ = np.histogram(\n",
    "                table_data.root.dl1.event.telescope.parameters.LST_LSTCam.col(\"intensity\"), \n",
    "                bins=dcheck_intensity_binning\n",
    "            )\n",
    "            table_data.close()\n",
    "        else:\n",
    "            # Simulated example data where we add random noise\n",
    "            simdata = [0,0,2,6,12,23,20,15,25,56,105,214,441,694,933,1244,1429,1582,1597,1545,1498,1479,1484,1364,1296,1290,\n",
    "                       1228,1089,1004,834,732,665,613,529,411,426,307,266,201,191,186,150,114,121,93,87,62,60,38,39,27,31,33,\n",
    "                       26,22,24,20,13,11,11,8,6,5,8,4,4,5,4,3,1,1,2,1,0,1,1,1,1,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "            simdata = np.array(simdata)\n",
    "            if iteration_step == \"original\":\n",
    "                data_counts_intensity = simdata + np.random.rand(100) * 100\n",
    "            elif iteration_step == \"upper\":\n",
    "                data_counts_intensity = simdata * 0.30 + np.random.rand(100) * 100\n",
    "            elif iteration_step == \"linear\":\n",
    "                data_counts_intensity = simdata * 0.15 + np.random.rand(100) * 100\n",
    "            elif iteration_step == \"final\":\n",
    "                data_counts_intensity = simdata * 0.18 + np.random.rand(100) * 100\n",
    "        \n",
    "        # Calculating the non binning dependent transformation\n",
    "        effective_time_srun = dict_dchecks[run_number][\"time\"][\"srunwise\"][\"telapsed\"][srun]\n",
    "        data_rates       = np.array(data_counts_intensity) / effective_time_srun / dcheck_intensity_binning_widths\n",
    "        data_delta_rates = np.sqrt(data_counts_intensity)  / effective_time_srun / dcheck_intensity_binning_widths\n",
    "\n",
    "\n",
    "        #################################################################\n",
    "        # Performing the fit\n",
    "        #################################################################\n",
    "        # Displacing the X-coordinates to the center of the fit, in order to decorrelate the fit\n",
    "        x_fit = dcheck_intensity_binning_centers[mask_dcheck_bins_fit] / ref_intensity\n",
    "        y_fit = data_rates[mask_dcheck_bins_fit]\n",
    "        yerr_fit = data_delta_rates[mask_dcheck_bins_fit]\n",
    "        \n",
    "        # Trying for the cases where the data is bad and the fit returns an error\n",
    "        try:\n",
    "            params, pcov, info, _, _ = curve_fit(\n",
    "                f     = geom.powerlaw,\n",
    "                xdata = x_fit,\n",
    "                ydata = y_fit,\n",
    "                sigma = yerr_fit,\n",
    "                p0    = [ref_p0, ref_p1],\n",
    "                full_output = True,\n",
    "            )\n",
    "        \n",
    "            srun_p0, srun_p1  = params\n",
    "            srun_delta_p0 = np.sqrt(pcov[0, 0])\n",
    "            srun_delta_p1 = np.sqrt(pcov[1, 1])\n",
    "            srun_chi2     = np.sum(info[\"fvec\"] ** 2)\n",
    "            srun_pvalue   = 1 - chi2.cdf(srun_chi2, sum(mask_dcheck_bins_fit))\n",
    "            dict_results[\"flag_error\"][srun] = False\n",
    "\n",
    "        # If the fit is not successful we return nan values\n",
    "        except RuntimeError:\n",
    "            logger.error(f\"For run {run_number} and subrun {srun}, the fit failed due to RuntimeError.\")\n",
    "            srun_p0, srun_p1  = np.nan, np.nan\n",
    "            srun_delta_p0 = np.nan\n",
    "            srun_delta_p1 = np.nan\n",
    "            srun_chi2     = np.nan\n",
    "            srun_pvalue   = np.nan\n",
    "            dict_results[\"flag_error\"][srun] = True\n",
    "            \n",
    "        dict_results[\"chi2\"][iteration_step][srun]   = srun_chi2\n",
    "        dict_results[\"pvalue\"][iteration_step][srun] = srun_pvalue\n",
    "        dict_results[\"scaled\"][iteration_step][srun] = data_scale_factor\n",
    "    \n",
    "        data_p0.append(srun_p0)\n",
    "        data_p1.append(srun_p1)\n",
    "        data_delta_p0.append(srun_delta_p0)\n",
    "        data_delta_p1.append(srun_delta_p1)\n",
    "        data_chi2.append(srun_chi2)\n",
    "        data_pvalue.append(srun_pvalue)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    data_p0       = np.array(data_p0)\n",
    "    data_p1       = np.array(data_p1)\n",
    "    data_delta_p0 = np.array(data_delta_p0)\n",
    "    data_delta_p1 = np.array(data_delta_p1)\n",
    "    data_chi2     = np.array(data_chi2)\n",
    "    data_pvalue   = np.array(data_pvalue)\n",
    "  \n",
    "    # Zenith corrections to the parameters\n",
    "    #########################################################\n",
    "    data_corr_p0 = data_p0 * corr_factor_p0\n",
    "    data_corr_p1 = data_p1 + corr_factor_p1\n",
    "    \n",
    "    data_corr_delta_p0 = data_delta_p0 * corr_factor_p0\n",
    "    data_corr_delta_p1 = data_delta_p1\n",
    "    \n",
    "    # Calculating the needed light yield  \n",
    "    data_light_yield, data_delta_light_yield = geom.calc_light_yield(\n",
    "        p0_fit = data_corr_p0,\n",
    "        p1_fit = data_corr_p1, \n",
    "        sigma_p0_fit = data_corr_delta_p0, \n",
    "        sigma_p1_fit = data_corr_delta_p1, \n",
    "        p0_ref = ref_p0,\n",
    "    )\n",
    "    # Scalings to apply\n",
    "    data_scaling       = 1 / data_light_yield\n",
    "    data_delta_scaling = 1 / data_light_yield ** 4 * data_delta_light_yield\n",
    "    # The scaling in percentage\n",
    "    data_scaling_percent       = (data_scaling - 1) * 100\n",
    "    data_delta_scaling_percent = data_delta_scaling * 100\n",
    "    \n",
    "    # Adding to dictionary\n",
    "    for i, srun in enumerate(srun_numbers):\n",
    "        dict_results[\"p0\"][iteration_step][srun]       = data_corr_p0[i]\n",
    "        dict_results[\"delta_p0\"][iteration_step][srun] = data_corr_delta_p0[i]\n",
    "        dict_results[\"p1\"][iteration_step][srun]       = data_corr_p1[i]\n",
    "        dict_results[\"delta_p1\"][iteration_step][srun] = data_corr_delta_p1[i]\n",
    "        \n",
    "        dict_results[\"light_yield\"][iteration_step][srun]       = data_light_yield[i]\n",
    "        dict_results[\"delta_light_yield\"][iteration_step][srun] = data_delta_light_yield[i]\n",
    "        dict_results[\"scaling\"][iteration_step][srun]           = data_scaling[i]\n",
    "        dict_results[\"delta_scaling\"][iteration_step][srun]     = data_delta_scaling[i]\n",
    "        dict_results[\"scaling_percent\"][iteration_step][srun]   = data_scaling_percent[i]\n",
    "        dict_results[\"delta_scaling_percent\"][iteration_step][srun] = data_delta_scaling_percent[i]\n",
    "\n",
    "    return dict_results\n",
    "\n",
    "other_parameters = {\n",
    "    \"srun_numbers\" : srun_numbers,\n",
    "    \"dict_dchecks\" : dict_dchecks,\n",
    "    \"ref_intensity\" : ref_intensity,\n",
    "    \"dcheck_intensity_binning\" : dcheck_intensity_binning,\n",
    "    \"dcheck_intensity_binning_widths\" : dcheck_intensity_binning_widths,\n",
    "    \"mask_dcheck_bins_fit\" : mask_dcheck_bins_fit,\n",
    "    \"corr_factor_p0\" : corr_factor_p0,\n",
    "    \"corr_factor_p1\" : corr_factor_p1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba6c1f7-b50a-4ec8-bb56-9424395dadd4",
   "metadata": {},
   "source": [
    "### Then scale the full datasets by the interpolated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2872fa71-5cfe-4dcf-a59f-a56d57326cdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dict_fname = root_final_results + f\"results_job_{run_number}.pkl\"\n",
    "\n",
    "# Reading the object\n",
    "with open(dict_fname, 'rb') as f:\n",
    "    dict_results = pickle.load(f)\n",
    "\n",
    "dict_results = find_scaling(\n",
    "    iteration_step=\"final\", dict_results=dict_results, other_parameters=other_parameters, simulated=simulate_data\n",
    ")\n",
    "\n",
    "# Saving the object again\n",
    "with open(dict_fname, 'wb') as f:\n",
    "    pickle.dump(dict_results, f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
