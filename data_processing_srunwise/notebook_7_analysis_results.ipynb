{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a78fc8f-47b6-474f-b484-2d17d7fdc167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import glob, os, pickle, json\n",
    "\n",
    "\"\"\" Source name in order to just complete the results file, and\n",
    "in order to improve run organization.\"\"\"\n",
    "source_name = \"crab\"\n",
    "\n",
    "# Root path of this script\n",
    "root = os.getcwd() + \"/\"\n",
    "# Path to store objects\n",
    "root_objects = root + f\"objects/\"\n",
    "# Data main directory\n",
    "root_data = root + f\"../../data/cherenkov_transparency_corrections/{source_name}/\"\n",
    "# Directory for the results of the fit of each run\n",
    "root_results = root_objects + \"results_fits/\"\n",
    "root_final_results = root_objects + \"final_results_fits/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5f54f9f-4563-4ceb-b356-403835a081a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from datetime import datetime\n",
    "import pickle, json, sys, os, glob\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import chi2\n",
    "from scipy import optimize\n",
    "import subprocess\n",
    "\n",
    "from astropy.coordinates     import SkyCoord\n",
    "from lstchain.io.config      import get_standard_config\n",
    "from ctapipe.io              import read_table\n",
    "import tables\n",
    "\n",
    "# Other auxiliar scripts\n",
    "sys.path.insert(0, os.getcwd() + \"/../scripts/\")\n",
    "import auxiliar as aux\n",
    "import geometry as geom\n",
    "import lstpipeline\n",
    "import plotting\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "logger.setLevel(logging.INFO)\n",
    "\"\"\" Source name in order to just complete the results file, and\n",
    "in order to improve run organization.\"\"\"\n",
    "source_name = \"crab\"\n",
    "\n",
    "\"\"\" Fit parameters\n",
    "Chosen limits in intensity (p.e.) for applying the fit i.e. the\n",
    "power law will be fitted only with the points within this range.\"\"\"\n",
    "limits_intensity = [316, 562]\n",
    "\"\"\" For the positive scaling cases (most of them), we need to have a lower \n",
    "limit in intensity. Thi slimit is used for the subset of events that are \n",
    "scaled just to find which is the scaling value. We use a very low limit by\n",
    "default 60 p.e. compared to the lower limit of the fit 316 p.e. because in \n",
    "the worst cases we will have a very non-linear scaling that will displace \n",
    "significantly the events intensities.\"\"\"\n",
    "limits_intensity_extended = 60\n",
    "\n",
    "\"\"\" Power law parameters for the reference\n",
    "All these parameters are taken from a common analysis of the full dataset\n",
    "Where the period of end of 2022 and start 2023 is taken as reference for good \n",
    "runs. Then we take as reference the mean power law parameters in that period.\n",
    "p0 is the normalization factor and p1 is the slope.\"\"\"\n",
    "ref_p0 =  1.74 \n",
    "ref_p1 = -2.23\n",
    "\n",
    "\"\"\" Threshold in statistics for the last subrun\n",
    "The limit in number of events after cleaning that we need to consider the last\n",
    "subrun has enough statistics to perform the analysis over it. Otherwise the \n",
    "values of the scaling that will be applied to this last rubrun are the same \n",
    "that are applied to the last last subrun.\"\"\"\n",
    "statistics_threshold = 15000\n",
    "\n",
    "\"\"\" Parameters for the empyrical fits for Zenith Distance corrections\n",
    "Are simply two 2 degree polynomials for each variable of the power law.\"\"\"\n",
    "p0a, p0b, p0c = -0.44751321, 3.62502037, -1.43611437\n",
    "p1a, p1b, p1c = -2.89253919, 0.99443581, -0.34013068\n",
    "\n",
    "# Standard paths for data in the IT cluster ---------\n",
    "root_dl1 = \"/fefs/aswg/data/real/DL1/*/v0.*/tailcut84/\"\n",
    "# root_rfs = \"/fefs/aswg/data/models/AllSky/20240131_allsky_v0.10.5_all_dec_base/\"\n",
    "root_rfs = \"/fefs/aswg/data/models/AllSky/20230927_v0.10.4_crab_tuned/\"\n",
    "# root_mcs = \"/fefs/aswg/data/mc/DL2/AllSky/20240131_allsky_v0.10.5_all_dec_base/TestingDataset/\"\n",
    "root_mcs = \"/fefs/aswg/data/mc/DL2/AllSky/20230927_v0.10.4_crab_tuned/TestingDataset/\"\n",
    "\n",
    "# Root path of this script\n",
    "root = os.getcwd() + \"/\"\n",
    "# Path to store the configuration file we are going to use\n",
    "config_file = root + \"config/standard_config.json\"\n",
    "# Path to store objects\n",
    "root_objects = root + f\"objects/\"\n",
    "# Data main directory\n",
    "root_data = root + f\"../../data/cherenkov_transparency_corrections/{source_name}/\"\n",
    "# Sub-dl1 objects directory\n",
    "root_sub_dl1 = root_objects + \"sub_dl1/\"\n",
    "# Directory for the results of the fit of each run\n",
    "root_results = root_objects + \"results_fits/\"\n",
    "root_final_results = root_objects + \"final_results_fits/\"\n",
    "# Slurm output folder\n",
    "root_slurm = root + \"objects/output_slurm\"\n",
    "# Configuration file for the job launching\n",
    "file_job_config = root_objects + \"config/job_config_runs.txt\"\n",
    "\n",
    "# Directories for the data\n",
    "dir_dl1b_scaled = root_data + \"dl1_scaled/\"\n",
    "dir_dl1m_scaled = root_data + \"dl1_merged_scaled/\"\n",
    "dir_dl2_scaled  = root_data + \"dl2_scaled/\"\n",
    "dir_dl2         = root_data + \"dl2/\"\n",
    "dir_dl3_scaled_base = root_data + \"dl3_scaled/\"\n",
    "dir_dl3_base        = root_data + \"dl3/\"\n",
    "dir_irfs        = root_data + \"irfs/\"\n",
    "\n",
    "def configure_lstchain():\n",
    "    \"\"\"Creates a file of standard configuration for the lstchain analysis. \n",
    "    It can be changed inside this function\"\"\"\n",
    "    dict_config = get_standard_config()\n",
    "    # We select the heuristic flatfield option in the standard configuration\n",
    "    dict_config[\"source_config\"][\"LSTEventSource\"][\"use_flatfield_heuristic\"] = True\n",
    "    with open(config_file, \"w\") as json_file:\n",
    "        json.dump(dict_config, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92220e80-698e-4d71-9f54-0e336b33630a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0332a0d-ab2c-44c2-b52f-2fe1025783f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2c8b191-0fac-4938-ad6d-9214b25abef3",
   "metadata": {},
   "source": [
    "## Checking intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7ca4285-9998-4486-9767-b3df60c8e23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding dl1  data to dictionary (Run 3271)...\n",
      "...Finished adding dl1 data to dictionary\n",
      "\n",
      "Adding dl1 datacheck data to dictionary (Run 3271)...\n",
      "...Finished adding dl1 data to dictionary\n"
     ]
    }
   ],
   "source": [
    "aa = glob.glob(root_final_results + \"*\")\n",
    "for i in range(len(aa))[:1]:\n",
    "    a = aa[i]\n",
    "\n",
    "    with open(a, 'rb') as f:\n",
    "        _dict_results_ = pickle.load(f)\n",
    "    \n",
    "    run = int(a.split(\"/\")[-1].split(\"_\")[-1].split(\".\")[0])\n",
    "    run_number = run\n",
    "    # We create a empty dictionary to store all the information needed inside\n",
    "    dict_dchecks = {}\n",
    "    for run in [run_number]:\n",
    "        dict_dchecks[run] = {\n",
    "            \"run_num\" : run,\n",
    "        }\n",
    "    # Then we add the paths to the files and the datachecks\n",
    "    dict_dchecks = lstpipeline.add_dl1_paths_to_dict(dict_dchecks, root_dl1)\n",
    "    dict_dchecks = lstpipeline.add_dl1_paths_to_dict(dict_dchecks, root_dl1, dchecking=True)\n",
    "\n",
    "    fname_dcheck = dict_dchecks[run_number][\"dchecks\"][\"runwise\"]\n",
    "    tab_dcheck_run = tables.open_file(fname_dcheck)\n",
    "    dcheck_intensity_binning = np.array(tab_dcheck_run.root.dl1datacheck.histogram_binning.col(\"hist_intensity\")[0])\n",
    "    dcheck_intensity_binning_centers = (dcheck_intensity_binning[:-1] * dcheck_intensity_binning[1:]) ** 0.5\n",
    "    dcheck_intensity_binning_widths = np.diff(dcheck_intensity_binning)\n",
    "    tab_dcheck_run.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee2d2d6-ca78-411e-86a2-b388374d2ace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0650d37d-86be-4e21-8742-7ab0cdc462a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19ee3f3-1137-4ace-949f-3d3e373ea365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aa = glob.glob(dir_dl2 + \"*.h5\")\n",
    "\n",
    "rates = []\n",
    "for ia, a in enumerate(aa):\n",
    "    print(ia, \"/\", len(aa))\n",
    "    tab = tables.open_file(a).root.dl2.event.telescope.parameters.LST_LSTCam\n",
    "\n",
    "    t = tab.col(\"dragon_time\")\n",
    "    efft = (t[-1] - t[0])\n",
    "    \n",
    "    counts, _ = np.histogram(tab.col(\"intensity\"), dcheck_intensity_binning)\n",
    "    c = counts / efft / dcheck_intensity_binning_widths\n",
    "    \n",
    "    tab.close()\n",
    "\n",
    "    rates.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca22afa-3f08-4448-beae-8e6e9d232aef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aa = glob.glob(dir_dl2_scaled + \"*.h5\")\n",
    "\n",
    "rates_s = []\n",
    "for ia, a in enumerate(aa):\n",
    "    print(ia, \"/\", len(aa))\n",
    "    tab = tables.open_file(a).root.dl2.event.telescope.parameters.LST_LSTCam\n",
    "\n",
    "    t = tab.col(\"dragon_time\")\n",
    "    efft = (t[-1] - t[0])\n",
    "    \n",
    "    counts, _ = np.histogram(tab.col(\"intensity\"), dcheck_intensity_binning)\n",
    "    c = counts / efft / dcheck_intensity_binning_widths\n",
    "    \n",
    "    tab.close()\n",
    "\n",
    "    rates_s.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7724669-cbcd-4f05-9b57-db3f7bfb781c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1bfd26-b6dd-4bb5-bd85-fb7bef770621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fcd018-f925-4817-b459-e5fdddaf0bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4.5))\n",
    "\n",
    "for i in range(len(rates_s))[:]:\n",
    "    ax.plot(dcheck_intensity_binning_centers, rates_s[i], color=\"b\", lw=1)\n",
    "\n",
    "for i in range(len(rates))[:]:\n",
    "    ax.plot(dcheck_intensity_binning_centers, rates[i], color=\"r\", lw=1)\n",
    "\n",
    "\n",
    "intensity_sample = np.linspace(limits_intensity[0] / 1.3, limits_intensity[1] * 1.5, 100)\n",
    "ax.axvspan(limits_intensity[0], limits_intensity[1], alpha=0.3, ls=\"-\", facecolor=\"none\", hatch=\"///\", edgecolor=\"k\")\n",
    "\n",
    "ax.set_xlabel(\"Intensity [p.e.]\")\n",
    "ax.set_ylabel(\"Rate [events / s / p.e.]\")\n",
    "ax.loglog()\n",
    "# ax.set_xlim(1.8e1, 1e4)\n",
    "# ax.set_ylim(1e-3, 1e2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f549e234-befb-49ce-8c0d-e4fa60ad8dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4.5))\n",
    "    \n",
    "for i in range(len(rates)):\n",
    "    ax.plot(dcheck_intensity_binning_centers, rates_s[i], zorder=np.random.rand())\n",
    "\n",
    "intensity_sample = np.linspace(limits_intensity[0] / 1.3, limits_intensity[1] * 1.5, 100)\n",
    "ax.axvspan(limits_intensity[0], limits_intensity[1], alpha=0.3, ls=\"-\", facecolor=\"none\", hatch=\"///\", edgecolor=\"k\")\n",
    "\n",
    "ax.set_xlabel(\"Intensity [p.e.]\")\n",
    "ax.set_ylabel(\"Rate [events / s / p.e.]\")\n",
    "ax.loglog()\n",
    "ax.set_xlim(1.8e1, 1e4)\n",
    "ax.set_ylim(1e-3, 1e2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09abdee-4a7a-4f5f-92fa-a021345dee6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bb1c02-6422-454b-be64-d4bd53eb7094",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.fromtimestamp(t[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b3a18e-002b-4c4f-b71c-16273ae08b66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b411fdff-bbfe-4448-965a-0c2aeb775ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bd055b-bc82-4523-81ca-1bff11cd442b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d726010a-c41c-4957-96a1-16500f2217b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = glob.glob(root_final_results + \"*\")\n",
    "dict_general_results = {}\n",
    "all_values_scaling = []\n",
    "all_dchecks = []\n",
    "\n",
    "for i in range(len(aa))[:1]:\n",
    "    a = aa[i]\n",
    "\n",
    "    with open(a, 'rb') as f:\n",
    "        _dict_results_ = pickle.load(f)\n",
    "    \n",
    "    run = int(a.split(\"/\")[-1].split(\"_\")[-1].split(\".\")[0])\n",
    "    run_number = run\n",
    "\n",
    "\n",
    "    # Getting coordinates of source\n",
    "    source_coords = SkyCoord.from_name(source_name)\n",
    "    \n",
    "    dict_source = {\n",
    "        \"name\"   : source_name,\n",
    "        \"coords\" : source_coords,\n",
    "        \"ra\"     : source_coords.ra.deg  * u.deg, # ra in degrees\n",
    "        \"dec\"    : source_coords.dec.deg * u.deg, # dec in degrees\n",
    "    }\n",
    "    \n",
    "    # We create a empty dictionary to store all the information needed inside\n",
    "    dict_dchecks = {}\n",
    "    for run in [run_number]:\n",
    "        dict_dchecks[run] = {\n",
    "            \"run_num\" : run,\n",
    "        }\n",
    "    # Then we add the paths to the files and the datachecks\n",
    "    dict_dchecks = lstpipeline.add_dl1_paths_to_dict(dict_dchecks, root_dl1)\n",
    "    dict_dchecks = lstpipeline.add_dl1_paths_to_dict(dict_dchecks, root_dl1, dchecking=True)\n",
    "\n",
    "\n",
    "    dcheck_zd, dcheck_az = [], []\n",
    "    dcheck_tstart, dcheck_telapsed = [], []\n",
    "    \n",
    "    for srun in range(len(dict_dchecks[run_number][\"dchecks\"][\"srunwise\"])):\n",
    "        tab_dcheck_srun = read_table(dict_dchecks[run_number][\"dchecks\"][\"srunwise\"][srun], \"/dl1datacheck/cosmics\")\n",
    "        \n",
    "        # reading the variables\n",
    "        dcheck_zd.append(90 - np.rad2deg(tab_dcheck_srun[\"mean_alt_tel\"]))\n",
    "        dcheck_az.append(np.rad2deg(tab_dcheck_srun[\"mean_az_tel\"]))\n",
    "        \n",
    "        dcheck_tstart.append(tab_dcheck_srun[\"dragon_time\"])\n",
    "        dcheck_telapsed.append(tab_dcheck_srun[\"elapsed_time\"])\n",
    "    \n",
    "    dcheck_zd = np.array(dcheck_zd)\n",
    "    dcheck_az = np.array(dcheck_az)\n",
    "    dcheck_tstart = np.array(dcheck_tstart)\n",
    "    dcheck_telapsed = np.array(dcheck_telapsed)\n",
    "    \n",
    "    dict_dchecks[run_number][\"time\"] = {\n",
    "        \"tstart\"   : dcheck_tstart[0],            # datetime object\n",
    "        \"telapsed\" : np.sum(dcheck_telapsed),  # s\n",
    "        \"srunwise\" : {\n",
    "            \"telapsed\" : dcheck_telapsed,      # s      \n",
    "        },\n",
    "    }\n",
    "    dict_dchecks[run_number][\"pointing\"] = {\n",
    "        \"zd\" : np.mean(dcheck_zd),  # deg\n",
    "        \"az\" : np.mean(dcheck_az),  # deg\n",
    "        \"srunwise\" : {\n",
    "            \"zd\" : dcheck_zd, # deg\n",
    "            \"az\" : dcheck_az, # deg\n",
    "        },\n",
    "    }\n",
    "    # then we also select the RFs and MC files looking at the nodes available\n",
    "    dict_dchecks = lstpipeline.add_rf_node(dict_dchecks, root_rfs, dict_source)\n",
    "    all_dchecks.append(dict_dchecks)\n",
    "\n",
    "    # Pivot intensity for decorrelation\n",
    "    ref_intensity = (limits_intensity[0] * limits_intensity[1]) ** 0.5\n",
    "    \n",
    "    ########################################################\n",
    "    # Reading the binning from the datacheck ---------------\n",
    "    # Opening the corresponding datacheck\n",
    "    fname_dcheck = dict_dchecks[run_number][\"dchecks\"][\"runwise\"]\n",
    "    tab_dcheck_run = tables.open_file(fname_dcheck)\n",
    "    \n",
    "    # Read the binning from the datacheck of the first subrun\n",
    "    dcheck_intensity_binning = np.array(tab_dcheck_run.root.dl1datacheck.histogram_binning.col(\"hist_intensity\")[0])\n",
    "    # Calculating the logarithmic center of each bin\n",
    "    dcheck_intensity_binning_centers = (dcheck_intensity_binning[:-1] * dcheck_intensity_binning[1:]) ** 0.5\n",
    "    # Calculating the width of each bin\n",
    "    dcheck_intensity_binning_widths = np.diff(dcheck_intensity_binning)\n",
    "    tab_dcheck_run.close()\n",
    "    \n",
    "    # Mask for the fitting region in the fits\n",
    "    mask_dcheck_bins_fit = (\n",
    "        (dcheck_intensity_binning_centers >= limits_intensity[0]) &\n",
    "        (dcheck_intensity_binning_centers <= limits_intensity[1])\n",
    "    )\n",
    "\n",
    "    ##########################################################\n",
    "    # Reading the histogram from the datacheck ---------------\n",
    "    # Opening the corresponding datacheck\n",
    "    dcheck_hist_intensities = []\n",
    "    for fname_dcheck_srun in dict_dchecks[run_number][\"dchecks\"][\"srunwise\"]:\n",
    "        tab_dcheck_srun = tables.open_file(fname_dcheck_srun)\n",
    "        dcheck_hist_intensities.append(np.array(tab_dcheck_srun.root.dl1datacheck.cosmics.col(\"hist_intensity\")))\n",
    "        tab_dcheck_srun.close()\n",
    "    \n",
    "    # Converting from counts to rate per intensity unit (non-binning dependent quantity)\n",
    "    dcheck_rates       = [] # Array of histogram of rates for each subrun\n",
    "    dcheck_delta_rates = [] # The statistical error\n",
    "    for srun, dcheck_hist_intensity in enumerate(dcheck_hist_intensities):\n",
    "    \n",
    "        effective_time_srun = dict_dchecks[run_number][\"time\"][\"srunwise\"][\"telapsed\"][srun]\n",
    "        \n",
    "        dcheck_rates.append(              dcheck_hist_intensity  / effective_time_srun / dcheck_intensity_binning_widths)\n",
    "    \n",
    "        dcheck_delta_rates.append(np.sqrt(dcheck_hist_intensity) / effective_time_srun / dcheck_intensity_binning_widths)\n",
    "\n",
    "\n",
    "    corr_factor_p0 = geom.pol2(1, p0a, p0b, p0c) / geom.pol2(np.cos(np.deg2rad(dict_dchecks[run_number][\"pointing\"][\"zd\"])), p0a, p0b, p0c)\n",
    "    corr_factor_p1 = geom.pol2(1, p1a, p1b, p1c) - geom.pol2(np.cos(np.deg2rad(dict_dchecks[run_number][\"pointing\"][\"zd\"])), p1a, p1b, p1c)\n",
    "    \n",
    "    # Zenith correction of the reference (putting the reference in the zenith of the determined subrun)\n",
    "    corr_ref_p0 = ref_p0 / corr_factor_p0\n",
    "    corr_ref_p1 = ref_p1 - corr_factor_p1\n",
    "\n",
    "\n",
    "    \n",
    "    sruns_array = np.sort(list(_dict_results_[\"statistics\"].keys()))\n",
    "    \n",
    "    run_num = _dict_results_[\"run\"]\n",
    "    a1 = np.array([_dict_results_[\"final_scaling_interpolated\"][srun] for srun in sruns_array])\n",
    "    a2 = np.array([_dict_results_[\"final_scaling\"][srun] for srun in sruns_array])\n",
    "    delta_a2 = np.array([_dict_results_[\"delta_final_scaling\"][srun] for srun in sruns_array])\n",
    "    p0 = np.array([_dict_results_[\"p0\"][\"original\"][srun] for srun in sruns_array])\n",
    "    pvalues = np.array([_dict_results_[\"pvalue\"][\"original\"][srun] for srun in sruns_array])\n",
    "    delta_p0 = np.array([_dict_results_[\"delta_p0\"][\"original\"][srun] for srun in sruns_array])\n",
    "    chi2 = _dict_results_[\"interpolation\"][\"chi2\"]\n",
    "    pval = _dict_results_[\"interpolation\"][\"p_value\"] \n",
    "    all_values_scaling = [*all_values_scaling, *a2]\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 6), sharex=True)\n",
    "    ax1.errorbar(sruns_array, p0, yerr=delta_p0, ls=\"\", marker=\".\", c=\"r\")\n",
    "    ax1.axhline(ref_p0, color=\"k\", ls=\"--\", label=\"dR/dI ref\")\n",
    "    ax1.set_xlabel(\"Srun number\")\n",
    "    ax1.set_ylabel(\"dR/dI [ev / s / p.e.]\")\n",
    "    \n",
    "    ax2.set_ylabel(\"p-value\")\n",
    "    ax2.plot(sruns_array, pvalues, c=\"k\")\n",
    "    ax2.set_ylim(-0.1, 1.1)\n",
    "    \n",
    "    ax3.plot(sruns_array, a1, c=\"k\", label=\"Fit\")\n",
    "    colors = plotting.get_colors_multiplot(sruns_array)\n",
    "    for i in range(len(sruns_array)):\n",
    "        ax3.errorbar(sruns_array[i], a2[i], yerr=delta_a2[i], ls=\"\", marker=\".\", c=colors[i])\n",
    "    ax3.axhline(1, color=\"k\", ls=\"--\", label=\"No scaling\")\n",
    "    ax3.set_xlabel(\"Subrun number\")\n",
    "    ax3.set_ylabel(\"Final scaling\")\n",
    "    \n",
    "    ax1.set_title(f\"Run {run_num}, chi2={chi2:.0f}\")\n",
    "    ax1.legend(loc=1, frameon=False)\n",
    "    ax3.legend(loc=1)\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.savefig(f\"plots/profiles_{run_number}.png\", bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    bins_space = np.linspace(limits_intensity[0] - 100, limits_intensity[1] + 200, 100)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 4.5))\n",
    "    \n",
    "    colors = plotting.get_colors_multiplot(sruns_array)\n",
    "    for i in range(len(dcheck_rates)):\n",
    "        ax.plot(dcheck_intensity_binning_centers, dcheck_rates[i][0], color=colors[i], zorder=np.random.rand())\n",
    "    \n",
    "    cmap = plotting.create_cmap_from_colors(plotting.default_colors)\n",
    "    plotting.plot_colorbar(fig, ax, range(len(dcheck_rates)), cmap=cmap, label=\"Subrun number\")\n",
    "    \n",
    "    intensity_sample = np.linspace(limits_intensity[0] / 1.3, limits_intensity[1] * 1.5, 100)\n",
    "    ax.axvspan(limits_intensity[0], limits_intensity[1], alpha=0.3, ls=\"-\", facecolor=\"none\", hatch=\"///\", edgecolor=\"k\")\n",
    "    \n",
    "    ax.set_xlabel(\"Intensity [p.e.]\")\n",
    "    ax.set_ylabel(\"Rate [events / s / p.e.]\")\n",
    "    ax.loglog()\n",
    "    ax.set_xlim(1.8e1, 1e4)\n",
    "    ax.set_ylim(1e-3, 1e2)\n",
    "    ax.set_title(f\"Run {run_num}\")\n",
    "    \n",
    "    plt.savefig(f\"plots/spectrums_{run_number}.png\", bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "all_values_scaling = np.array(all_values_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d238c00c-5751-4954-89db-580c07fbb0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e4992b-ede9-44b5-8c4f-b0126eda3cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39963aeb-8f8f-4f08-8fac-b1ce3b0c8301",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "ax.hist((all_values_scaling-1) * 100, bins=np.linspace(-10, 18, 70), color=\"royalblue\")\n",
    "\n",
    "ax.axvline(0, color=\"k\", ls=\"--\")\n",
    "ax.set_ylabel(\"Counts\")\n",
    "ax.set_xlabel(\"Scaling needed %\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ae839e-9787-480d-a65a-a8f894b957d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb18751-f78b-49e8-afef-5552dac640fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aa = glob.glob(root_final_results + \"*2914*\")[0]\n",
    "with open(aa, 'rb') as f:\n",
    "    _dict_results_ = pickle.load(f)\n",
    "\n",
    "for srun in range(30):\n",
    "\n",
    "    points_scaling           = np.array([_dict_results_[\"scaled\"][key][srun]            for key in [\"original\", \"linear\", \"upper\"]])\n",
    "    points_light_yield       = np.array([_dict_results_[\"light_yield\"][key][srun]       for key in [\"original\", \"linear\", \"upper\"]])\n",
    "    points_delta_light_yield = np.array([_dict_results_[\"delta_light_yield\"][key][srun] for key in [\"original\", \"linear\", \"upper\"]])\n",
    "    \n",
    "    srun_a, srun_b, srun_c, srun_delta_a, srun_delta_b, srun_delta_c = geom.parabola_3points(\n",
    "        *points_scaling, *points_light_yield, *points_delta_light_yield\n",
    "    )\n",
    "    \n",
    "    range_avg_point = np.mean(points_scaling)\n",
    "    x0, delta_x0 = geom.get_roots_pol2(\n",
    "        range_avg_point, 1,*points_scaling, *points_light_yield, *points_delta_light_yield\n",
    "    )\n",
    "\n",
    "    \n",
    "    ############################################\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 3))\n",
    "    ax.errorbar(points_scaling, points_light_yield, yerr=points_delta_light_yield,\n",
    "                marker=\".\", color=\"k\", label=\"Points\", zorder=10, ls=\"\")\n",
    "    size_points = max(points_scaling) - min(points_scaling)\n",
    "    sample_scaling = np.linspace(min(points_scaling) - 0.1 * size_points, max(points_scaling) + 0.1 * size_points, 100)\n",
    "    ax.plot(sample_scaling, geom.pol2(sample_scaling, srun_a, srun_b, srun_c), \n",
    "            color=\"crimson\", ls=\"-\", label=\"Polynomial fit\", zorder=5)\n",
    "    ax.axvline(x0, color=\"k\", ls=\"--\", label=f\"Final scaling = {x0:.2f}\")\n",
    "    ax.axvspan(x0 - delta_x0, x0 + delta_x0, color=\"k\", alpha=0.2, ls=\"\")\n",
    "    ax.axhline(1, color=\"k\", ls=\":\", label=\"Reference\")\n",
    "    ax.set_xlabel(\"Scaling\")\n",
    "    ax.set_ylabel(\"Light yield\")\n",
    "    ax.legend(frameon=False)\n",
    "    ax.set_title(f\"Subrun {srun}\")\n",
    "    plt.savefig(f\"plots/{srun}.png\", bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show()\n",
    "    ############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c271d1-7d66-4772-8224-436b2122ee34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6bcbed-c3f5-488b-802d-aa0ac7d58693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd9b40-fcdc-4732-abfe-9dac3d2e996c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bins = np.logspace(0.35, 4, 100)\n",
    "bins_c = (bins[1:] + bins[:-1]) / 2\n",
    "bins_w = np.diff(bins)\n",
    "intensities_bp = []\n",
    "srun_num = []\n",
    "datas   = dict_dchecks[3278][\"dl1a\"][\"srunwise\"]\n",
    "dchecks = dict_dchecks[3278][\"dchecks\"][\"srunwise\"]\n",
    "for i, dcheck in enumerate(dchecks):\n",
    "    data = datas[i]\n",
    "    print(i)\n",
    "    tab = tables.open_file(data).root.dl1.event.telescope.parameters.LST_LSTCam\n",
    "    time = tables.open_file(dcheck).root.dl1datacheck.cosmics.col(\"elapsed_time\")[0]\n",
    "    intensity = tab.col(\"intensity\")\n",
    "    concentr_pixel = tab.col(\"concentration_pixel\")\n",
    "    int_bp = np.array(intensity) * np.array(concentr_pixel)\n",
    "    counts, bins = np.histogram(int_bp, bins)\n",
    "    counts = counts / time / bins_w\n",
    "    \n",
    "    intensities_bp.append(counts)\n",
    "    srun_num.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8020ecf-a53e-4db4-9b6c-14418857bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.logspace(0.25, 4, 100)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "\n",
    "colors = plotting.get_colors_multiplot(srun_num)\n",
    "for i in range(len(intensities_bp)):\n",
    "    ax.plot(bins_c, intensities_bp[i], color=colors[i], zorder=np.random.rand())\n",
    "cmap = plotting.create_cmap_from_colors(plotting.default_colors)\n",
    "plotting.plot_colorbar(fig, ax, range(len(srun_num)), cmap=cmap, label=\"Subrun number\")\n",
    "\n",
    "ax.set_xlabel(\"Intensity in brightest pixel [p.e.]\")\n",
    "ax.set_ylabel(\"rate [ev / s / p.e.]\")\n",
    "\n",
    "plt.loglog()\n",
    "plt.savefig(f\"plots/total.png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1126e4-1c3d-45d6-ba56-72dc6cc19c26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intensities_bp2 = []\n",
    "srun_num2 = []\n",
    "for i, d in enumerate(all_dchecks[:]):\n",
    "    d = d[list(d.keys())[0]]\n",
    "    run = d[\"run_num\"]\n",
    "    data   = d[\"dl1a\"][\"runwise\"]\n",
    "    dcheck = d[\"dchecks\"][\"runwise\"]\n",
    "\n",
    "    print(i, run)\n",
    "    \n",
    "    tab = tables.open_file(data).root.dl1.event.telescope.parameters.LST_LSTCam\n",
    "    time = np.sum(tables.open_file(dcheck).root.dl1datacheck.cosmics.col(\"elapsed_time\"))\n",
    "    intensity = tab.col(\"intensity\")\n",
    "    concentr_pixel = tab.col(\"concentration_pixel\")\n",
    "    int_bp = np.array(intensity) * np.array(concentr_pixel)\n",
    "    counts, bins = np.histogram(int_bp, bins)\n",
    "    counts = counts / time / bins_w\n",
    "    \n",
    "    intensities_bp2.append(counts)\n",
    "    srun_num2.append(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b341d36-efd7-4381-bf4a-921303bdd3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.logspace(0.25, 4, 100)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "\n",
    "colors = plotting.get_colors_multiplot(srun_num2)\n",
    "for i in range(len(srun_num2)):\n",
    "    ax.plot(bins_c, intensities_bp2[i], color=colors[i], zorder=np.random.rand())\n",
    "cmap = plotting.create_cmap_from_colors(plotting.default_colors)\n",
    "plotting.plot_colorbar(fig, ax, srun_num2, cmap=cmap, label=\"Run number\")\n",
    "\n",
    "ax.set_xlabel(\"Intensity in brightest pixel [p.e.]\")\n",
    "ax.set_ylabel(\"rate [ev / s / p.e.]\")\n",
    "\n",
    "plt.loglog()\n",
    "plt.savefig(f\"plots/total.png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3df91ae-fa04-41e4-ac26-a9af1a912668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a22190e-de55-4c53-9ae8-8f46cd746e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
