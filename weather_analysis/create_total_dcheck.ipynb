{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "917884c7-fe96-4f8c-b9f7-3415203222be",
   "metadata": {},
   "source": [
    "### Import needed packages and scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9c64ef8-c4a0-429b-b73c-4583cb3e9cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from datetime import datetime\n",
    "import pickle, json, sys, os, glob\n",
    "import pandas as pd\n",
    "\n",
    "# Display settings for Jupyter Notebook\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# Importing custom utility functions\n",
    "sys.path.insert(0, os.getcwd() + \"/../scripts/\")\n",
    "import auxiliar as aux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b14c99a-90db-42ea-8f1a-ca49125cb941",
   "metadata": {},
   "source": [
    "### Paths to data and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d01a3051-7ed0-445a-9abf-0cb82e6f4007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows for each job\n",
    "n_rows = 6000\n",
    "\n",
    "# Root path of this script\n",
    "root = os.getcwd() + \"/\"\n",
    "# Objects directory\n",
    "root_objects = root + \"objects/\"\n",
    "\n",
    "# Directory of all the night-wise datachecks\n",
    "root_dchecks = \"/fefs/aswg/workspace/abelardo.moralejo/data/datachecks/night_wise/DL1_datacheck_\"\n",
    "# Weather station file\n",
    "ws_database = root_objects + \"WS2003-22_short.h5\"\n",
    "\n",
    "# Some filenames -------------------\n",
    "# Filename of the datacheck dictionary\n",
    "fname_datacheck_dict = root_objects + \"datacheck_dict.pkl\"\n",
    "# Filename of the total dictionary\n",
    "fname_total_dict = root_objects + \"total_dict.pkl\"\n",
    "# Job list file\n",
    "fname_job_list = root_objects + \"bash_job_list.txt\"\n",
    "# Filename of the relation between run and night\n",
    "fname_run_night_relation = root_objects + \"ws_run_relation.txt\"\n",
    "\n",
    "\n",
    "# Flags for computing or not different parts\n",
    "# Compute the datacheck dictionary\n",
    "compute_datacheck_dict = False\n",
    "# Send all the bash jobs to the cluster\n",
    "send_jobs = True\n",
    "\n",
    "# Create needed folders\n",
    "for dir in [root_objects, root_objects + \"output_slurm/\", root + \"plots\"]:\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76748282-b2c5-469b-81ed-db2c2a664f54",
   "metadata": {},
   "source": [
    "### Extracting dates and parameters of all runs/subruns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4e9f54b-2cde-4177-932e-839fd6fd782a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 181 ms, sys: 126 ms, total: 306 ms\n",
      "Wall time: 325 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if compute_datacheck_dict:\n",
    "\n",
    "    run_number   = [] # Run numbers\n",
    "    srun_number  = [] # Subrun numbers\n",
    "    timestamps   = [] # Timestamps of each subrun\n",
    "    time_elapsed = [] # Elapsed time of each subrun\n",
    "    mean_azimuth = [] # Mean azimuth of each run\n",
    "    mean_zenith_distance = [] # Mean zenith of each run\n",
    "    zd_corrected_intensity_at_half_peak_rate = [] # ZD corrected intensity at half peak rate\n",
    "    zd_corrected_cosmics_rate_at_422_pe = [] # ZD corrected cosmics rate at 422 pe\n",
    "    zd_corrected_delta_cosmics_rate_at_422_pe = [] # ZD corrected delta cosmics rate at 422 pe\n",
    "    zd_corrected_cosmics_spectral_index = [] # ZD corrected cosmics spectral index\n",
    "    light_yield = [] # Light yield\n",
    "\n",
    "    # All the datachecks for all the nights\n",
    "    dchecks = np.sort(glob.glob(root_dchecks + \"*.h5\"))\n",
    "\n",
    "    # We iterate over all the datachecks\n",
    "    for i, dcheck in enumerate(dchecks):\n",
    "\n",
    "        print(f\"Analysing... {i:3}/{len(dchecks)}\") if i % 20 == 0 else None\n",
    "\n",
    "        # The datacheck file of the run summary (runwise)\n",
    "        ds = pd.read_hdf(dcheck, key=\"runsummary\")\n",
    "        # The datacheck file of the intensity spectrums (subrunwise)\n",
    "        di = pd.read_hdf(dcheck, key=\"cosmics_intensity_spectrum\")\n",
    "        \n",
    "        # Iterating over all the entries of each night, the subruns\n",
    "        for j in range(len(ds)):\n",
    "\n",
    "            # Reference run number\n",
    "            runref = ds[\"runnumber\"].iloc[j]\n",
    "            \n",
    "            # Intensity datacheck for only the subruns of the reference run\n",
    "            di_run = di.query(f\"runnumber == {runref}\")\n",
    "            \n",
    "            # Subrun iteration and storing all the data we are interested in\n",
    "            for k in range(len(di_run)):\n",
    "\n",
    "                run_number.append(runref)\n",
    "                srun_number.append(di_run[\"subrun\"].iloc[k])\n",
    "                timestamps.append(datetime.fromtimestamp(di_run[\"time\"].iloc[k]))\n",
    "                time_elapsed.append(di_run[\"corrected_elapsed_time\"].iloc[k])\n",
    "                mean_azimuth.append(ds[\"mean_azimuth\"].iloc[j])\n",
    "                mean_zenith_distance.append(np.arccos(di_run[\"cos_zenith\"].iloc[k]))\n",
    "                zd_corrected_intensity_at_half_peak_rate.append(di_run[\"ZD_corrected_intensity_at_half_peak_rate\"].iloc[k])\n",
    "                zd_corrected_cosmics_rate_at_422_pe.append(di_run[\"ZD_corrected_cosmics_rate_at_422_pe\"].iloc[k])\n",
    "                zd_corrected_delta_cosmics_rate_at_422_pe.append(di_run[\"ZD_corrected_delta_cosmics_rate_at_422_pe\"].iloc[k])\n",
    "                zd_corrected_cosmics_spectral_index.append(di_run[\"ZD_corrected_cosmics_spectral_index\"].iloc[k])\n",
    "                light_yield.append(di_run[\"light_yield\"].iloc[k])            \n",
    "\n",
    "    print(f\"Analysing... {len(dchecks):3}/{len(dchecks)}\\n\")\n",
    "\n",
    "    # Now we are going to sort looking to the timestamps\n",
    "    _, run_number = aux.sort_based(run_number, timestamps)\n",
    "    _, srun_number = aux.sort_based(srun_number, timestamps)\n",
    "    _, time_elapsed = aux.sort_based(time_elapsed, timestamps)\n",
    "    _, mean_azimuth = aux.sort_based(mean_azimuth, timestamps)\n",
    "    _, mean_zenith_distance = aux.sort_based(mean_zenith_distance, timestamps)\n",
    "    _, zd_corrected_intensity_at_half_peak_rate = aux.sort_based(zd_corrected_intensity_at_half_peak_rate, timestamps)\n",
    "    _, zd_corrected_cosmics_rate_at_422_pe = aux.sort_based(zd_corrected_cosmics_rate_at_422_pe, timestamps)\n",
    "    _, zd_corrected_delta_cosmics_rate_at_422_pe = aux.sort_based(zd_corrected_delta_cosmics_rate_at_422_pe, timestamps)\n",
    "    _, zd_corrected_cosmics_spectral_index = aux.sort_based(zd_corrected_cosmics_spectral_index, timestamps)\n",
    "    timestamps, light_yield = aux.sort_based(light_yield, timestamps)\n",
    "\n",
    "    # Creating the data dictionary\n",
    "    dict_dcheck = {\n",
    "        \"run\" : np.array(run_number),\n",
    "        \"srun\" : np.array(srun_number),\n",
    "        \"time\" : np.array(timestamps),\n",
    "        \"telapsed\" : np.array(time_elapsed),\n",
    "        \"az\" : np.rad2deg(mean_azimuth),\n",
    "        \"zd\" : np.rad2deg(mean_zenith_distance),\n",
    "        \"ZD_corrected_intensity_at_half_peak_rate\" : np.array(zd_corrected_intensity_at_half_peak_rate),\n",
    "        \"ZD_corrected_cosmics_rate_at_422_pe\" : np.array(zd_corrected_cosmics_rate_at_422_pe),\n",
    "        \"ZD_corrected_delta_cosmics_rate_at_422_pe\" : np.array(zd_corrected_delta_cosmics_rate_at_422_pe),\n",
    "        \"ZD_corrected_cosmics_spectral_index\" : np.array(zd_corrected_cosmics_spectral_index),\n",
    "        \"light_yield\" : np.array(light_yield)\n",
    "    }        \n",
    "\n",
    "    # Saving the objects in the objects directory\n",
    "    with open(fname_datacheck_dict, 'wb') as f:\n",
    "        pickle.dump(dict_dcheck, f, pickle.HIGHEST_PROTOCOL)  \n",
    "else:\n",
    "    # To read the file:\n",
    "    with open(fname_datacheck_dict, 'rb') as f:\n",
    "            dict_dcheck = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff6f7cc",
   "metadata": {},
   "source": [
    "### Creating the total dictionary run-subrun-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ec69328",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dict = {}\n",
    "\n",
    "# We create an entry per run\n",
    "for run in np.unique(dict_dcheck[\"run\"]):\n",
    "    total_dict[run] = {}\n",
    "\n",
    "# Converting dcheck dictionary to total dictionary\n",
    "for i in range(len(dict_dcheck[\"run\"])):\n",
    "\n",
    "    total_dict[dict_dcheck[\"run\"][i]][dict_dcheck[\"srun\"][i]] = {\n",
    "        \"time\" : dict_dcheck[\"time\"][i],\n",
    "        \"telapsed\" : dict_dcheck[\"telapsed\"][i],\n",
    "        \"az\" : dict_dcheck[\"az\"][i],\n",
    "        \"zd\" : dict_dcheck[\"zd\"][i],\n",
    "        \"ZD_corrected_intensity_at_half_peak_rate\" : dict_dcheck[\"ZD_corrected_intensity_at_half_peak_rate\"][i],\n",
    "        \"ZD_corrected_cosmics_rate_at_422_pe\" : dict_dcheck[\"ZD_corrected_cosmics_rate_at_422_pe\"][i],\n",
    "        \"ZD_corrected_delta_cosmics_rate_at_422_pe\" : dict_dcheck[\"ZD_corrected_delta_cosmics_rate_at_422_pe\"][i],\n",
    "        \"ZD_corrected_cosmics_spectral_index\" : dict_dcheck[\"ZD_corrected_cosmics_spectral_index\"][i],\n",
    "        \"light_yield\" : dict_dcheck[\"light_yield\"][i]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdf3a27-3dbf-409e-b9a1-4588a1630362",
   "metadata": {},
   "source": [
    "#### Reading the WS table and we reduce it to the part we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e384b3e-6e12-4556-89c4-f69f4c88357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the weather station database\n",
    "df_ws = pd.read_hdf(ws_database)\n",
    "\n",
    "# Loading the timestamp of each entry in the datacheck dictionary\n",
    "dates_dcheck = dict_dcheck[\"time\"]\n",
    "\n",
    "# Getting the min and max dates\n",
    "maxdate = np.max(dates_dcheck)\n",
    "mindate = np.min(dates_dcheck)\n",
    "\n",
    "# Converting the weather station dates to datetime objects\n",
    "dates_ws = np.array([datetime.fromisoformat(str(d).split(\".\")[0]) for d in df_ws.index])\n",
    "\n",
    "# Getting the max date of the weather station\n",
    "maxdate_ws = np.max(dates_ws)\n",
    "\n",
    "# Masking the weather station data to the min and max dates of the datacheck dictionary\n",
    "mask_dates  = ((dates_ws > mindate) & (dates_ws < maxdate))\n",
    "\n",
    "# Masking also for day data, i.e. sun_alt > 0 we are not interested in \n",
    "mask_night = (df_ws[\"sun_alt\"] < 0)\n",
    "\n",
    "total_mask = (mask_dates & mask_night)\n",
    "\n",
    "dates_ws = dates_ws[total_mask]\n",
    "df_ws    = df_ws[total_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b47fb89-a53d-4386-8db5-ba5a7e439cee",
   "metadata": {},
   "source": [
    "### Separating in bunchs of small number of jobs and writting into a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85c8366c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With groups of 6000 subruns, the number of prepared jobs is 124\n"
     ]
    }
   ],
   "source": [
    "start_indexes = []\n",
    "end_indexes   = []\n",
    "\n",
    "i, total = 0, 0\n",
    "while total < len(dict_dcheck[\"run\"]):\n",
    "    start_indexes.append(total)\n",
    "    end_indexes.append(total + n_rows - 1)\n",
    "    \n",
    "    i     += 1\n",
    "    total += n_rows\n",
    "\n",
    "print(f\"With groups of {n_rows} subruns, the number of prepared jobs is {len(start_indexes)}\")\n",
    "\n",
    "# Opening a new txt file with a job per column\n",
    "file_job_list = open(fname_job_list, \"w\")\n",
    "\n",
    "for s, e in zip(start_indexes, end_indexes):\n",
    "    file_job_list.write(f\"{s},{e}\\n\") \n",
    "\n",
    "file_job_list.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea4ec1e",
   "metadata": {},
   "source": [
    "### Launching the jobs to the queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "727c6f33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending job 0,5999 to the queue...\n",
      "\n",
      "Submitted batch job 31606590\n",
      "Sending job 6000,11999 to the queue...\n",
      "\n",
      "Submitted batch job 31606591\n",
      "Sending job 12000,17999 to the queue...\n",
      "\n",
      "Submitted batch job 31606592\n",
      "Sending job 18000,23999 to the queue...\n",
      "\n",
      "Submitted batch job 31606593\n",
      "Sending job 24000,29999 to the queue...\n",
      "\n",
      "Submitted batch job 31606594\n",
      "Sending job 30000,35999 to the queue...\n",
      "\n",
      "Submitted batch job 31606595\n",
      "Sending job 36000,41999 to the queue...\n",
      "\n",
      "Submitted batch job 31606596\n",
      "Sending job 42000,47999 to the queue...\n",
      "\n",
      "Submitted batch job 31606597\n",
      "Sending job 48000,53999 to the queue...\n",
      "\n",
      "Submitted batch job 31606598\n",
      "Sending job 54000,59999 to the queue...\n",
      "\n",
      "Submitted batch job 31606599\n",
      "Sending job 60000,65999 to the queue...\n",
      "\n",
      "Submitted batch job 31606600\n",
      "Sending job 66000,71999 to the queue...\n",
      "\n",
      "Submitted batch job 31606601\n",
      "Sending job 72000,77999 to the queue...\n",
      "\n",
      "Submitted batch job 31606602\n",
      "Sending job 78000,83999 to the queue...\n",
      "\n",
      "Submitted batch job 31606603\n",
      "Sending job 84000,89999 to the queue...\n",
      "\n",
      "Submitted batch job 31606604\n",
      "Sending job 90000,95999 to the queue...\n",
      "\n",
      "Submitted batch job 31606605\n",
      "Sending job 96000,101999 to the queue...\n",
      "\n",
      "Submitted batch job 31606606\n",
      "Sending job 102000,107999 to the queue...\n",
      "\n",
      "Submitted batch job 31606607\n",
      "Sending job 108000,113999 to the queue...\n",
      "\n",
      "Submitted batch job 31606608\n",
      "Sending job 114000,119999 to the queue...\n",
      "\n",
      "Submitted batch job 31606609\n",
      "Sending job 120000,125999 to the queue...\n",
      "\n",
      "Submitted batch job 31606610\n",
      "Sending job 126000,131999 to the queue...\n",
      "\n",
      "Submitted batch job 31606611\n",
      "Sending job 132000,137999 to the queue...\n",
      "\n",
      "Submitted batch job 31606612\n",
      "Sending job 138000,143999 to the queue...\n",
      "\n",
      "Submitted batch job 31606613\n",
      "Sending job 144000,149999 to the queue...\n",
      "\n",
      "Submitted batch job 31606614\n",
      "Sending job 150000,155999 to the queue...\n",
      "\n",
      "Submitted batch job 31606615\n",
      "Sending job 156000,161999 to the queue...\n",
      "\n",
      "Submitted batch job 31606616\n",
      "Sending job 162000,167999 to the queue...\n",
      "\n",
      "Submitted batch job 31606617\n",
      "Sending job 168000,173999 to the queue...\n",
      "\n",
      "Submitted batch job 31606618\n",
      "Sending job 174000,179999 to the queue...\n",
      "\n",
      "Submitted batch job 31606619\n",
      "Sending job 180000,185999 to the queue...\n",
      "\n",
      "Submitted batch job 31606620\n",
      "Sending job 186000,191999 to the queue...\n",
      "\n",
      "Submitted batch job 31606621\n",
      "Sending job 192000,197999 to the queue...\n",
      "\n",
      "Submitted batch job 31606622\n",
      "Sending job 198000,203999 to the queue...\n",
      "\n",
      "Submitted batch job 31606623\n",
      "Sending job 204000,209999 to the queue...\n",
      "\n",
      "Submitted batch job 31606624\n",
      "Sending job 210000,215999 to the queue...\n",
      "\n",
      "Submitted batch job 31606625\n",
      "Sending job 216000,221999 to the queue...\n",
      "\n",
      "Submitted batch job 31606626\n",
      "Sending job 222000,227999 to the queue...\n",
      "\n",
      "Submitted batch job 31606627\n",
      "Sending job 228000,233999 to the queue...\n",
      "\n",
      "Submitted batch job 31606628\n",
      "Sending job 234000,239999 to the queue...\n",
      "\n",
      "Submitted batch job 31606629\n",
      "Sending job 240000,245999 to the queue...\n",
      "\n",
      "Submitted batch job 31606630\n",
      "Sending job 246000,251999 to the queue...\n",
      "\n",
      "Submitted batch job 31606631\n",
      "Sending job 252000,257999 to the queue...\n",
      "\n",
      "Submitted batch job 31606632\n",
      "Sending job 258000,263999 to the queue...\n",
      "\n",
      "Submitted batch job 31606633\n",
      "Sending job 264000,269999 to the queue...\n",
      "\n",
      "Submitted batch job 31606634\n",
      "Sending job 270000,275999 to the queue...\n",
      "\n",
      "Submitted batch job 31606635\n",
      "Sending job 276000,281999 to the queue...\n",
      "\n",
      "Submitted batch job 31606636\n",
      "Sending job 282000,287999 to the queue...\n",
      "\n",
      "Submitted batch job 31606637\n",
      "Sending job 288000,293999 to the queue...\n",
      "\n",
      "Submitted batch job 31606638\n",
      "Sending job 294000,299999 to the queue...\n",
      "\n",
      "Submitted batch job 31606639\n",
      "Sending job 300000,305999 to the queue...\n",
      "\n",
      "Submitted batch job 31606640\n",
      "Sending job 306000,311999 to the queue...\n",
      "\n",
      "Submitted batch job 31606641\n",
      "Sending job 312000,317999 to the queue...\n",
      "\n",
      "Submitted batch job 31606642\n",
      "Sending job 318000,323999 to the queue...\n",
      "\n",
      "Submitted batch job 31606643\n",
      "Sending job 324000,329999 to the queue...\n",
      "\n",
      "Submitted batch job 31606644\n",
      "Sending job 330000,335999 to the queue...\n",
      "\n",
      "Submitted batch job 31606645\n",
      "Sending job 336000,341999 to the queue...\n",
      "\n",
      "Submitted batch job 31606646\n",
      "Sending job 342000,347999 to the queue...\n",
      "\n",
      "Submitted batch job 31606647\n",
      "Sending job 348000,353999 to the queue...\n",
      "\n",
      "Submitted batch job 31606648\n",
      "Sending job 354000,359999 to the queue...\n",
      "\n",
      "Submitted batch job 31606649\n",
      "Sending job 360000,365999 to the queue...\n",
      "\n",
      "Submitted batch job 31606650\n",
      "Sending job 366000,371999 to the queue...\n",
      "\n",
      "Submitted batch job 31606651\n",
      "Sending job 372000,377999 to the queue...\n",
      "\n",
      "Submitted batch job 31606652\n",
      "Sending job 378000,383999 to the queue...\n",
      "\n",
      "Submitted batch job 31606653\n",
      "Sending job 384000,389999 to the queue...\n",
      "\n",
      "Submitted batch job 31606654\n",
      "Sending job 390000,395999 to the queue...\n",
      "\n",
      "Submitted batch job 31606655\n",
      "Sending job 396000,401999 to the queue...\n",
      "\n",
      "Submitted batch job 31606656\n",
      "Sending job 402000,407999 to the queue...\n",
      "\n",
      "Submitted batch job 31606657\n",
      "Sending job 408000,413999 to the queue...\n",
      "\n",
      "Submitted batch job 31606658\n",
      "Sending job 414000,419999 to the queue...\n",
      "\n",
      "Submitted batch job 31606659\n",
      "Sending job 420000,425999 to the queue...\n",
      "\n",
      "Submitted batch job 31606660\n",
      "Sending job 426000,431999 to the queue...\n",
      "\n",
      "Submitted batch job 31606661\n",
      "Sending job 432000,437999 to the queue...\n",
      "\n",
      "Submitted batch job 31606662\n",
      "Sending job 438000,443999 to the queue...\n",
      "\n",
      "Submitted batch job 31606663\n",
      "Sending job 444000,449999 to the queue...\n",
      "\n",
      "Submitted batch job 31606664\n",
      "Sending job 450000,455999 to the queue...\n",
      "\n",
      "Submitted batch job 31606665\n",
      "Sending job 456000,461999 to the queue...\n",
      "\n",
      "Submitted batch job 31606666\n",
      "Sending job 462000,467999 to the queue...\n",
      "\n",
      "Submitted batch job 31606667\n",
      "Sending job 468000,473999 to the queue...\n",
      "\n",
      "Submitted batch job 31606668\n",
      "Sending job 474000,479999 to the queue...\n",
      "\n",
      "Submitted batch job 31606669\n",
      "Sending job 480000,485999 to the queue...\n",
      "\n",
      "Submitted batch job 31606670\n",
      "Sending job 486000,491999 to the queue...\n",
      "\n",
      "Submitted batch job 31606671\n",
      "Sending job 492000,497999 to the queue...\n",
      "\n",
      "Submitted batch job 31606672\n",
      "Sending job 498000,503999 to the queue...\n",
      "\n",
      "Submitted batch job 31606673\n",
      "Sending job 504000,509999 to the queue...\n",
      "\n",
      "Submitted batch job 31606674\n",
      "Sending job 510000,515999 to the queue...\n",
      "\n",
      "Submitted batch job 31606675\n",
      "Sending job 516000,521999 to the queue...\n",
      "\n",
      "Submitted batch job 31606676\n",
      "Sending job 522000,527999 to the queue...\n",
      "\n",
      "Submitted batch job 31606677\n",
      "Sending job 528000,533999 to the queue...\n",
      "\n",
      "Submitted batch job 31606678\n",
      "Sending job 534000,539999 to the queue...\n",
      "\n",
      "Submitted batch job 31606679\n",
      "Sending job 540000,545999 to the queue...\n",
      "\n",
      "Submitted batch job 31606680\n",
      "Sending job 546000,551999 to the queue...\n",
      "\n",
      "Submitted batch job 31606681\n",
      "Sending job 552000,557999 to the queue...\n",
      "\n",
      "Submitted batch job 31606682\n",
      "Sending job 558000,563999 to the queue...\n",
      "\n",
      "Submitted batch job 31606683\n",
      "Sending job 564000,569999 to the queue...\n",
      "\n",
      "Submitted batch job 31606684\n",
      "Sending job 570000,575999 to the queue...\n",
      "\n",
      "Submitted batch job 31606685\n",
      "Sending job 576000,581999 to the queue...\n",
      "\n",
      "Submitted batch job 31606686\n",
      "Sending job 582000,587999 to the queue...\n",
      "\n",
      "Submitted batch job 31606687\n",
      "Sending job 588000,593999 to the queue...\n",
      "\n",
      "Submitted batch job 31606688\n",
      "Sending job 594000,599999 to the queue...\n",
      "\n",
      "Submitted batch job 31606689\n",
      "Sending job 600000,605999 to the queue...\n",
      "\n",
      "Submitted batch job 31606690\n",
      "Sending job 606000,611999 to the queue...\n",
      "\n",
      "Submitted batch job 31606691\n",
      "Sending job 612000,617999 to the queue...\n",
      "\n",
      "Submitted batch job 31606692\n",
      "Sending job 618000,623999 to the queue...\n",
      "\n",
      "Submitted batch job 31606693\n",
      "Sending job 624000,629999 to the queue...\n",
      "\n",
      "Submitted batch job 31606694\n",
      "Sending job 630000,635999 to the queue...\n",
      "\n",
      "Submitted batch job 31606695\n",
      "Sending job 636000,641999 to the queue...\n",
      "\n",
      "Submitted batch job 31606696\n",
      "Sending job 642000,647999 to the queue...\n",
      "\n",
      "Submitted batch job 31606697\n",
      "Sending job 648000,653999 to the queue...\n",
      "\n",
      "Submitted batch job 31606698\n",
      "Sending job 654000,659999 to the queue...\n",
      "\n",
      "Submitted batch job 31606699\n",
      "Sending job 660000,665999 to the queue...\n",
      "\n",
      "Submitted batch job 31606700\n",
      "Sending job 666000,671999 to the queue...\n",
      "\n",
      "Submitted batch job 31606701\n",
      "Sending job 672000,677999 to the queue...\n",
      "\n",
      "Submitted batch job 31606702\n",
      "Sending job 678000,683999 to the queue...\n",
      "\n",
      "Submitted batch job 31606703\n",
      "Sending job 684000,689999 to the queue...\n",
      "\n",
      "Submitted batch job 31606704\n",
      "Sending job 690000,695999 to the queue...\n",
      "\n",
      "Submitted batch job 31606705\n",
      "Sending job 696000,701999 to the queue...\n",
      "\n",
      "Submitted batch job 31606706\n",
      "Sending job 702000,707999 to the queue...\n",
      "\n",
      "Submitted batch job 31606707\n",
      "Sending job 708000,713999 to the queue...\n",
      "\n",
      "Submitted batch job 31606708\n",
      "Sending job 714000,719999 to the queue...\n",
      "\n",
      "Submitted batch job 31606709\n",
      "Sending job 720000,725999 to the queue...\n",
      "\n",
      "Submitted batch job 31606710\n",
      "Sending job 726000,731999 to the queue...\n",
      "\n",
      "Submitted batch job 31606711\n",
      "Sending job 732000,737999 to the queue...\n",
      "\n",
      "Submitted batch job 31606712\n",
      "Sending job 738000,743999 to the queue...\n",
      "\n",
      "Submitted batch job 31606713\n"
     ]
    }
   ],
   "source": [
    "if send_jobs == True:\n",
    "    # Creating a file to store the results of the jobs\n",
    "    file_results = open(fname_run_night_relation, \"w\")\n",
    "    file_results.write(\"# Run - Subrun , WS entry id (date in ISO format)\")\n",
    "    file_results.close()\n",
    "    \n",
    "    \n",
    "    # Launching the jobs\n",
    "    !sh bash_jobs_indexes_ws_run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac284e5",
   "metadata": {},
   "source": [
    "### <span style=\"color:red;\">⚠️ Wait untill the jobs are processed and then the results need to be fully stored ⚠️</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f0cd62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02583cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a938425",
   "metadata": {},
   "source": [
    "### Adding the WS database timestamp/index to the total dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4973c083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba67dcd9-14c7-4f9d-a0db-e913511e722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dcheck[\"ws_entry\"]     = []\n",
    "dict_dcheck[\"ws_timestamp\"] = []\n",
    "\n",
    "tmp_df_ws = df_ws.copy()\n",
    "tmp_dates_ws = dates_ws.copy()\n",
    "\n",
    "for i, date_dcheck in enumerate(dict_dcheck[\"time\"]):\n",
    "    \n",
    "    print(\"Analysinng... {:5}/{}\".format(i, len(dict_dcheck[\"time\"]))) if i % 1000 == 0 else None\n",
    "    \n",
    "    if date_dcheck > maxdate_ws:\n",
    "        dict_dcheck[\"ws_entry\"].append(None)\n",
    "        dict_dcheck[\"ws_timestamp\"].append(None)\n",
    "    else:\n",
    "\n",
    "        str_id = str(tmp_df_ws.iloc[np.argmin(np.abs(tmp_dates_ws - date_dcheck))].name)\n",
    "        dict_dcheck[\"ws_entry\"].append(str_id)\n",
    "        dict_dcheck[\"ws_timestamp\"].append(datetime.fromisoformat(str_id))\n",
    "        \n",
    "        if i % 500 == 0:\n",
    "\n",
    "            tmp_mask = (tmp_dates_ws > date_dcheck)\n",
    "            \n",
    "            tmp_df_ws    = tmp_df_ws[tmp_mask]\n",
    "            tmp_dates_ws = tmp_dates_ws[tmp_mask]\n",
    "\n",
    "            print(len(tmp_dates_ws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6257f2c5-916a-44a1-821d-29b9eed24d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dates_dcheck, \"o\");\n",
    "plt.plot(dict_dcheck[\"ws_timestamp\"], \"o\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fac092-9456-4710-9dd2-2fa8b3fd76d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e4773e-77d2-4ab1-9137-4cb95fef7285",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_ws - date_dcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920f2d26-139c-4b1a-830c-5242fd6c720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(df_ws.iloc[np.argmin(np.abs(dates_ws - dict_dcheck[\"time\"][110000]))].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6086be11-698b-4951-ac26-59c528890875",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dcheck[\"time\"][110000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d80de66-ee4e-4a5d-a81e-ea59e2fc13ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "    indexes = []\n",
    "    ref     = []\n",
    "    for j, date in enumerate(dates):\n",
    "        \n",
    "        if j >= init and j <= end:\n",
    "            \n",
    "            if j % 200 == 0:\n",
    "                print(f\"{j}/{6000}\")\n",
    "\n",
    "            ref.append(j)\n",
    "            indexes.append(np.argmin(np.abs(dates[j] - dates_ws)))   \n",
    "\n",
    "    print(\"Writting...\")\n",
    "    with open(results_path, \"a\") as f:\n",
    "        for i, r in zip(indexes, ref):\n",
    "            f.write(f\"{i},{r}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17921b28-1a89-4b32-abd7-6ad5d96de188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28b7cb4-9b8c-486e-86dd-a9d0059b3da2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df459cdc-2e3a-46f3-9baa-c3e11d0f494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the index of the weather station data\n",
    "_index_ws   = [int(s.split(\",\")[0]) for s in np.loadtxt(fname_run_night_relation_dict, dtype=str)]\n",
    "_index_dcheck = [int(s.split(\",\")[1]) for s in np.loadtxt(fname_run_night_relation_dict, dtype=str)]\n",
    "\n",
    "# Sorting the weather station data indexes\n",
    "_index_ws, _index_dcheck = aux.sortbased(_index_ws, _index_dcheck)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d08f2b-1f7e-4483-8518-a855dcc4d96d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645e5b53-9f21-4f45-86df-c331f06cd185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the index of the weather station data and the datacheck dictionary\n",
    "index_ws, index_dcheck = [], []\n",
    "\n",
    "# Iterating over all the dates of the datacheck dictionary\n",
    "for i in range(len((dates_dcheck))):\n",
    "    \n",
    "    # We only want to iterate over the dates that are in the weather station database\n",
    "    # If the date is higher than the max date of the weather station, we append None\n",
    "    if dates_dcheck[i] > maxdate_ws:\n",
    "        index_ws.append(None)\n",
    "        index_dcheck.append(_index_dcheck[i])\n",
    "    else:\n",
    "        index_ws.append(_index_ws[i])\n",
    "        index_dcheck.append(_index_dcheck[i])          \n",
    "    \n",
    "dict_dcheck[\"index_ws\"] = np.array(index_ws)\n",
    "\n",
    "\n",
    "temperature, pressure, humidity = [], [], []\n",
    "tngDust, tngSeeing, rain        = [], [], []\n",
    "for i in range(len(dates)):\n",
    "    if i % 50000 == 0:\n",
    "        print(f\"{i}/{len(dates)}\")\n",
    "    if index_ws[i] != None:\n",
    "        temperature.append(df_ws.iloc[index_ws[i]][\"temperature\"])\n",
    "        pressure.append(df_ws.iloc[index_ws[i]][\"pressure\"])\n",
    "        humidity.append(df_ws.iloc[index_ws[i]][\"humidity\"])\n",
    "        tngDust.append(df_ws.iloc[index_ws[i]][\"tngDust\"])\n",
    "        tngSeeing.append(df_ws.iloc[index_ws[i]][\"tngSeeing\"])\n",
    "        rain.append(df_ws.iloc[index_ws[i]][\"rain\"])\n",
    "    else:\n",
    "        temperature.append(None)\n",
    "        pressure.append(None)\n",
    "        humidity.append(None)\n",
    "        tngDust.append(None)\n",
    "        tngSeeing.append(None)\n",
    "        rain.append(None)\n",
    "\n",
    "dict_dcheck[\"temperature\"] = np.array(temperature)\n",
    "dict_dcheck[\"pressure\"]    = np.array(pressure)\n",
    "dict_dcheck[\"humidity\"]    = np.array(humidity)\n",
    "dict_dcheck[\"tngDust\"]     = np.array(tngDust)\n",
    "dict_dcheck[\"tngSeeing\"]   = np.array(tngSeeing)\n",
    "dict_dcheck[\"rain\"]        = np.array(rain)\n",
    "\n",
    "with open(dir_objects + \"/data_dict.pkl\", 'wb') as f:\n",
    "    pickle.dump(dict_dcheck, f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
