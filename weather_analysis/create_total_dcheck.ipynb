{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "917884c7-fe96-4f8c-b9f7-3415203222be",
   "metadata": {},
   "source": [
    "# Script to generate a datachek with all the relevant information \n",
    "#### The information is stored per per subrun / run that is contained either in the datachecks and the MAGIC weather station\n",
    "\n",
    "The data we will store in the datacheck will be the following:\n",
    "\n",
    "#### <span style=\"color:darkred;\">- Time:</span>\n",
    "<span style=\"color:darkblue;\">- Timestamp [datetime object]</span>\\\n",
    "<span style=\"color:darkblue;\">- Time elapsed [s]</span>\n",
    "#### <span style=\"color:darkred;\">- Pointing:</span>\n",
    "<span style=\"color:darkblue;\">- Azimuth [deg]</span>\\\n",
    "<span style=\"color:darkblue;\">- Zenith distance [deg]</span>\n",
    "#### <span style=\"color:darkred;\">- Intensity profiles:</span>\n",
    "<span style=\"color:darkblue;\">- Intensity at Half Peak Rate [p.e.]</span>\\\n",
    "<span style=\"color:darkblue;\">- Cosmics Rate at 422p.e. [ev / s / p.e.]</span>\\\n",
    "<span style=\"color:darkblue;\">- Delta Cosmics Rate at 422p.e. [ev / s / p.e.]</span>\\\n",
    "<span style=\"color:darkblue;\">- Cosmics Spectral Index []</span>\\\n",
    "<span style=\"color:darkblue;\">- Light Yield [p.e./p.e.]</span>\n",
    "#### <span style=\"color:darkred;\">- Weather:</span>\n",
    "<span style=\"color:darkblue;\">- Temperature [CÂº]</span>\\\n",
    "<span style=\"color:darkblue;\">- Pressure [mmHg]</span>\\\n",
    "<span style=\"color:darkblue;\">- Humidity [%]</span>\\\n",
    "<span style=\"color:darkblue;\">- Wind Speed [km/h]</span>\\\n",
    "<span style=\"color:darkblue;\">- Wind Gust [km/h]</span>\\\n",
    "<span style=\"color:darkblue;\">- Wind Speed Average [km/h]</span>\\\n",
    "<span style=\"color:darkblue;\">- TNG Dust [$\\micro g/m^3$]</span>\\\n",
    "<span style=\"color:darkblue;\">- TNG Seeing [arcsecond]</span>\\\n",
    "<span style=\"color:darkblue;\">- Rain [tbd]</span>\n",
    "\n",
    "\n",
    "\n",
    "## Datacheck `cosmics_intensity_spectrum` (subrun-wise)\n",
    "Contains:\\\n",
    "yyyymmdd, ra_tel, dec_tel, cos_zenith, az_tel, runnumber,\n",
    "       subrun, time, elapsed_time, corrected_elapsed_time,\n",
    "       cosmics_rate, cosmics_cleaned_rate, intensity_at_half_peak_rate,\n",
    "       ZD_corrected_intensity_at_half_peak_rate, cosmics_peak_rate,\n",
    "       ZD_corrected_cosmics_peak_rate, cosmics_rate_at_422_pe,\n",
    "       ZD_corrected_cosmics_rate_at_422_pe, cosmics_spectral_index,\n",
    "       ZD_corrected_cosmics_spectral_index, intensity_spectrum_fit_p_value,\n",
    "       intensity_at_reference_rate, diffuse_nsb_std,\n",
    "       num_star_affected_pixels, anomalous_low_intensity_peak\n",
    "\n",
    "## Datachek `runsummary` (run-wise)\n",
    "Contains:\\\n",
    "runnumber, time, elapsed_time, min_altitude, mean_altitude,\n",
    "       max_altitude, min_azimuth, max_azimuth, mean_azimuth, mean_ra,\n",
    "       mean_dec, num_cosmics, num_pedestals, num_flatfield,\n",
    "       num_unknown_ucts_trigger_tags, num_wrong_ucts_tags_in_cosmics,\n",
    "       num_wrong_ucts_tags_in_pedestals, num_wrong_ucts_tags_in_flatfield,\n",
    "       num_ucts_jumps, num_unknown_tib_trigger_tags,\n",
    "       num_wrong_tib_tags_in_cosmics, num_wrong_tib_tags_in_pedestals,\n",
    "       num_wrong_tib_tags_in_flatfield, num_pedestals_after_cleaning,\n",
    "       num_contained_mu_rings, ff_charge_mean, ff_charge_mean_err,\n",
    "       ff_charge_stddev, ff_time_mean, ff_time_mean_err,\n",
    "       ff_time_stddev, ff_rel_time_stddev, ped_charge_mean,\n",
    "       ped_charge_mean_err, ped_charge_stddev,\n",
    "       ped_fraction_pulses_above10, ped_fraction_pulses_above30,\n",
    "       cosmics_fraction_pulses_above10, cosmics_fraction_pulses_above30,\n",
    "       mu_effi_mean, mu_effi_stddev, mu_width_mean, mu_width_stddev,\n",
    "       mu_hg_peak_sample_mean, mu_hg_peak_sample_stddev,\n",
    "       mu_intensity_mean, mean_number_of_pixels_nearby_stars\n",
    "       \n",
    "## Weather Station data\n",
    "Contains:\\\n",
    "sun_alt, sun_az, fBits, mjd, temperature, pressure,\n",
    "       windDirection, humidity, windSpeedCurrent, windGust,\n",
    "       windSpeedAverage, windDirectionAverage, tempSensor, tngDust,\n",
    "       tngSeeing, rain, state, Any, Mes, DP, diff1, is_dup,\n",
    "       temperatureR\n",
    "\n",
    "#### Import needed packages and scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9c64ef8-c4a0-429b-b73c-4583cb3e9cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from datetime import datetime\n",
    "import pickle, json, sys, os, glob\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Display settings for Jupyter Notebook\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# Importing custom utility functions\n",
    "sys.path.insert(0, os.getcwd() + \"/../scripts/\")\n",
    "import auxiliar as aux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b14c99a-90db-42ea-8f1a-ca49125cb941",
   "metadata": {},
   "source": [
    "### Paths to data and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d01a3051-7ed0-445a-9abf-0cb82e6f4007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows for each job\n",
    "n_rows = 6000\n",
    "\n",
    "# Root path of this script\n",
    "root = os.getcwd() + \"/\"\n",
    "# Objects directory\n",
    "root_objects = root + \"objects/\"\n",
    "\n",
    "# Directory of all the night-wise datachecks\n",
    "root_dchecks = \"/fefs/aswg/workspace/abelardo.moralejo/data/datachecks/night_wise/DL1_datacheck_\"\n",
    "# Weather station file\n",
    "ws_database = root_objects + \"WS2003-23.h5\"\n",
    "\n",
    "# Some filenames -------------------\n",
    "# Filename of the datacheck dictionary\n",
    "fname_datacheck_dict = root_objects + \"datacheck_dict.pkl\"\n",
    "# Filename of the total dictionary\n",
    "fname_total_dict = root_objects + \"total_dict.pkl\"\n",
    "# Job list file\n",
    "fname_job_list = root_objects + \"bash_job_list.txt\"\n",
    "# Filename of the relation between run and night\n",
    "fname_run_night_relation = root_objects + \"ws_run_relation.txt\"\n",
    "\n",
    "\n",
    "# Flags for computing or not different parts\n",
    "# Compute the datacheck dictionary\n",
    "compute_datacheck_dict = True\n",
    "# Send all the bash jobs to the cluster\n",
    "send_jobs = True\n",
    "\n",
    "# Create needed folders\n",
    "for dir in [root_objects, root_objects + \"output_slurm/\"]:\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76748282-b2c5-469b-81ed-db2c2a664f54",
   "metadata": {},
   "source": [
    "### Extracting dates and parameters of all runs/subruns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4e9f54b-2cde-4177-932e-839fd6fd782a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing...   0/529\n",
      "Analysing...  30/529\n",
      "Analysing...  60/529\n",
      "Analysing...  90/529\n",
      "Analysing... 120/529\n",
      "Analysing... 150/529\n",
      "Analysing... 180/529\n",
      "Analysing... 210/529\n",
      "Analysing... 240/529\n",
      "Analysing... 270/529\n",
      "Analysing... 300/529\n",
      "Analysing... 330/529\n",
      "Analysing... 360/529\n",
      "Analysing... 390/529\n",
      "Analysing... 420/529\n",
      "Analysing... 450/529\n",
      "Analysing... 480/529\n",
      "Analysing... 510/529\n",
      "Analysing... 529/529\n",
      "\n",
      "CPU times: user 5min 51s, sys: 24.6 s, total: 6min 15s\n",
      "Wall time: 7min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if compute_datacheck_dict:\n",
    "\n",
    "    run_number   = [] # Run numbers\n",
    "    srun_number  = [] # Subrun numbers\n",
    "    timestamps   = [] # Timestamps of each subrun\n",
    "    time_elapsed = [] # Elapsed time of each subrun\n",
    "    mean_azimuth = [] # Mean azimuth of each run\n",
    "    mean_zenith_distance = [] # Mean zenith of each run\n",
    "    mean_ra = [] # Mean right ascension\n",
    "    mean_dec = [] # Mean dec\n",
    "    zd_corrected_intensity_at_half_peak_rate = [] # ZD corrected intensity at half peak rate\n",
    "    zd_corrected_cosmics_rate_at_422_pe = [] # ZD corrected cosmics rate at 422 pe\n",
    "    zd_corrected_delta_cosmics_rate_at_422_pe = [] # ZD corrected delta cosmics rate at 422 pe\n",
    "    zd_corrected_cosmics_spectral_index = [] # ZD corrected cosmics spectral index\n",
    "    zd_corrected_delta_cosmics_spectral_index = [] # ZD corrected delta cosmics spectral index\n",
    "    light_yield = [] # Light yield\n",
    "\n",
    "    # All the datachecks for all the nights\n",
    "    dchecks = np.sort(glob.glob(root_dchecks + \"*.h5\"))\n",
    "\n",
    "    # We iterate over all the datachecks\n",
    "    for i, dcheck in enumerate(dchecks):\n",
    "\n",
    "        print(f\"Analysing... {i:3}/{len(dchecks)}\") if i % 30 == 0 else None\n",
    "\n",
    "        # The datacheck file of the run summary (runwise)\n",
    "        ds = pd.read_hdf(dcheck, key=\"runsummary\")\n",
    "        # The datacheck file of the intensity spectrums (subrunwise)\n",
    "        di = pd.read_hdf(dcheck, key=\"cosmics_intensity_spectrum\")\n",
    "        \n",
    "        # Iterating over all the entries of each night, the subruns\n",
    "        for j in range(len(ds)):\n",
    "\n",
    "            # Reference run number\n",
    "            runref = ds[\"runnumber\"].iloc[j]\n",
    "            \n",
    "            # Intensity datacheck for only the subruns of the reference run\n",
    "            di_run = di.query(f\"runnumber == {runref}\")\n",
    "            \n",
    "            # Subrun iteration and storing all the data we are interested in\n",
    "            for k in range(len(di_run)):\n",
    "\n",
    "                run_number.append(runref)\n",
    "                srun_number.append(di_run[\"subrun\"].iloc[k])\n",
    "                timestamps.append(datetime.fromtimestamp(di_run[\"time\"].iloc[k]))\n",
    "                time_elapsed.append(di_run[\"corrected_elapsed_time\"].iloc[k])\n",
    "                mean_azimuth.append(ds[\"mean_azimuth\"].iloc[j])\n",
    "                mean_zenith_distance.append(np.arccos(di_run[\"cos_zenith\"].iloc[k]))                \n",
    "                mean_ra.append(ds[\"mean_ra\"].iloc[j])\n",
    "                mean_dec.append(ds[\"mean_dec\"].iloc[j])\n",
    "                zd_corrected_intensity_at_half_peak_rate.append(di_run[\"ZD_corrected_intensity_at_half_peak_rate\"].iloc[k])\n",
    "                zd_corrected_cosmics_rate_at_422_pe.append(di_run[\"ZD_corrected_cosmics_rate_at_422_pe\"].iloc[k])\n",
    "                zd_corrected_delta_cosmics_rate_at_422_pe.append(di_run[\"ZD_corrected_delta_cosmics_rate_at_422_pe\"].iloc[k])\n",
    "                zd_corrected_cosmics_spectral_index.append(di_run[\"ZD_corrected_cosmics_spectral_index\"].iloc[k])\n",
    "                zd_corrected_delta_cosmics_spectral_index.append(di_run[\"delta_cosmics_spectral_index\"].iloc[k])\n",
    "                light_yield.append(di_run[\"light_yield\"].iloc[k])            \n",
    "\n",
    "    print(f\"Analysing... {len(dchecks):3}/{len(dchecks)}\\n\")\n",
    "\n",
    "    # Now we are going to sort looking to the timestamps\n",
    "    _, run_number = aux.sort_based(run_number, timestamps)\n",
    "    _, srun_number = aux.sort_based(srun_number, timestamps)\n",
    "    _, time_elapsed = aux.sort_based(time_elapsed, timestamps)\n",
    "    _, mean_azimuth = aux.sort_based(mean_azimuth, timestamps)\n",
    "    _, mean_zenith_distance = aux.sort_based(mean_zenith_distance, timestamps)\n",
    "    _, mean_ra = aux.sort_based(mean_ra, timestamps)\n",
    "    _, mean_dec = aux.sort_based(mean_dec, timestamps)\n",
    "    _, zd_corrected_intensity_at_half_peak_rate = aux.sort_based(zd_corrected_intensity_at_half_peak_rate, timestamps)\n",
    "    _, zd_corrected_cosmics_rate_at_422_pe = aux.sort_based(zd_corrected_cosmics_rate_at_422_pe, timestamps)\n",
    "    _, zd_corrected_delta_cosmics_rate_at_422_pe = aux.sort_based(zd_corrected_delta_cosmics_rate_at_422_pe, timestamps)\n",
    "    _, zd_corrected_cosmics_spectral_index = aux.sort_based(zd_corrected_cosmics_spectral_index, timestamps)\n",
    "    _, zd_corrected_delta_cosmics_spectral_index = aux.sort_based(zd_corrected_delta_cosmics_spectral_index, timestamps)\n",
    "    timestamps, light_yield = aux.sort_based(light_yield, timestamps)\n",
    "\n",
    "    # Creating the data dictionary\n",
    "    dict_dcheck = {\n",
    "        \"run\" : np.array(run_number),\n",
    "        \"srun\" : np.array(srun_number),\n",
    "        \"time\" : np.array(timestamps),\n",
    "        \"telapsed\" : np.array(time_elapsed),\n",
    "        \"az\" : np.rad2deg(mean_azimuth),\n",
    "        \"zd\" : np.rad2deg(mean_zenith_distance),\n",
    "        \"ra\" : np.array(mean_ra),\n",
    "        \"dec\" : np.array(mean_dec),\n",
    "        \"ZD_corrected_intensity_at_half_peak_rate\" : np.array(zd_corrected_intensity_at_half_peak_rate),\n",
    "        \"ZD_corrected_cosmics_rate_at_422_pe\" : np.array(zd_corrected_cosmics_rate_at_422_pe),\n",
    "        \"ZD_corrected_delta_cosmics_rate_at_422_pe\" : np.array(zd_corrected_delta_cosmics_rate_at_422_pe),\n",
    "        \"ZD_corrected_cosmics_spectral_index\" : np.array(zd_corrected_cosmics_spectral_index),\n",
    "        \"delta_cosmics_spectral_index\" : np.array(zd_corrected_delta_cosmics_spectral_index),\n",
    "        \"light_yield\" : np.array(light_yield)\n",
    "    }        \n",
    "\n",
    "    # Saving the objects in the objects directory\n",
    "    with open(fname_datacheck_dict, 'wb') as f:\n",
    "        pickle.dump(dict_dcheck, f, pickle.HIGHEST_PROTOCOL)  \n",
    "else:\n",
    "    # To read the file:\n",
    "    with open(fname_datacheck_dict, 'rb') as f:\n",
    "            dict_dcheck = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff6f7cc",
   "metadata": {},
   "source": [
    "### Creating the total dictionary run-subrun-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ec69328",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "total_dict = {}\n",
    "\n",
    "# We create an entry per run\n",
    "for run in np.unique(dict_dcheck[\"run\"]):\n",
    "    total_dict[run] = {}\n",
    "\n",
    "# Converting dcheck dictionary to total dictionary\n",
    "for i in range(len(dict_dcheck[\"run\"])):\n",
    "\n",
    "    total_dict[dict_dcheck[\"run\"][i]][dict_dcheck[\"srun\"][i]] = {\n",
    "        \"time\" : dict_dcheck[\"time\"][i],\n",
    "        \"telapsed\" : dict_dcheck[\"telapsed\"][i],\n",
    "        \"az\" : dict_dcheck[\"az\"][i],\n",
    "        \"zd\" : dict_dcheck[\"zd\"][i],\n",
    "        \"ra\" : dict_dcheck[\"ra\"][i],\n",
    "        \"dec\" : dict_dcheck[\"dec\"][i],\n",
    "        \"ZD_corrected_intensity_at_half_peak_rate\" : dict_dcheck[\"ZD_corrected_intensity_at_half_peak_rate\"][i],\n",
    "        \"ZD_corrected_cosmics_rate_at_422_pe\" : dict_dcheck[\"ZD_corrected_cosmics_rate_at_422_pe\"][i],\n",
    "        \"ZD_corrected_delta_cosmics_rate_at_422_pe\" : dict_dcheck[\"ZD_corrected_delta_cosmics_rate_at_422_pe\"][i],\n",
    "        \"ZD_corrected_cosmics_spectral_index\" : dict_dcheck[\"ZD_corrected_cosmics_spectral_index\"][i],\n",
    "        \"delta_cosmics_spectral_index\" : dict_dcheck[\"delta_cosmics_spectral_index\"][i],        \n",
    "        \"light_yield\" : dict_dcheck[\"light_yield\"][i]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdf3a27-3dbf-409e-b9a1-4588a1630362",
   "metadata": {},
   "source": [
    "#### Reading the WS table and we reduce it to the part we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e384b3e-6e12-4556-89c4-f69f4c88357c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Loading the weather station database\n",
    "df_ws = pd.read_hdf(ws_database)\n",
    "\n",
    "# Loading the timestamp of each entry in the datacheck dictionary\n",
    "dates_dcheck = dict_dcheck[\"time\"]\n",
    "\n",
    "# Getting the min and max dates\n",
    "maxdate = np.max(dates_dcheck)\n",
    "mindate = np.min(dates_dcheck)\n",
    "\n",
    "# Converting the weather station dates to datetime objects\n",
    "dates_ws = np.array([datetime.fromisoformat(str(d).split(\".\")[0]) for d in df_ws.index])\n",
    "\n",
    "# Getting the max date of the weather station\n",
    "maxdate_ws = np.max(dates_ws)\n",
    "\n",
    "# Masking the weather station data to the min and max dates of the datacheck dictionary\n",
    "mask_dates  = ((dates_ws > mindate) & (dates_ws < maxdate))\n",
    "\n",
    "# Masking also for day data, i.e. sun_alt > 0 we are not interested in \n",
    "mask_night = (df_ws[\"sun_alt\"] < 0)\n",
    "\n",
    "total_mask = (mask_dates & mask_night)\n",
    "\n",
    "dates_ws = dates_ws[total_mask]\n",
    "df_ws    = df_ws[total_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b47fb89-a53d-4386-8db5-ba5a7e439cee",
   "metadata": {},
   "source": [
    "### Separating in bunchs of small number of jobs and writting into a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85c8366c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With groups of 6000 subruns, the number of prepared jobs is 129\n"
     ]
    }
   ],
   "source": [
    "start_indexes = []\n",
    "end_indexes   = []\n",
    "\n",
    "i, total = 0, 0\n",
    "while total < len(dict_dcheck[\"run\"]):\n",
    "    start_indexes.append(total)\n",
    "    end_indexes.append(total + n_rows - 1)\n",
    "    \n",
    "    i     += 1\n",
    "    total += n_rows\n",
    "\n",
    "print(f\"With groups of {n_rows} subruns, the number of prepared jobs is {len(start_indexes)}\")\n",
    "\n",
    "# Opening a new txt file with a job per column\n",
    "file_job_list = open(fname_job_list, \"w\")\n",
    "\n",
    "for s, e in zip(start_indexes, end_indexes):\n",
    "    file_job_list.write(f\"{s},{e}\\n\") \n",
    "\n",
    "file_job_list.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea4ec1e",
   "metadata": {},
   "source": [
    "### Launching the jobs to the queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "727c6f33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sending job 0,5999 to the queue...\n",
      "\n",
      "Submitted batch job 35016362\n",
      "Sending job 6000,11999 to the queue...\n",
      "\n",
      "Submitted batch job 35016363\n",
      "Sending job 12000,17999 to the queue...\n",
      "\n",
      "Submitted batch job 35016364\n",
      "Sending job 18000,23999 to the queue...\n",
      "\n",
      "Submitted batch job 35016365\n",
      "Sending job 24000,29999 to the queue...\n",
      "\n",
      "Submitted batch job 35016366\n",
      "Sending job 30000,35999 to the queue...\n",
      "\n",
      "Submitted batch job 35016367\n",
      "Sending job 36000,41999 to the queue...\n",
      "\n",
      "Submitted batch job 35016368\n",
      "Sending job 42000,47999 to the queue...\n",
      "\n",
      "Submitted batch job 35016369\n",
      "Sending job 48000,53999 to the queue...\n",
      "\n",
      "Submitted batch job 35016370\n",
      "Sending job 54000,59999 to the queue...\n",
      "\n",
      "Submitted batch job 35016371\n",
      "Sending job 60000,65999 to the queue...\n",
      "\n",
      "Submitted batch job 35016372\n",
      "Sending job 66000,71999 to the queue...\n",
      "\n",
      "Submitted batch job 35016373\n",
      "Sending job 72000,77999 to the queue...\n",
      "\n",
      "Submitted batch job 35016374\n",
      "Sending job 78000,83999 to the queue...\n",
      "\n",
      "Submitted batch job 35016375\n",
      "Sending job 84000,89999 to the queue...\n",
      "\n",
      "Submitted batch job 35016376\n",
      "Sending job 90000,95999 to the queue...\n",
      "\n",
      "Submitted batch job 35016377\n",
      "Sending job 96000,101999 to the queue...\n",
      "\n",
      "Submitted batch job 35016378\n",
      "Sending job 102000,107999 to the queue...\n",
      "\n",
      "Submitted batch job 35016379\n",
      "Sending job 108000,113999 to the queue...\n",
      "\n",
      "Submitted batch job 35016380\n",
      "Sending job 114000,119999 to the queue...\n",
      "\n",
      "Submitted batch job 35016381\n",
      "Sending job 120000,125999 to the queue...\n",
      "\n",
      "Submitted batch job 35016382\n",
      "Sending job 126000,131999 to the queue...\n",
      "\n",
      "Submitted batch job 35016383\n",
      "Sending job 132000,137999 to the queue...\n",
      "\n",
      "Submitted batch job 35016384\n",
      "Sending job 138000,143999 to the queue...\n",
      "\n",
      "Submitted batch job 35016385\n",
      "Sending job 144000,149999 to the queue...\n",
      "\n",
      "Submitted batch job 35016386\n",
      "Sending job 150000,155999 to the queue...\n",
      "\n",
      "Submitted batch job 35016387\n",
      "Sending job 156000,161999 to the queue...\n",
      "\n",
      "Submitted batch job 35016388\n",
      "Sending job 162000,167999 to the queue...\n",
      "\n",
      "Submitted batch job 35016389\n",
      "Sending job 168000,173999 to the queue...\n",
      "\n",
      "Submitted batch job 35016390\n",
      "Sending job 174000,179999 to the queue...\n",
      "\n",
      "Submitted batch job 35016391\n",
      "Sending job 180000,185999 to the queue...\n",
      "\n",
      "Submitted batch job 35016392\n",
      "Sending job 186000,191999 to the queue...\n",
      "\n",
      "Submitted batch job 35016393\n",
      "Sending job 192000,197999 to the queue...\n",
      "\n",
      "Submitted batch job 35016394\n",
      "Sending job 198000,203999 to the queue...\n",
      "\n",
      "Submitted batch job 35016395\n",
      "Sending job 204000,209999 to the queue...\n",
      "\n",
      "Submitted batch job 35016396\n",
      "Sending job 210000,215999 to the queue...\n",
      "\n",
      "Submitted batch job 35016397\n",
      "Sending job 216000,221999 to the queue...\n",
      "\n",
      "Submitted batch job 35016398\n",
      "Sending job 222000,227999 to the queue...\n",
      "\n",
      "Submitted batch job 35016399\n",
      "Sending job 228000,233999 to the queue...\n",
      "\n",
      "Submitted batch job 35016400\n",
      "Sending job 234000,239999 to the queue...\n",
      "\n",
      "Submitted batch job 35016401\n",
      "Sending job 240000,245999 to the queue...\n",
      "\n",
      "Submitted batch job 35016402\n",
      "Sending job 246000,251999 to the queue...\n",
      "\n",
      "Submitted batch job 35016403\n",
      "Sending job 252000,257999 to the queue...\n",
      "\n",
      "Submitted batch job 35016404\n",
      "Sending job 258000,263999 to the queue...\n",
      "\n",
      "Submitted batch job 35016405\n",
      "Sending job 264000,269999 to the queue...\n",
      "\n",
      "Submitted batch job 35016406\n",
      "Sending job 270000,275999 to the queue...\n",
      "\n",
      "Submitted batch job 35016407\n",
      "Sending job 276000,281999 to the queue...\n",
      "\n",
      "Submitted batch job 35016408\n",
      "Sending job 282000,287999 to the queue...\n",
      "\n",
      "Submitted batch job 35016409\n",
      "Sending job 288000,293999 to the queue...\n",
      "\n",
      "Submitted batch job 35016410\n",
      "Sending job 294000,299999 to the queue...\n",
      "\n",
      "Submitted batch job 35016411\n",
      "Sending job 300000,305999 to the queue...\n",
      "\n",
      "Submitted batch job 35016412\n",
      "Sending job 306000,311999 to the queue...\n",
      "\n",
      "Submitted batch job 35016413\n",
      "Sending job 312000,317999 to the queue...\n",
      "\n",
      "Submitted batch job 35016414\n",
      "Sending job 318000,323999 to the queue...\n",
      "\n",
      "Submitted batch job 35016415\n",
      "Sending job 324000,329999 to the queue...\n",
      "\n",
      "Submitted batch job 35016416\n",
      "Sending job 330000,335999 to the queue...\n",
      "\n",
      "Submitted batch job 35016417\n",
      "Sending job 336000,341999 to the queue...\n",
      "\n",
      "Submitted batch job 35016418\n",
      "Sending job 342000,347999 to the queue...\n",
      "\n",
      "Submitted batch job 35016419\n",
      "Sending job 348000,353999 to the queue...\n",
      "\n",
      "Submitted batch job 35016420\n",
      "Sending job 354000,359999 to the queue...\n",
      "\n",
      "Submitted batch job 35016421\n",
      "Sending job 360000,365999 to the queue...\n",
      "\n",
      "Submitted batch job 35016422\n",
      "Sending job 366000,371999 to the queue...\n",
      "\n",
      "Submitted batch job 35016423\n",
      "Sending job 372000,377999 to the queue...\n",
      "\n",
      "Submitted batch job 35016424\n",
      "Sending job 378000,383999 to the queue...\n",
      "\n",
      "Submitted batch job 35016425\n",
      "Sending job 384000,389999 to the queue...\n",
      "\n",
      "Submitted batch job 35016426\n",
      "Sending job 390000,395999 to the queue...\n",
      "\n",
      "Submitted batch job 35016427\n",
      "Sending job 396000,401999 to the queue...\n",
      "\n",
      "Submitted batch job 35016428\n",
      "Sending job 402000,407999 to the queue...\n",
      "\n",
      "Submitted batch job 35016429\n",
      "Sending job 408000,413999 to the queue...\n",
      "\n",
      "Submitted batch job 35016430\n",
      "Sending job 414000,419999 to the queue...\n",
      "\n",
      "Submitted batch job 35016431\n",
      "Sending job 420000,425999 to the queue...\n",
      "\n",
      "Submitted batch job 35016432\n",
      "Sending job 426000,431999 to the queue...\n",
      "\n",
      "Submitted batch job 35016433\n",
      "Sending job 432000,437999 to the queue...\n",
      "\n",
      "Submitted batch job 35016434\n",
      "Sending job 438000,443999 to the queue...\n",
      "\n",
      "Submitted batch job 35016435\n",
      "Sending job 444000,449999 to the queue...\n",
      "\n",
      "Submitted batch job 35016436\n",
      "Sending job 450000,455999 to the queue...\n",
      "\n",
      "Submitted batch job 35016437\n",
      "Sending job 456000,461999 to the queue...\n",
      "\n",
      "Submitted batch job 35016438\n",
      "Sending job 462000,467999 to the queue...\n",
      "\n",
      "Submitted batch job 35016439\n",
      "Sending job 468000,473999 to the queue...\n",
      "\n",
      "Submitted batch job 35016440\n",
      "Sending job 474000,479999 to the queue...\n",
      "\n",
      "Submitted batch job 35016441\n",
      "Sending job 480000,485999 to the queue...\n",
      "\n",
      "Submitted batch job 35016442\n",
      "Sending job 486000,491999 to the queue...\n",
      "\n",
      "Submitted batch job 35016443\n",
      "Sending job 492000,497999 to the queue...\n",
      "\n",
      "Submitted batch job 35016444\n",
      "Sending job 498000,503999 to the queue...\n",
      "\n",
      "Submitted batch job 35016445\n",
      "Sending job 504000,509999 to the queue...\n",
      "\n",
      "Submitted batch job 35016446\n",
      "Sending job 510000,515999 to the queue...\n",
      "\n",
      "Submitted batch job 35016447\n",
      "Sending job 516000,521999 to the queue...\n",
      "\n",
      "Submitted batch job 35016448\n",
      "Sending job 522000,527999 to the queue...\n",
      "\n",
      "Submitted batch job 35016449\n",
      "Sending job 528000,533999 to the queue...\n",
      "\n",
      "Submitted batch job 35016450\n",
      "Sending job 534000,539999 to the queue...\n",
      "\n",
      "Submitted batch job 35016451\n",
      "Sending job 540000,545999 to the queue...\n",
      "\n",
      "Submitted batch job 35016452\n",
      "Sending job 546000,551999 to the queue...\n",
      "\n",
      "Submitted batch job 35016453\n",
      "Sending job 552000,557999 to the queue...\n",
      "\n",
      "Submitted batch job 35016454\n",
      "Sending job 558000,563999 to the queue...\n",
      "\n",
      "Submitted batch job 35016455\n",
      "Sending job 564000,569999 to the queue...\n",
      "\n",
      "Submitted batch job 35016456\n",
      "Sending job 570000,575999 to the queue...\n",
      "\n",
      "Submitted batch job 35016457\n",
      "Sending job 576000,581999 to the queue...\n",
      "\n",
      "Submitted batch job 35016458\n",
      "Sending job 582000,587999 to the queue...\n",
      "\n",
      "Submitted batch job 35016459\n",
      "Sending job 588000,593999 to the queue...\n",
      "\n",
      "Submitted batch job 35016460\n",
      "Sending job 594000,599999 to the queue...\n",
      "\n",
      "Submitted batch job 35016461\n",
      "Sending job 600000,605999 to the queue...\n",
      "\n",
      "Submitted batch job 35016462\n",
      "Sending job 606000,611999 to the queue...\n",
      "\n",
      "Submitted batch job 35016463\n",
      "Sending job 612000,617999 to the queue...\n",
      "\n",
      "Submitted batch job 35016464\n",
      "Sending job 618000,623999 to the queue...\n",
      "\n",
      "Submitted batch job 35016465\n",
      "Sending job 624000,629999 to the queue...\n",
      "\n",
      "Submitted batch job 35016466\n",
      "Sending job 630000,635999 to the queue...\n",
      "\n",
      "Submitted batch job 35016467\n",
      "Sending job 636000,641999 to the queue...\n",
      "\n",
      "Submitted batch job 35016468\n",
      "Sending job 642000,647999 to the queue...\n",
      "\n",
      "Submitted batch job 35016469\n",
      "Sending job 648000,653999 to the queue...\n",
      "\n",
      "Submitted batch job 35016470\n",
      "Sending job 654000,659999 to the queue...\n",
      "\n",
      "Submitted batch job 35016471\n",
      "Sending job 660000,665999 to the queue...\n",
      "\n",
      "Submitted batch job 35016472\n",
      "Sending job 666000,671999 to the queue...\n",
      "\n",
      "Submitted batch job 35016473\n",
      "Sending job 672000,677999 to the queue...\n",
      "\n",
      "Submitted batch job 35016474\n",
      "Sending job 678000,683999 to the queue...\n",
      "\n",
      "Submitted batch job 35016475\n",
      "Sending job 684000,689999 to the queue...\n",
      "\n",
      "Submitted batch job 35016476\n",
      "Sending job 690000,695999 to the queue...\n",
      "\n",
      "Submitted batch job 35016477\n",
      "Sending job 696000,701999 to the queue...\n",
      "\n",
      "Submitted batch job 35016478\n",
      "Sending job 702000,707999 to the queue...\n",
      "\n",
      "Submitted batch job 35016479\n",
      "Sending job 708000,713999 to the queue...\n",
      "\n",
      "Submitted batch job 35016480\n",
      "Sending job 714000,719999 to the queue...\n",
      "\n",
      "Submitted batch job 35016481\n",
      "Sending job 720000,725999 to the queue...\n",
      "\n",
      "Submitted batch job 35016482\n",
      "Sending job 726000,731999 to the queue...\n",
      "\n",
      "Submitted batch job 35016483\n",
      "Sending job 732000,737999 to the queue...\n",
      "\n",
      "Submitted batch job 35016484\n",
      "Sending job 738000,743999 to the queue...\n",
      "\n",
      "Submitted batch job 35016485\n",
      "Sending job 744000,749999 to the queue...\n",
      "\n",
      "Submitted batch job 35016486\n",
      "Sending job 750000,755999 to the queue...\n",
      "\n",
      "Submitted batch job 35016487\n",
      "Sending job 756000,761999 to the queue...\n",
      "\n",
      "Submitted batch job 35016488\n",
      "Sending job 762000,767999 to the queue...\n",
      "\n",
      "Submitted batch job 35016489\n",
      "Sending job 768000,773999 to the queue...\n",
      "\n",
      "Submitted batch job 35016490\n"
     ]
    }
   ],
   "source": [
    "if send_jobs == True:\n",
    "    # Creating a file to store the results of the jobs\n",
    "    file_results = open(fname_run_night_relation, \"w\")\n",
    "    file_results.write(\"# Run - Subrun , WS entry id (date in ISO format)\")\n",
    "    file_results.close()\n",
    "    \n",
    "    # Launching the jobs\n",
    "    print(\"\")\n",
    "    !sh bash_jobs_indexes_ws_run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac284e5",
   "metadata": {},
   "source": [
    "### <span style=\"color:red;\">------------------------------------------------------------------------------------------------------------------</span>\n",
    "### <span style=\"color:red;\">------------------------------------------------------------------------------------------------------------------</span>\n",
    "\n",
    "### <span style=\"color:red;\"> Wait untill the jobs are processed and then the results need to be fully stored </span>\n",
    "### <span style=\"color:red;\">------------------------------------------------------------------------------------------------------------------</span>\n",
    "### <span style=\"color:red;\">------------------------------------------------------------------------------------------------------------------</span>\n",
    "\n",
    "#### Now we read the results file where we associate each subrun to a entry of the WS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3888946-9eb7-4d5c-bb81-d73ba12c747f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.24 s, sys: 175 ms, total: 2.41 s\n",
      "Wall time: 2.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Reading the results\n",
    "file_results_lines = np.loadtxt(fname_run_night_relation, dtype=str, delimiter=\",\")\n",
    "\n",
    "# Creating a dictionary to organise them\n",
    "dict_results = {}\n",
    "\n",
    "for line in file_results_lines:\n",
    "\n",
    "    runsubrun, date_str = line\n",
    "\n",
    "    run, srun = runsubrun.split(\"-\")\n",
    "    run = int(run)\n",
    "    srun = int(srun)\n",
    "\n",
    "    date_str = date_str if date_str != \"None\" else None\n",
    "\n",
    "    try:\n",
    "        dict_results[run][srun] = date_str    \n",
    "    except KeyError:\n",
    "        dict_results[run] = {srun : date_str}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2767e73-7c7a-4904-a6e7-e864ec9e6bcd",
   "metadata": {},
   "source": [
    "#### Now the weather station data can be added to the total dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b24fcde-3787-485b-beb7-6e74ca061fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding data...      0/7771 runs\n",
      "Adding data...    500/7771 runs\n",
      "Adding data...   1000/7771 runs\n",
      "Adding data...   1500/7771 runs\n",
      "Adding data...   2000/7771 runs\n",
      "Adding data...   2500/7771 runs\n",
      "Adding data...   3000/7771 runs\n",
      "Adding data...   3500/7771 runs\n",
      "Adding data...   4000/7771 runs\n",
      "Adding data...   4500/7771 runs\n",
      "Adding data...   5000/7771 runs\n",
      "Adding data...   5500/7771 runs\n",
      "Adding data...   6000/7771 runs\n",
      "Adding data...   6500/7771 runs\n",
      "Adding data...   7000/7771 runs\n",
      "Adding data...   7500/7771 runs\n",
      "CPU times: user 12min 14s, sys: 1.72 s, total: 12min 16s\n",
      "Wall time: 12min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, run in enumerate(total_dict.keys()):\n",
    "\n",
    "    print(f\"Adding data... {i:6}/{len(total_dict.keys())} runs\") if i % 500 == 0 else None\n",
    "        \n",
    "    for srun in total_dict[run].keys():\n",
    "\n",
    "        try:\n",
    "            string_date = dict_results[run][srun]\n",
    "\n",
    "            if string_date != None:\n",
    "                empty_flag = False\n",
    "            else:\n",
    "                empty_flag = True\n",
    "        except KeyError:\n",
    "            empty_flag = True\n",
    "\n",
    "        if not empty_flag:\n",
    "            try:\n",
    "                total_dict[run][srun][\"weather\"] = {\n",
    "                    \"temperature\" :        df_ws.loc[string_date][\"temperature\"],      # degree celsius\n",
    "                    \"pressure\" :           df_ws.loc[string_date][\"pressure\"],         # mmHg\n",
    "                    \"humidity\" :           df_ws.loc[string_date][\"humidity\"],         # %\n",
    "                    \"wind_speed\" :         df_ws.loc[string_date][\"windSpeedCurrent\"], # km/h\n",
    "                    \"wind_gust\" :          df_ws.loc[string_date][\"windGust\"],         # km/h\n",
    "                    \"wind_speed_average\" : df_ws.loc[string_date][\"windSpeedAverage\"], # km/s\n",
    "                    \"tng_dust\" :           df_ws.loc[string_date][\"tngDust\"],          # ug/m3\n",
    "                    \"tng_seeing\" :         df_ws.loc[string_date][\"tngSeeing\"],        # arcseconds\n",
    "                    \"rain\" :               df_ws.loc[string_date][\"rain\"],             #\n",
    "                }\n",
    "            except KeyError:\n",
    "                print(f\"KeyError in Run {run}, Subrun {srun} with entry ID {string_date}.\")\n",
    "                empty_flag = True\n",
    "\n",
    "        if empty_flag:            \n",
    "            total_dict[run][srun][\"weather\"] = {\n",
    "                \"temperature\" :        None, # degree celsius\n",
    "                \"pressure\" :           None, # mmHg\n",
    "                \"humidity\" :           None, # %\n",
    "                \"wind_speed\" :         None, # km/h\n",
    "                \"wind_gust\" :          None, # km/h\n",
    "                \"wind_speed_average\" : None, # km/s\n",
    "                \"tng_dust\" :           None, # ug/m3\n",
    "                \"tng_seeing\" :         None, # arcseconds\n",
    "                \"rain\" :               None, #\n",
    "            } "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a938425",
   "metadata": {},
   "source": [
    "#### Saving the dictionary with all the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "645e5b53-9f21-4f45-86df-c331f06cd185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the object\n",
    "with open(fname_total_dict, 'wb') as f:\n",
    "    pickle.dump(total_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # To read the file:\n",
    "# with open(fname_total_dict, 'rb') as f:\n",
    "#         total_dict = pickle.load(f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940ac771-e529-4517-bec4-cfa01f09e3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
