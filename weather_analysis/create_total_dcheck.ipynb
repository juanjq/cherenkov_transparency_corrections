{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "917884c7-fe96-4f8c-b9f7-3415203222be",
   "metadata": {},
   "source": [
    "# Script to generate a datachek with all the relevant information \n",
    "#### The information is stored per per subrun / run that is contained either in the datachecks and the MAGIC weather station\n",
    "\n",
    "The data we will store in the datacheck will be the following:\n",
    "\n",
    "#### <span style=\"color:darkred;\">- Time:</span>\n",
    "<span style=\"color:darkblue;\">- Timestamp [datetime object]</span>\\\n",
    "<span style=\"color:darkblue;\">- Time elapsed [s]</span>\n",
    "#### <span style=\"color:darkred;\">- Pointing:</span>\n",
    "<span style=\"color:darkblue;\">- Azimuth [deg]</span>\\\n",
    "<span style=\"color:darkblue;\">- Zenith distance [deg]</span>\n",
    "#### <span style=\"color:darkred;\">- Intensity profiles:</span>\n",
    "<span style=\"color:darkblue;\">- Intensity at Half Peak Rate [p.e.]</span>\\\n",
    "<span style=\"color:darkblue;\">- Cosmics Rate at 422p.e. [ev / s / p.e.]</span>\\\n",
    "<span style=\"color:darkblue;\">- Delta Cosmics Rate at 422p.e. [ev / s / p.e.]</span>\\\n",
    "<span style=\"color:darkblue;\">- Cosmics Spectral Index []</span>\\\n",
    "<span style=\"color:darkblue;\">- Light Yield [p.e./p.e.]</span>\n",
    "#### <span style=\"color:darkred;\">- Weather:</span>\n",
    "<span style=\"color:darkblue;\">- Temperature [CÂº]</span>\\\n",
    "<span style=\"color:darkblue;\">- Pressure [mmHg]</span>\\\n",
    "<span style=\"color:darkblue;\">- Humidity [%]</span>\\\n",
    "<span style=\"color:darkblue;\">- Wind Speed [km/h]</span>\\\n",
    "<span style=\"color:darkblue;\">- Wind Gust [km/h]</span>\\\n",
    "<span style=\"color:darkblue;\">- Wind Speed Average [km/h]</span>\\\n",
    "<span style=\"color:darkblue;\">- TNG Dust [$\\micro g/m^3$]</span>\\\n",
    "<span style=\"color:darkblue;\">- TNG Seeing [arcsecond]</span>\\\n",
    "<span style=\"color:darkblue;\">- Rain [tbd]</span>\n",
    "\n",
    "\n",
    "\n",
    "## Datacheck `cosmics_intensity_spectrum` (subrun-wise)\n",
    "Contains:\\\n",
    "yyyymmdd, ra_tel, dec_tel, cos_zenith, az_tel, runnumber,\n",
    "       subrun, time, elapsed_time, corrected_elapsed_time,\n",
    "       cosmics_rate, cosmics_cleaned_rate, intensity_at_half_peak_rate,\n",
    "       ZD_corrected_intensity_at_half_peak_rate, cosmics_peak_rate,\n",
    "       ZD_corrected_cosmics_peak_rate, cosmics_rate_at_422_pe,\n",
    "       ZD_corrected_cosmics_rate_at_422_pe, cosmics_spectral_index,\n",
    "       ZD_corrected_cosmics_spectral_index, intensity_spectrum_fit_p_value,\n",
    "       intensity_at_reference_rate, diffuse_nsb_std,\n",
    "       num_star_affected_pixels, anomalous_low_intensity_peak\n",
    "\n",
    "## Datachek `runsummary` (run-wise)\n",
    "Contains:\\\n",
    "runnumber, time, elapsed_time, min_altitude, mean_altitude,\n",
    "       max_altitude, min_azimuth, max_azimuth, mean_azimuth, mean_ra,\n",
    "       mean_dec, num_cosmics, num_pedestals, num_flatfield,\n",
    "       num_unknown_ucts_trigger_tags, num_wrong_ucts_tags_in_cosmics,\n",
    "       num_wrong_ucts_tags_in_pedestals, num_wrong_ucts_tags_in_flatfield,\n",
    "       num_ucts_jumps, num_unknown_tib_trigger_tags,\n",
    "       num_wrong_tib_tags_in_cosmics, num_wrong_tib_tags_in_pedestals,\n",
    "       num_wrong_tib_tags_in_flatfield, num_pedestals_after_cleaning,\n",
    "       num_contained_mu_rings, ff_charge_mean, ff_charge_mean_err,\n",
    "       ff_charge_stddev, ff_time_mean, ff_time_mean_err,\n",
    "       ff_time_stddev, ff_rel_time_stddev, ped_charge_mean,\n",
    "       ped_charge_mean_err, ped_charge_stddev,\n",
    "       ped_fraction_pulses_above10, ped_fraction_pulses_above30,\n",
    "       cosmics_fraction_pulses_above10, cosmics_fraction_pulses_above30,\n",
    "       mu_effi_mean, mu_effi_stddev, mu_width_mean, mu_width_stddev,\n",
    "       mu_hg_peak_sample_mean, mu_hg_peak_sample_stddev,\n",
    "       mu_intensity_mean, mean_number_of_pixels_nearby_stars\n",
    "       \n",
    "## Weather Station data\n",
    "Contains:\\\n",
    "sun_alt, sun_az, fBits, mjd, temperature, pressure,\n",
    "       windDirection, humidity, windSpeedCurrent, windGust,\n",
    "       windSpeedAverage, windDirectionAverage, tempSensor, tngDust,\n",
    "       tngSeeing, rain, state, Any, Mes, DP, diff1, is_dup,\n",
    "       temperatureR\n",
    "\n",
    "#### Import needed packages and scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9c64ef8-c4a0-429b-b73c-4583cb3e9cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from datetime import datetime\n",
    "import pickle, json, sys, os, glob\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Display settings for Jupyter Notebook\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# Importing custom utility functions\n",
    "sys.path.insert(0, os.getcwd() + \"/../scripts/\")\n",
    "import auxiliar as aux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b14c99a-90db-42ea-8f1a-ca49125cb941",
   "metadata": {},
   "source": [
    "### Paths to data and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d01a3051-7ed0-445a-9abf-0cb82e6f4007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows for each job\n",
    "n_rows = 6000\n",
    "\n",
    "# Root path of this script\n",
    "root = os.getcwd() + \"/\"\n",
    "# Objects directory\n",
    "root_objects = root + \"objects/\"\n",
    "\n",
    "# Directory of all the night-wise datachecks\n",
    "root_dchecks = \"/fefs/aswg/workspace/abelardo.moralejo/data/datachecks/night_wise/DL1_datacheck_\"\n",
    "# Weather station file\n",
    "ws_database = root_objects + \"WS2003-23.h5\"\n",
    "\n",
    "# Some filenames -------------------\n",
    "# Filename of the datacheck dictionary\n",
    "fname_datacheck_dict = root_objects + \"datacheck_dict.pkl\"\n",
    "# Filename of the total dictionary\n",
    "fname_total_dict = root_objects + \"total_dict.pkl\"\n",
    "# Job list file\n",
    "fname_job_list = root_objects + \"bash_job_list.txt\"\n",
    "# Filename of the relation between run and night\n",
    "fname_run_night_relation = root_objects + \"ws_run_relation.txt\"\n",
    "\n",
    "\n",
    "# Flags for computing or not different parts\n",
    "# Compute the datacheck dictionary\n",
    "compute_datacheck_dict = False\n",
    "# Send all the bash jobs to the cluster\n",
    "send_jobs = True\n",
    "\n",
    "# Create needed folders\n",
    "for dir in [root_objects, root_objects + \"output_slurm/\"]:\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76748282-b2c5-469b-81ed-db2c2a664f54",
   "metadata": {},
   "source": [
    "### Extracting dates and parameters of all runs/subruns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4e9f54b-2cde-4177-932e-839fd6fd782a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 262 ms, sys: 185 ms, total: 447 ms\n",
      "Wall time: 447 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if compute_datacheck_dict:\n",
    "\n",
    "    run_number   = [] # Run numbers\n",
    "    srun_number  = [] # Subrun numbers\n",
    "    timestamps   = [] # Timestamps of each subrun\n",
    "    time_elapsed = [] # Elapsed time of each subrun\n",
    "    mean_azimuth = [] # Mean azimuth of each run\n",
    "    mean_zenith_distance = [] # Mean zenith of each run\n",
    "    zd_corrected_intensity_at_half_peak_rate = [] # ZD corrected intensity at half peak rate\n",
    "    zd_corrected_cosmics_rate_at_422_pe = [] # ZD corrected cosmics rate at 422 pe\n",
    "    zd_corrected_delta_cosmics_rate_at_422_pe = [] # ZD corrected delta cosmics rate at 422 pe\n",
    "    zd_corrected_cosmics_spectral_index = [] # ZD corrected cosmics spectral index\n",
    "    zd_corrected_delta_cosmics_spectral_index = [] # ZD corrected delta cosmics spectral index\n",
    "    light_yield = [] # Light yield\n",
    "\n",
    "    # All the datachecks for all the nights\n",
    "    dchecks = np.sort(glob.glob(root_dchecks + \"*.h5\"))\n",
    "\n",
    "    # We iterate over all the datachecks\n",
    "    for i, dcheck in enumerate(dchecks):\n",
    "\n",
    "        print(f\"Analysing... {i:3}/{len(dchecks)}\") if i % 30 == 0 else None\n",
    "\n",
    "        # The datacheck file of the run summary (runwise)\n",
    "        ds = pd.read_hdf(dcheck, key=\"runsummary\")\n",
    "        # The datacheck file of the intensity spectrums (subrunwise)\n",
    "        di = pd.read_hdf(dcheck, key=\"cosmics_intensity_spectrum\")\n",
    "        \n",
    "        # Iterating over all the entries of each night, the subruns\n",
    "        for j in range(len(ds)):\n",
    "\n",
    "            # Reference run number\n",
    "            runref = ds[\"runnumber\"].iloc[j]\n",
    "            \n",
    "            # Intensity datacheck for only the subruns of the reference run\n",
    "            di_run = di.query(f\"runnumber == {runref}\")\n",
    "            \n",
    "            # Subrun iteration and storing all the data we are interested in\n",
    "            for k in range(len(di_run)):\n",
    "\n",
    "                run_number.append(runref)\n",
    "                srun_number.append(di_run[\"subrun\"].iloc[k])\n",
    "                timestamps.append(datetime.fromtimestamp(di_run[\"time\"].iloc[k]))\n",
    "                time_elapsed.append(di_run[\"corrected_elapsed_time\"].iloc[k])\n",
    "                mean_azimuth.append(ds[\"mean_azimuth\"].iloc[j])\n",
    "                mean_zenith_distance.append(np.arccos(di_run[\"cos_zenith\"].iloc[k]))\n",
    "                zd_corrected_intensity_at_half_peak_rate.append(di_run[\"ZD_corrected_intensity_at_half_peak_rate\"].iloc[k])\n",
    "                zd_corrected_cosmics_rate_at_422_pe.append(di_run[\"ZD_corrected_cosmics_rate_at_422_pe\"].iloc[k])\n",
    "                zd_corrected_delta_cosmics_rate_at_422_pe.append(di_run[\"ZD_corrected_delta_cosmics_rate_at_422_pe\"].iloc[k])\n",
    "                zd_corrected_cosmics_spectral_index.append(di_run[\"ZD_corrected_cosmics_spectral_index\"].iloc[k])\n",
    "                zd_corrected_delta_cosmics_spectral_index.append(di_run[\"delta_cosmics_spectral_index\"].iloc[k])\n",
    "                light_yield.append(di_run[\"light_yield\"].iloc[k])            \n",
    "\n",
    "    print(f\"Analysing... {len(dchecks):3}/{len(dchecks)}\\n\")\n",
    "\n",
    "    # Now we are going to sort looking to the timestamps\n",
    "    _, run_number = aux.sort_based(run_number, timestamps)\n",
    "    _, srun_number = aux.sort_based(srun_number, timestamps)\n",
    "    _, time_elapsed = aux.sort_based(time_elapsed, timestamps)\n",
    "    _, mean_azimuth = aux.sort_based(mean_azimuth, timestamps)\n",
    "    _, mean_zenith_distance = aux.sort_based(mean_zenith_distance, timestamps)\n",
    "    _, zd_corrected_intensity_at_half_peak_rate = aux.sort_based(zd_corrected_intensity_at_half_peak_rate, timestamps)\n",
    "    _, zd_corrected_cosmics_rate_at_422_pe = aux.sort_based(zd_corrected_cosmics_rate_at_422_pe, timestamps)\n",
    "    _, zd_corrected_delta_cosmics_rate_at_422_pe = aux.sort_based(zd_corrected_delta_cosmics_rate_at_422_pe, timestamps)\n",
    "    _, zd_corrected_cosmics_spectral_index = aux.sort_based(zd_corrected_cosmics_spectral_index, timestamps)\n",
    "    _, zd_corrected_delta_cosmics_spectral_index = aux.sort_based(zd_corrected_delta_cosmics_spectral_index, timestamps)\n",
    "    timestamps, light_yield = aux.sort_based(light_yield, timestamps)\n",
    "\n",
    "    # Creating the data dictionary\n",
    "    dict_dcheck = {\n",
    "        \"run\" : np.array(run_number),\n",
    "        \"srun\" : np.array(srun_number),\n",
    "        \"time\" : np.array(timestamps),\n",
    "        \"telapsed\" : np.array(time_elapsed),\n",
    "        \"az\" : np.rad2deg(mean_azimuth),\n",
    "        \"zd\" : np.rad2deg(mean_zenith_distance),\n",
    "        \"ZD_corrected_intensity_at_half_peak_rate\" : np.array(zd_corrected_intensity_at_half_peak_rate),\n",
    "        \"ZD_corrected_cosmics_rate_at_422_pe\" : np.array(zd_corrected_cosmics_rate_at_422_pe),\n",
    "        \"ZD_corrected_delta_cosmics_rate_at_422_pe\" : np.array(zd_corrected_delta_cosmics_rate_at_422_pe),\n",
    "        \"ZD_corrected_cosmics_spectral_index\" : np.array(zd_corrected_cosmics_spectral_index),\n",
    "        \"delta_cosmics_spectral_index\" : np.array(zd_corrected_delta_cosmics_spectral_index),\n",
    "        \"light_yield\" : np.array(light_yield)\n",
    "    }        \n",
    "\n",
    "    # Saving the objects in the objects directory\n",
    "    with open(fname_datacheck_dict, 'wb') as f:\n",
    "        pickle.dump(dict_dcheck, f, pickle.HIGHEST_PROTOCOL)  \n",
    "else:\n",
    "    # To read the file:\n",
    "    with open(fname_datacheck_dict, 'rb') as f:\n",
    "            dict_dcheck = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff6f7cc",
   "metadata": {},
   "source": [
    "### Creating the total dictionary run-subrun-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ec69328",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dict = {}\n",
    "\n",
    "# We create an entry per run\n",
    "for run in np.unique(dict_dcheck[\"run\"]):\n",
    "    total_dict[run] = {}\n",
    "\n",
    "# Converting dcheck dictionary to total dictionary\n",
    "for i in range(len(dict_dcheck[\"run\"])):\n",
    "\n",
    "    total_dict[dict_dcheck[\"run\"][i]][dict_dcheck[\"srun\"][i]] = {\n",
    "        \"time\" : dict_dcheck[\"time\"][i],\n",
    "        \"telapsed\" : dict_dcheck[\"telapsed\"][i],\n",
    "        \"az\" : dict_dcheck[\"az\"][i],\n",
    "        \"zd\" : dict_dcheck[\"zd\"][i],\n",
    "        \"ZD_corrected_intensity_at_half_peak_rate\" : dict_dcheck[\"ZD_corrected_intensity_at_half_peak_rate\"][i],\n",
    "        \"ZD_corrected_cosmics_rate_at_422_pe\" : dict_dcheck[\"ZD_corrected_cosmics_rate_at_422_pe\"][i],\n",
    "        \"ZD_corrected_delta_cosmics_rate_at_422_pe\" : dict_dcheck[\"ZD_corrected_delta_cosmics_rate_at_422_pe\"][i],\n",
    "        \"ZD_corrected_cosmics_spectral_index\" : dict_dcheck[\"ZD_corrected_cosmics_spectral_index\"][i],\n",
    "        \"delta_cosmics_spectral_index\" : dict_dcheck[\"delta_cosmics_spectral_index\"][i],        \n",
    "        \"light_yield\" : dict_dcheck[\"light_yield\"][i]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdf3a27-3dbf-409e-b9a1-4588a1630362",
   "metadata": {},
   "source": [
    "#### Reading the WS table and we reduce it to the part we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e384b3e-6e12-4556-89c4-f69f4c88357c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading the weather station database\n",
    "df_ws = pd.read_hdf(ws_database)\n",
    "\n",
    "# Loading the timestamp of each entry in the datacheck dictionary\n",
    "dates_dcheck = dict_dcheck[\"time\"]\n",
    "\n",
    "# Getting the min and max dates\n",
    "maxdate = np.max(dates_dcheck)\n",
    "mindate = np.min(dates_dcheck)\n",
    "\n",
    "# Converting the weather station dates to datetime objects\n",
    "dates_ws = np.array([datetime.fromisoformat(str(d).split(\".\")[0]) for d in df_ws.index])\n",
    "\n",
    "# Getting the max date of the weather station\n",
    "maxdate_ws = np.max(dates_ws)\n",
    "\n",
    "# Masking the weather station data to the min and max dates of the datacheck dictionary\n",
    "mask_dates  = ((dates_ws > mindate) & (dates_ws < maxdate))\n",
    "\n",
    "# Masking also for day data, i.e. sun_alt > 0 we are not interested in \n",
    "mask_night = (df_ws[\"sun_alt\"] < 0)\n",
    "\n",
    "total_mask = (mask_dates & mask_night)\n",
    "\n",
    "dates_ws = dates_ws[total_mask]\n",
    "df_ws    = df_ws[total_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b47fb89-a53d-4386-8db5-ba5a7e439cee",
   "metadata": {},
   "source": [
    "### Separating in bunchs of small number of jobs and writting into a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85c8366c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With groups of 6000 subruns, the number of prepared jobs is 126\n"
     ]
    }
   ],
   "source": [
    "start_indexes = []\n",
    "end_indexes   = []\n",
    "\n",
    "i, total = 0, 0\n",
    "while total < len(dict_dcheck[\"run\"]):\n",
    "    start_indexes.append(total)\n",
    "    end_indexes.append(total + n_rows - 1)\n",
    "    \n",
    "    i     += 1\n",
    "    total += n_rows\n",
    "\n",
    "print(f\"With groups of {n_rows} subruns, the number of prepared jobs is {len(start_indexes)}\")\n",
    "\n",
    "# Opening a new txt file with a job per column\n",
    "file_job_list = open(fname_job_list, \"w\")\n",
    "\n",
    "for s, e in zip(start_indexes, end_indexes):\n",
    "    file_job_list.write(f\"{s},{e}\\n\") \n",
    "\n",
    "file_job_list.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea4ec1e",
   "metadata": {},
   "source": [
    "### Launching the jobs to the queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "727c6f33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending job 0,5999 to the queue...\n",
      "\n",
      "Submitted batch job 31938240\n",
      "Sending job 6000,11999 to the queue...\n",
      "\n",
      "Submitted batch job 31938241\n",
      "Sending job 12000,17999 to the queue...\n",
      "\n",
      "Submitted batch job 31938242\n",
      "Sending job 18000,23999 to the queue...\n",
      "\n",
      "Submitted batch job 31938243\n",
      "Sending job 24000,29999 to the queue...\n",
      "\n",
      "Submitted batch job 31938244\n",
      "Sending job 30000,35999 to the queue...\n",
      "\n",
      "Submitted batch job 31938245\n",
      "Sending job 36000,41999 to the queue...\n",
      "\n",
      "Submitted batch job 31938246\n",
      "Sending job 42000,47999 to the queue...\n",
      "\n",
      "Submitted batch job 31938247\n",
      "Sending job 48000,53999 to the queue...\n",
      "\n",
      "Submitted batch job 31938248\n",
      "Sending job 54000,59999 to the queue...\n",
      "\n",
      "Submitted batch job 31938249\n",
      "Sending job 60000,65999 to the queue...\n",
      "\n",
      "Submitted batch job 31938250\n",
      "Sending job 66000,71999 to the queue...\n",
      "\n",
      "Submitted batch job 31938251\n",
      "Sending job 72000,77999 to the queue...\n",
      "\n",
      "Submitted batch job 31938252\n",
      "Sending job 78000,83999 to the queue...\n",
      "\n",
      "Submitted batch job 31938253\n",
      "Sending job 84000,89999 to the queue...\n",
      "\n",
      "Submitted batch job 31938254\n",
      "Sending job 90000,95999 to the queue...\n",
      "\n",
      "Submitted batch job 31938255\n",
      "Sending job 96000,101999 to the queue...\n",
      "\n",
      "Submitted batch job 31938256\n",
      "Sending job 102000,107999 to the queue...\n",
      "\n",
      "Submitted batch job 31938257\n",
      "Sending job 108000,113999 to the queue...\n",
      "\n",
      "Submitted batch job 31938258\n",
      "Sending job 114000,119999 to the queue...\n",
      "\n",
      "Submitted batch job 31938259\n",
      "Sending job 120000,125999 to the queue...\n",
      "\n",
      "Submitted batch job 31938260\n",
      "Sending job 126000,131999 to the queue...\n",
      "\n",
      "Submitted batch job 31938261\n",
      "Sending job 132000,137999 to the queue...\n",
      "\n",
      "Submitted batch job 31938262\n",
      "Sending job 138000,143999 to the queue...\n",
      "\n",
      "Submitted batch job 31938263\n",
      "Sending job 144000,149999 to the queue...\n",
      "\n",
      "Submitted batch job 31938264\n",
      "Sending job 150000,155999 to the queue...\n",
      "\n",
      "Submitted batch job 31938265\n",
      "Sending job 156000,161999 to the queue...\n",
      "\n",
      "Submitted batch job 31938266\n",
      "Sending job 162000,167999 to the queue...\n",
      "\n",
      "Submitted batch job 31938267\n",
      "Sending job 168000,173999 to the queue...\n",
      "\n",
      "Submitted batch job 31938268\n",
      "Sending job 174000,179999 to the queue...\n",
      "\n",
      "Submitted batch job 31938269\n",
      "Sending job 180000,185999 to the queue...\n",
      "\n",
      "Submitted batch job 31938270\n",
      "Sending job 186000,191999 to the queue...\n",
      "\n",
      "Submitted batch job 31938271\n",
      "Sending job 192000,197999 to the queue...\n",
      "\n",
      "Submitted batch job 31938272\n",
      "Sending job 198000,203999 to the queue...\n",
      "\n",
      "Submitted batch job 31938273\n",
      "Sending job 204000,209999 to the queue...\n",
      "\n",
      "Submitted batch job 31938274\n",
      "Sending job 210000,215999 to the queue...\n",
      "\n",
      "Submitted batch job 31938275\n",
      "Sending job 216000,221999 to the queue...\n",
      "\n",
      "Submitted batch job 31938276\n",
      "Sending job 222000,227999 to the queue...\n",
      "\n",
      "Submitted batch job 31938277\n",
      "Sending job 228000,233999 to the queue...\n",
      "\n",
      "Submitted batch job 31938278\n",
      "Sending job 234000,239999 to the queue...\n",
      "\n",
      "Submitted batch job 31938279\n",
      "Sending job 240000,245999 to the queue...\n",
      "\n",
      "Submitted batch job 31938280\n",
      "Sending job 246000,251999 to the queue...\n",
      "\n",
      "Submitted batch job 31938281\n",
      "Sending job 252000,257999 to the queue...\n",
      "\n",
      "Submitted batch job 31938282\n",
      "Sending job 258000,263999 to the queue...\n",
      "\n",
      "Submitted batch job 31938283\n",
      "Sending job 264000,269999 to the queue...\n",
      "\n",
      "Submitted batch job 31938284\n",
      "Sending job 270000,275999 to the queue...\n",
      "\n",
      "Submitted batch job 31938285\n",
      "Sending job 276000,281999 to the queue...\n",
      "\n",
      "Submitted batch job 31938286\n",
      "Sending job 282000,287999 to the queue...\n",
      "\n",
      "Submitted batch job 31938287\n",
      "Sending job 288000,293999 to the queue...\n",
      "\n",
      "Submitted batch job 31938288\n",
      "Sending job 294000,299999 to the queue...\n",
      "\n",
      "Submitted batch job 31938289\n",
      "Sending job 300000,305999 to the queue...\n",
      "\n",
      "Submitted batch job 31938290\n",
      "Sending job 306000,311999 to the queue...\n",
      "\n",
      "Submitted batch job 31938291\n",
      "Sending job 312000,317999 to the queue...\n",
      "\n",
      "Submitted batch job 31938292\n",
      "Sending job 318000,323999 to the queue...\n",
      "\n",
      "Submitted batch job 31938293\n",
      "Sending job 324000,329999 to the queue...\n",
      "\n",
      "Submitted batch job 31938294\n",
      "Sending job 330000,335999 to the queue...\n",
      "\n",
      "Submitted batch job 31938295\n",
      "Sending job 336000,341999 to the queue...\n",
      "\n",
      "Submitted batch job 31938296\n",
      "Sending job 342000,347999 to the queue...\n",
      "\n",
      "Submitted batch job 31938297\n",
      "Sending job 348000,353999 to the queue...\n",
      "\n",
      "Submitted batch job 31938298\n",
      "Sending job 354000,359999 to the queue...\n",
      "\n",
      "Submitted batch job 31938299\n",
      "Sending job 360000,365999 to the queue...\n",
      "\n",
      "Submitted batch job 31938300\n",
      "Sending job 366000,371999 to the queue...\n",
      "\n",
      "Submitted batch job 31938301\n",
      "Sending job 372000,377999 to the queue...\n",
      "\n",
      "Submitted batch job 31938302\n",
      "Sending job 378000,383999 to the queue...\n",
      "\n",
      "Submitted batch job 31938303\n",
      "Sending job 384000,389999 to the queue...\n",
      "\n",
      "Submitted batch job 31938304\n",
      "Sending job 390000,395999 to the queue...\n",
      "\n",
      "Submitted batch job 31938305\n",
      "Sending job 396000,401999 to the queue...\n",
      "\n",
      "Submitted batch job 31938306\n",
      "Sending job 402000,407999 to the queue...\n",
      "\n",
      "Submitted batch job 31938307\n",
      "Sending job 408000,413999 to the queue...\n",
      "\n",
      "Submitted batch job 31938308\n",
      "Sending job 414000,419999 to the queue...\n",
      "\n",
      "Submitted batch job 31938309\n",
      "Sending job 420000,425999 to the queue...\n",
      "\n",
      "Submitted batch job 31938310\n",
      "Sending job 426000,431999 to the queue...\n",
      "\n",
      "Submitted batch job 31938311\n",
      "Sending job 432000,437999 to the queue...\n",
      "\n",
      "Submitted batch job 31938312\n",
      "Sending job 438000,443999 to the queue...\n",
      "\n",
      "Submitted batch job 31938313\n",
      "Sending job 444000,449999 to the queue...\n",
      "\n",
      "Submitted batch job 31938314\n",
      "Sending job 450000,455999 to the queue...\n",
      "\n",
      "Submitted batch job 31938315\n",
      "Sending job 456000,461999 to the queue...\n",
      "\n",
      "Submitted batch job 31938316\n",
      "Sending job 462000,467999 to the queue...\n",
      "\n",
      "Submitted batch job 31938317\n",
      "Sending job 468000,473999 to the queue...\n",
      "\n",
      "Submitted batch job 31938318\n",
      "Sending job 474000,479999 to the queue...\n",
      "\n",
      "Submitted batch job 31938319\n",
      "Sending job 480000,485999 to the queue...\n",
      "\n",
      "Submitted batch job 31938320\n",
      "Sending job 486000,491999 to the queue...\n",
      "\n",
      "Submitted batch job 31938321\n",
      "Sending job 492000,497999 to the queue...\n",
      "\n",
      "Submitted batch job 31938322\n",
      "Sending job 498000,503999 to the queue...\n",
      "\n",
      "Submitted batch job 31938323\n",
      "Sending job 504000,509999 to the queue...\n",
      "\n",
      "Submitted batch job 31938324\n",
      "Sending job 510000,515999 to the queue...\n",
      "\n",
      "Submitted batch job 31938325\n",
      "Sending job 516000,521999 to the queue...\n",
      "\n",
      "Submitted batch job 31938326\n",
      "Sending job 522000,527999 to the queue...\n",
      "\n",
      "Submitted batch job 31938327\n",
      "Sending job 528000,533999 to the queue...\n",
      "\n",
      "Submitted batch job 31938328\n",
      "Sending job 534000,539999 to the queue...\n",
      "\n",
      "Submitted batch job 31938329\n",
      "Sending job 540000,545999 to the queue...\n",
      "\n",
      "Submitted batch job 31938330\n",
      "Sending job 546000,551999 to the queue...\n",
      "\n",
      "Submitted batch job 31938331\n",
      "Sending job 552000,557999 to the queue...\n",
      "\n",
      "Submitted batch job 31938332\n",
      "Sending job 558000,563999 to the queue...\n",
      "\n",
      "Submitted batch job 31938333\n",
      "Sending job 564000,569999 to the queue...\n",
      "\n",
      "Submitted batch job 31938334\n",
      "Sending job 570000,575999 to the queue...\n",
      "\n",
      "Submitted batch job 31938335\n",
      "Sending job 576000,581999 to the queue...\n",
      "\n",
      "Submitted batch job 31938336\n",
      "Sending job 582000,587999 to the queue...\n",
      "\n",
      "Submitted batch job 31938337\n",
      "Sending job 588000,593999 to the queue...\n",
      "\n",
      "Submitted batch job 31938338\n",
      "Sending job 594000,599999 to the queue...\n",
      "\n",
      "Submitted batch job 31938339\n",
      "Sending job 600000,605999 to the queue...\n",
      "\n",
      "Submitted batch job 31938340\n",
      "Sending job 606000,611999 to the queue...\n",
      "\n",
      "Submitted batch job 31938341\n",
      "Sending job 612000,617999 to the queue...\n",
      "\n",
      "Submitted batch job 31938342\n",
      "Sending job 618000,623999 to the queue...\n",
      "\n",
      "Submitted batch job 31938343\n",
      "Sending job 624000,629999 to the queue...\n",
      "\n",
      "Submitted batch job 31938344\n",
      "Sending job 630000,635999 to the queue...\n",
      "\n",
      "Submitted batch job 31938345\n",
      "Sending job 636000,641999 to the queue...\n",
      "\n",
      "Submitted batch job 31938346\n",
      "Sending job 642000,647999 to the queue...\n",
      "\n",
      "Submitted batch job 31938347\n",
      "Sending job 648000,653999 to the queue...\n",
      "\n",
      "Submitted batch job 31938348\n",
      "Sending job 654000,659999 to the queue...\n",
      "\n",
      "Submitted batch job 31938349\n",
      "Sending job 660000,665999 to the queue...\n",
      "\n",
      "Submitted batch job 31938350\n",
      "Sending job 666000,671999 to the queue...\n",
      "\n",
      "Submitted batch job 31938351\n",
      "Sending job 672000,677999 to the queue...\n",
      "\n",
      "Submitted batch job 31938352\n",
      "Sending job 678000,683999 to the queue...\n",
      "\n",
      "Submitted batch job 31938353\n",
      "Sending job 684000,689999 to the queue...\n",
      "\n",
      "Submitted batch job 31938354\n",
      "Sending job 690000,695999 to the queue...\n",
      "\n",
      "Submitted batch job 31938355\n",
      "Sending job 696000,701999 to the queue...\n",
      "\n",
      "Submitted batch job 31938356\n",
      "Sending job 702000,707999 to the queue...\n",
      "\n",
      "Submitted batch job 31938357\n",
      "Sending job 708000,713999 to the queue...\n",
      "\n",
      "Submitted batch job 31938358\n",
      "Sending job 714000,719999 to the queue...\n",
      "\n",
      "Submitted batch job 31938359\n",
      "Sending job 720000,725999 to the queue...\n",
      "\n",
      "Submitted batch job 31938360\n",
      "Sending job 726000,731999 to the queue...\n",
      "\n",
      "Submitted batch job 31938361\n",
      "Sending job 732000,737999 to the queue...\n",
      "\n",
      "Submitted batch job 31938362\n",
      "Sending job 738000,743999 to the queue...\n",
      "\n",
      "Submitted batch job 31938363\n",
      "Sending job 744000,749999 to the queue...\n",
      "\n",
      "Submitted batch job 31938364\n",
      "Sending job 750000,755999 to the queue...\n",
      "\n",
      "Submitted batch job 31938365\n"
     ]
    }
   ],
   "source": [
    "if send_jobs == True:\n",
    "    # Creating a file to store the results of the jobs\n",
    "    file_results = open(fname_run_night_relation, \"w\")\n",
    "    file_results.write(\"# Run - Subrun , WS entry id (date in ISO format)\")\n",
    "    file_results.close()\n",
    "    \n",
    "    \n",
    "    # Launching the jobs\n",
    "    !sh bash_jobs_indexes_ws_run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac284e5",
   "metadata": {},
   "source": [
    "### <span style=\"color:red;\">------------------------------------------------------------------------------------------------------------------</span>\n",
    "### <span style=\"color:red;\">------------------------------------------------------------------------------------------------------------------</span>\n",
    "\n",
    "### <span style=\"color:red;\"> Wait untill the jobs are processed and then the results need to be fully stored </span>\n",
    "### <span style=\"color:red;\">------------------------------------------------------------------------------------------------------------------</span>\n",
    "### <span style=\"color:red;\">------------------------------------------------------------------------------------------------------------------</span>\n",
    "\n",
    "#### Now we read the results file where we associate each subrun to a entry of the WS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3888946-9eb7-4d5c-bb81-d73ba12c747f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.54 s, sys: 234 ms, total: 2.78 s\n",
      "Wall time: 3.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Reading the results\n",
    "file_results_lines = np.loadtxt(fname_run_night_relation, dtype=str, delimiter=\",\")\n",
    "\n",
    "# Creating a dictionary to organise them\n",
    "dict_results = {}\n",
    "\n",
    "for line in file_results_lines:\n",
    "\n",
    "    runsubrun, date_str = line\n",
    "\n",
    "    run, srun = runsubrun.split(\"-\")\n",
    "    run = int(run)\n",
    "    srun = int(srun)\n",
    "\n",
    "    date_str = date_str if date_str != \"None\" else None\n",
    "\n",
    "    try:\n",
    "        dict_results[run][srun] = date_str    \n",
    "    except KeyError:\n",
    "        dict_results[run] = {srun : date_str}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2767e73-7c7a-4904-a6e7-e864ec9e6bcd",
   "metadata": {},
   "source": [
    "#### Now the weather station data can be added to the total dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b24fcde-3787-485b-beb7-6e74ca061fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding data...      0/7528 runs\n",
      "Adding data...    500/7528 runs\n",
      "Adding data...   1000/7528 runs\n",
      "Adding data...   1500/7528 runs\n",
      "Adding data...   2000/7528 runs\n",
      "Adding data...   2500/7528 runs\n",
      "Adding data...   3000/7528 runs\n",
      "Adding data...   3500/7528 runs\n",
      "Adding data...   4000/7528 runs\n",
      "Adding data...   4500/7528 runs\n",
      "Adding data...   5000/7528 runs\n",
      "Adding data...   5500/7528 runs\n",
      "Adding data...   6000/7528 runs\n",
      "Adding data...   6500/7528 runs\n",
      "Adding data...   7000/7528 runs\n",
      "Adding data...   7500/7528 runs\n",
      "CPU times: user 12min 39s, sys: 7.49 s, total: 12min 46s\n",
      "Wall time: 12min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, run in enumerate(total_dict.keys()):\n",
    "\n",
    "    print(f\"Adding data... {i:6}/{len(total_dict.keys())} runs\") if i % 500 == 0 else None\n",
    "        \n",
    "    for srun in total_dict[run].keys():\n",
    "\n",
    "        try:\n",
    "            string_date = dict_results[run][srun]\n",
    "\n",
    "            if string_date != None:\n",
    "                empty_flag = False\n",
    "            else:\n",
    "                empty_flag = True\n",
    "        except KeyError:\n",
    "            empty_flag = True\n",
    "\n",
    "        if not empty_flag:\n",
    "            try:\n",
    "                total_dict[run][srun][\"weather\"] = {\n",
    "                    \"temperature\" :        df_ws.loc[string_date][\"temperature\"],      # degree celsius\n",
    "                    \"pressure\" :           df_ws.loc[string_date][\"pressure\"],         # mmHg\n",
    "                    \"humidity\" :           df_ws.loc[string_date][\"humidity\"],         # %\n",
    "                    \"wind_speed\" :         df_ws.loc[string_date][\"windSpeedCurrent\"], # km/h\n",
    "                    \"wind_gust\" :          df_ws.loc[string_date][\"windGust\"],         # km/h\n",
    "                    \"wind_speed_average\" : df_ws.loc[string_date][\"windSpeedAverage\"], # km/s\n",
    "                    \"tng_dust\" :           df_ws.loc[string_date][\"tngDust\"],          # ug/m3\n",
    "                    \"tng_seeing\" :         df_ws.loc[string_date][\"tngSeeing\"],        # arcseconds\n",
    "                    \"rain\" :               df_ws.loc[string_date][\"rain\"],             #\n",
    "                }\n",
    "            except KeyError:\n",
    "                print(f\"KeyError in Run {run}, Subrun {srun} with entry ID {string_date}.\")\n",
    "                empty_flag = True\n",
    "\n",
    "        if empty_flag:            \n",
    "            total_dict[run][srun][\"weather\"] = {\n",
    "                \"temperature\" :        None, # degree celsius\n",
    "                \"pressure\" :           None, # mmHg\n",
    "                \"humidity\" :           None, # %\n",
    "                \"wind_speed\" :         None, # km/h\n",
    "                \"wind_gust\" :          None, # km/h\n",
    "                \"wind_speed_average\" : None, # km/s\n",
    "                \"tng_dust\" :           None, # ug/m3\n",
    "                \"tng_seeing\" :         None, # arcseconds\n",
    "                \"rain\" :               None, #\n",
    "            } "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a938425",
   "metadata": {},
   "source": [
    "#### Saving the dictionary with all the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "645e5b53-9f21-4f45-86df-c331f06cd185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the object\n",
    "with open(fname_total_dict, 'wb') as f:\n",
    "    pickle.dump(total_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # To read the file:\n",
    "# with open(fname_total_dict, 'rb') as f:\n",
    "#         total_dict = pickle.load(f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea91827-83d8-4066-a4c4-707afe3f4270",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
