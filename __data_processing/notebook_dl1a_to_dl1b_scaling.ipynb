{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e70776f7-e2e1-4057-b7ef-878b2be0556d",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b3b4638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from datetime import datetime\n",
    "import pickle, json, sys, os\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import chi2\n",
    "from scipy import optimize\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from traitlets.config.loader import Config\n",
    "from astropy.coordinates     import SkyCoord\n",
    "from lstchain.io.config      import get_standard_config\n",
    "from ctapipe.io              import read_table\n",
    "import tables\n",
    "\n",
    "# Other auxiliar scripts\n",
    "sys.path.insert(0, os.getcwd() + \"/../scripts/\")\n",
    "import auxiliar as aux\n",
    "import geometry as geom\n",
    "import lstpipeline\n",
    "import plotting\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cff6f04",
   "metadata": {},
   "source": [
    "# Some configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d56ff173",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Source name in order to just complete the results file, and\n",
    "in order to improve run organization.\"\"\"\n",
    "source_name = \"crab\"\n",
    "\n",
    "\"\"\" The run number that we are interested in apply the corrections.\n",
    "The process is done run-wise, so the input will be an individual run.\"\"\"\n",
    "run_number = 6172\n",
    "\n",
    "\"\"\" Fit parameters\n",
    "Chosen limits in intensity (p.e.) for applying the fit i.e. the\n",
    "power law will be fitted only with the points within this range.\"\"\"\n",
    "limits_intensity = [316, 562]\n",
    "\"\"\" For the positive scaling cases (most of them), we need to have a lower \n",
    "limit in intensity. Thi slimit is used for the subset of events that are \n",
    "scaled just to find which is the scaling value. We use a very low limit by\n",
    "default 60 p.e. compared to the lower limit of the fit 316 p.e. because in \n",
    "the worst cases we will have a very non-linear scaling that will displace \n",
    "significantly the events intensities.\"\"\"\n",
    "limits_intensity_extended = 60\n",
    "\n",
    "\"\"\" Power law parameters for the reference\n",
    "All these parameters are taken from a common analysis of the full dataset\n",
    "Where the period of end of 2022 and start 2023 is taken as reference for good \n",
    "runs. Then we take as reference the mean power law parameters in that period.\n",
    "p0 is the normalization factor and p1 is the slope.\"\"\"\n",
    "ref_p0 =  1.74 \n",
    "ref_p1 = -2.23\n",
    "\n",
    "\"\"\" Threshold in statistics for the last subrun\n",
    "The limit in number of events after cleaning that we need to consider the last\n",
    "subrun has enough statistics to perform the analysis over it. Otherwise the \n",
    "values of the scaling that will be applied to this last rubrun are the same \n",
    "that are applied to the last last subrun.\"\"\"\n",
    "statistics_threshold_last_srun = 15000\n",
    "\n",
    "\"\"\" Parameters for the empyrical fits for Zenith Distance corrections\n",
    "Are simply two 2 degree polynomials for each variable of the power law.\"\"\"\n",
    "p0a, p0b, p0c = -0.44751321, 3.62502037, -1.43611437\n",
    "p1a, p1b, p1c = -2.89253919, 0.99443581, -0.34013068\n",
    "\n",
    "\"\"\" Empty dictionary to store all the results of one run.\"\"\"\n",
    "dict_results_empty = { \n",
    "    \"run\": run_number, \"filenames\": {},\n",
    "    \"scaled\" :           {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "    \"p0\":                {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "    \"delta_p0\":          {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "    \"p1\":                {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "    \"delta_p1\":          {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "    \"chi2\":              {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "    \"pvalue\":            {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "    \"light_yield\":       {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "    \"delta_light_yield\": {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "    \"scaling\":           {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "    \"delta_scaling\":     {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "    \"scaling_percent\":       {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "    \"delta_scaling_percent\": {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "    \"final_scaling\": {}, \"final_scaling_interpolated\": {}, \"interpolation\" : {},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ea7368",
   "metadata": {},
   "source": [
    "# Paths to data and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66854bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root path of this script\n",
    "root = os.getcwd() + \"/\"\n",
    "# Path to store the configuration file we are going to use\n",
    "config_file = root + \"config/standard_config.json\"\n",
    "# Path to store objects\n",
    "root_objects = root + f\"objects/\"\n",
    "# Data main directory\n",
    "root_data = root + f\"../../data/cherenkov_transparency_corrections/{source_name}/\"\n",
    "# Directory for the results of the fit of each run\n",
    "root_results = root_objects + \"results_fits/\"\n",
    "\n",
    "# STANDARD paths ---------\n",
    "root_dl1 = \"/fefs/aswg/data/real/DL1/*/v0.*/tailcut84/\"\n",
    "root_rfs = \"/fefs/aswg/data/models/AllSky/20230901_v0.10.4_allsky_base_prod/\"\n",
    "root_mcs = \"/fefs/aswg/data/mc/DL2/AllSky/20230901_v0.10.4_allsky_base_prod/TestingDataset/\"\n",
    "\n",
    "# Create the paths that do not exist\n",
    "for path in [os.path.dirname(config_file), root_objects, root_results]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(os.path.join(path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb8d10f",
   "metadata": {},
   "source": [
    "# Opening and storing configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5030e785",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_config = get_standard_config()\n",
    "\n",
    "# We select the heuristic flatfield option in the standard configuration\n",
    "dict_config[\"source_config\"][\"LSTEventSource\"][\"use_flatfield_heuristic\"] = True\n",
    "\n",
    "with open(config_file, \"w\") as json_file:\n",
    "    json.dump(dict_config, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b32e12",
   "metadata": {},
   "source": [
    "# Finding the files that interest us\n",
    "#### Extracting dl1 files and dl1 datachecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e95eb908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding dl1  data to dictionary...\n",
      "...Finished adding dl1 data to dictionary\n",
      "\n",
      "Adding dl1 datacheck data to dictionary...\n",
      "...Finished adding dl1 data to dictionary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.4 s, sys: 6.53 s, total: 17.9 s\n",
      "Wall time: 29.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Getting coordinates of source\n",
    "source_coords = SkyCoord.from_name(source_name)\n",
    "\n",
    "dict_source = {\n",
    "    \"name\"   : source_name,\n",
    "    \"coords\" : source_coords,\n",
    "    \"ra\"     : source_coords.ra.deg  * u.deg, # ra in degrees\n",
    "    \"dec\"    : source_coords.dec.deg * u.deg, # dec in degrees\n",
    "}\n",
    "\n",
    "# We create a empty dictionary to store all the information needed inside\n",
    "dict_dchecks = {}\n",
    "for run in [run_number]:\n",
    "    dict_dchecks[run] = {\n",
    "        \"run_num\" : run,\n",
    "    }\n",
    "\n",
    "dict_dchecks = lstpipeline.add_dl1_paths_to_dict(dict_dchecks, root_dl1)\n",
    "dict_dchecks = lstpipeline.add_dl1_paths_to_dict(dict_dchecks, root_dl1, dchecking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac5fda0",
   "metadata": {},
   "source": [
    "#### Then we read the observations information and also the selected nodes for MC and RFs and we add it to the DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91f1e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_dcheck_run = read_table(dict_dchecks[run_number][\"dchecks\"][\"runwise\"], \"/dl1datacheck/cosmics\")\n",
    "\n",
    "# reading the variables\n",
    "dcheck_zd = 90 - np.rad2deg(np.array(tab_dcheck_run[\"mean_alt_tel\"]))\n",
    "dcheck_az = np.rad2deg(np.array(tab_dcheck_run[\"mean_az_tel\"]))\n",
    "dcheck_tstart   = tab_dcheck_run[\"dragon_time\"][0][0]\n",
    "dcheck_telapsed = np.array(tab_dcheck_run[\"elapsed_time\"])\n",
    "\n",
    "dict_dchecks[run_number][\"time\"] = {\n",
    "    \"tstart\"   : dcheck_tstart,            # datetime object\n",
    "    \"telapsed\" : np.sum(dcheck_telapsed),  # s\n",
    "    \"srunwise\" : {\n",
    "        \"telapsed\" : dcheck_telapsed,      # s      \n",
    "    },\n",
    "}\n",
    "dict_dchecks[run_number][\"pointing\"] = {\n",
    "    \"zd\" : np.mean(dcheck_zd),  # deg\n",
    "    \"az\" : np.mean(dcheck_az),  # deg\n",
    "    \"srunwise\" : {\n",
    "        \"zd\" : dcheck_zd, # deg\n",
    "        \"az\" : dcheck_az, # deg\n",
    "    },\n",
    "}\n",
    "    \n",
    "# then we also select the RFs and MC files looking at the nodes available\n",
    "dict_dchecks, dict_nodes = lstpipeline.add_mc_and_rfs_nodes(dict_dchecks, root_rfs, root_mcs, dict_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261c0e68-ee3e-40b1-a11f-0eeaed2ca94b",
   "metadata": {},
   "source": [
    "### Read datacheck\n",
    "#### - The binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89f3cc7f-95f6-405c-a8ad-19b8cbc8f26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The intensity in the middle of the intensity range is 421.4 p.e.\n"
     ]
    }
   ],
   "source": [
    "ref_intensity = (limits_intensity[0] * limits_intensity[1]) ** 0.5\n",
    "logger.info(f\"The intensity in the middle of the intensity range is {ref_intensity:.1f} p.e.\")\n",
    "\n",
    "########################################################\n",
    "# Reading the binning from the datacheck ---------------\n",
    "# Opening the corresponding datacheck\n",
    "fname_dcheck = dict_dchecks[run_number][\"dchecks\"][\"runwise\"]\n",
    "tab_dcheck_run = tables.open_file(fname_dcheck)\n",
    "\n",
    "# Read the binning from the datacheck of the first subrun\n",
    "dcheck_intensity_binning = np.array(tab_dcheck_run.root.dl1datacheck.histogram_binning.col(\"hist_intensity\")[0])\n",
    "# Calculating the logarithmic center of each bin\n",
    "dcheck_intensity_binning_centers = (dcheck_intensity_binning[:-1] * dcheck_intensity_binning[1:]) ** 0.5\n",
    "# Calculating the width of each bin\n",
    "dcheck_intensity_binning_widths = np.diff(dcheck_intensity_binning)\n",
    "tab_dcheck_run.close()\n",
    "\n",
    "# Mask for the fitting region in the fits\n",
    "mask_dcheck_bins_fit = (\n",
    "    (dcheck_intensity_binning_centers >= limits_intensity[0]) &\n",
    "    (dcheck_intensity_binning_centers <= limits_intensity[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eca996a-e892-4e46-9a04-7b18fb9411cf",
   "metadata": {},
   "source": [
    "#### - The intensity data from the datacheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7716a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# Reading the histogram from the datacheck ---------------\n",
    "# Opening the corresponding datacheck\n",
    "tab_dcheck_run = tables.open_file(fname_dcheck)\n",
    "dcheck_hist_intensities = np.array(tab_dcheck_run.root.dl1datacheck.cosmics.col(\"hist_intensity\"))\n",
    "tab_dcheck_run.close()\n",
    "\n",
    "# Converting from counts to rate per intensity unit (non-binning dependent quantity)\n",
    "dcheck_rates       = [] # Array of histogram of rates for each subrun\n",
    "dcheck_delta_rates = [] # The statistical error\n",
    "for srun, dcheck_hist_intensity in enumerate(dcheck_hist_intensities):\n",
    "\n",
    "    effective_time_srun = dict_dchecks[run_number][\"time\"][\"srunwise\"][\"telapsed\"][srun]\n",
    "    \n",
    "    dcheck_rates.append(              dcheck_hist_intensity  / effective_time_srun / dcheck_intensity_binning_widths)\n",
    "\n",
    "    dcheck_delta_rates.append(np.sqrt(dcheck_hist_intensity) / effective_time_srun / dcheck_intensity_binning_widths)\n",
    "    \n",
    "# Number of events in last subrun\n",
    "last_subrun_statistics = np.sum(dcheck_hist_intensities[-1])\n",
    "\n",
    "# Subruns array \n",
    "sruns_array = np.sort([int(f[-7:-3]) for f in dict_dchecks[run_number][\"dl1a\"][\"srunwise\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1da3d41-9c8c-43a7-807f-d0a9df93f8b4",
   "metadata": {},
   "source": [
    "### Correction factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfdc14ca-4311-439a-bfdb-e447ae9c4735",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_factor_p0 = geom.pol2(1, p0a, p0b, p0c) / geom.pol2(np.cos(np.deg2rad(dict_dchecks[run_number][\"pointing\"][\"zd\"])), p0a, p0b, p0c)\n",
    "corr_factor_p1 = geom.pol2(1, p1a, p1b, p1c) - geom.pol2(np.cos(np.deg2rad(dict_dchecks[run_number][\"pointing\"][\"zd\"])), p1a, p1b, p1c)\n",
    "\n",
    "# Zenith correction of the reference (putting the reference in the zenith of the determined subrun)\n",
    "corr_ref_p0 = ref_p0 / corr_factor_p0\n",
    "corr_ref_p1 = ref_p1 - corr_factor_p1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ff66a0-303c-4ee9-8461-33cb3b779791",
   "metadata": {},
   "source": [
    "## Function to perform all the scaling and then the reading\n",
    "#### - Zenith corrections and light yield\n",
    "\n",
    "Taking into account the logarithmic scale that is used, the light yield can be calculated with the following expression:\\\n",
    "\\\n",
    "$\\qquad\\qquad LY(A_{fit}, \\alpha; A_{ref}) = \\left(\\frac{A_{fit}}{A_{ref}}\\right)^{-\\frac{1}{1+\\alpha}}$\\\n",
    "\\\n",
    "The uncertainties have been calculated propagating the known ones.\\\n",
    "$\\sigma_{LY} = \\sqrt{\\sum_x\\left(\\frac{\\partial LY}{\\partial x}\\right)^2 \\sigma_x^2}$\\\n",
    "Where:\\\n",
    "$\\left(\\frac{\\partial LY}{\\partial A}\\right) =\\left(\\frac{A_{fit}}{A_{ref}}\\right)^{-\\frac{1}{1+\\alpha}} \\dfrac{-1}{\\left({\\alpha}+1\\right)A_{fit}}$\\\n",
    "$\\left(\\frac{\\partial LY}{\\partial \\alpha}\\right) = \\ln \\left(\\frac{A_{fit}}{A_{ref}}\\right) \\left(\\frac{A_{fit}}{A_{ref}}\\right)^{-\\frac{1}{1+\\alpha}} \\frac{1}{(1+\\alpha)^2}$\\\n",
    "\\\n",
    "So the final error will be:\n",
    "\\\n",
    "$\\sigma_{LY} = \\left(\\frac{A_{fit}}{A_{ref}}\\right)^{-\\frac{1}{1+\\alpha}} \\frac{1}{1+\\alpha}\\sqrt{A_{fit}^{-2}\\sigma_{A}^2+\\ln\\left(\\frac{A_{fit}}{A_{ref}}\\right)^2\\frac{\\sigma_{\\alpha}^2}{(1+\\alpha)^2}}$\n",
    "\n",
    "#### - Defining scaling\n",
    "In order to sample we will set two scalings for the conversion.\n",
    "\n",
    "* $1/LY$ The ideal scaling needed. The case where no new pixels emerge on the cleaning (unrealistic).\n",
    "\n",
    "And the error is calculated now as:\n",
    "\\\n",
    "$\\sigma_{s} = \\frac{1}{LY^4}\\sigma_{LY}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6c93261-d9f7-427c-8c6c-bd53930d0ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_scaling(iteration_step, dict_results):\n",
    "    \"\"\"\n",
    "    A function to perform scaling and evaluating the results. Returning everything in a updated dictionary\n",
    "\n",
    "    Input:\n",
    "    - iteration_step: (str) \n",
    "        The iteration step you are in, that can be \"original\" for the original data, \"upper\" for the upper\n",
    "        limit on the scale factor, \"linear\" for the linear intepolation factor and \"final\" for the final scaling \n",
    "        and results.\n",
    "    - dict_results: (dict)\n",
    "        Dictionary with the results of the before step.\n",
    "    \"\"\"\n",
    "\n",
    "    # Creating a arrray of subruns looking at the datachecks and also extracting the run number\n",
    "    run_number  = dict_results[\"run\"]\n",
    "    sruns_array = np.sort([int(f[-7:-3]) for f in dict_dchecks[run_number][\"dl1a\"][\"srunwise\"]])\n",
    "    \n",
    "    # Empty arrays to store the fit information\n",
    "    data_p0, data_delta_p0  = [], []\n",
    "    data_p1, data_delta_p1  = [], []\n",
    "    data_chi2, data_pvalue = [], []\n",
    "    \n",
    "    # Processing subrun by subrun\n",
    "    for srun in sruns_array:    \n",
    "\n",
    "        input_fname = dict_dchecks[run_number][\"dl1a\"][\"srunwise\"][srun]   # Input dl1a        \n",
    "        data_scale_factor = dict_results[\"scaled\"][iteration_step][srun]   # Reading the scaling factor\n",
    "\n",
    "        # Here we do different things depending on the iteration step\n",
    "        # If is the first one i.e. == \"original\"\n",
    "        # We do not run lstchain_dl1ab because the data is already scaled\n",
    "        if iteration_step == \"original\":\n",
    "            data_output_fname = input_fname\n",
    "\n",
    "        # If is the second or third: \"upper\" or \"linear\"\n",
    "        # We perform lstchain_dl1ab but over a subset of the data only to keep it shorter\n",
    "        elif iteration_step in [\"upper\", \"linear\"]:\n",
    "\n",
    "            # Temporal dl1 file that will be overwritten in the next iteration / subrun\n",
    "            data_output_fname = root_objects + f\"tmp_dl1_srunwise_{iteration_step}_scaled.h5\" \n",
    "\n",
    "            logger.info(f\"\\nProcessing subrun {srun}/{len(sruns_array)}\")\n",
    "            logger.info(f\"Running lstchain_dl1ab... scale: {data_scale_factor:.2f}\")\n",
    "\n",
    "            # If scale is greater than 1 we select a range lower than the upper one\n",
    "            # otherwise we select a range higher than the upper one\n",
    "            if data_scale_factor > 1:\n",
    "                dl1_selected_range = f\"{limits_intensity_extended:.2f},{limits_intensity[1]:.2f}\"\n",
    "            else:\n",
    "                dl1_selected_range = f\"{limits_intensity[0]:.2f},inf\"\n",
    "                \n",
    "            # If the file already exists we delete it\n",
    "            if os.path.exists(data_output_fname):\n",
    "                os.remove(data_output_fname)\n",
    "        \n",
    "            !lstchain_dl1ab \\\n",
    "            --input-file $input_fname \\\n",
    "            --output-file $data_output_fname \\\n",
    "            --config $config_file \\\n",
    "            --no-image \\\n",
    "            --light-scaling $data_scale_factor \\\n",
    "            --intensity-range $dl1_selected_range\n",
    "\n",
    "        # If is the last step i.e. \"final\"\n",
    "        # The lstchain_dl1ab script is run over all thedataset to generate the final file\n",
    "        elif iteration_step == \"final\":\n",
    "\n",
    "            data_output_fname = root_data + f\"dl1_scaled/{run_number:05}/\" + os.path.basename(dict_dchecks[run_number][\"dl1a\"][\"srunwise\"][srun])\n",
    "            logger.info(f\"\\nProcessing subrun {srun}/{len(sruns_array)}\")\n",
    "            logger.info(f\"Running lstchain_dl1ab... scale: {data_scale_factor:.2f}\")\n",
    "                \n",
    "            # If the file already exists we delete it\n",
    "            if os.path.exists(data_output_fname):\n",
    "                os.remove(data_output_fname)\n",
    "        \n",
    "            !lstchain_dl1ab \\\n",
    "            --input-file $input_fname \\\n",
    "            --output-file $data_output_fname \\\n",
    "            --config $config_file \\\n",
    "            --no-image \\\n",
    "            --light-scaling $data_scale_factor\n",
    "\n",
    "            # We store this info also in the dictionary in the final case\n",
    "            dict_results[\"filenames\"][srun] = data_output_fname\n",
    "    \n",
    "        # Reading the file\n",
    "        table_data = tables.open_file(data_output_fname)\n",
    "        data_counts_intensity, _ = np.histogram(\n",
    "            table_data.root.dl1.event.telescope.parameters.LST_LSTCam.col(\"intensity\"), \n",
    "            bins=dcheck_intensity_binning\n",
    "        )\n",
    "        table_data.close()\n",
    "        \n",
    "        # Calculating the non binning dependent transformation\n",
    "        effective_time_srun = dict_dchecks[run_number][\"time\"][\"srunwise\"][\"telapsed\"][srun]\n",
    "        data_rates       = np.array(data_counts_intensity) / effective_time_srun / dcheck_intensity_binning_widths\n",
    "        data_delta_rates = np.sqrt(data_counts_intensity)  / effective_time_srun / dcheck_intensity_binning_widths\n",
    "    \n",
    "        # Displacing the X-coordinates to the center of the fit, in order to decorrelate the fit\n",
    "        x_fit    = dcheck_intensity_binning_centers[mask_dcheck_bins_fit] / ref_intensity\n",
    "        y_fit    = data_rates[mask_dcheck_bins_fit]\n",
    "        yerr_fit = data_delta_rates[mask_dcheck_bins_fit]\n",
    "    \n",
    "        # Performing the fit\n",
    "        params, pcov, info, _, _ = curve_fit(\n",
    "            f     = geom.powerlaw,\n",
    "            xdata = x_fit,\n",
    "            ydata = y_fit,\n",
    "            sigma = yerr_fit,\n",
    "            p0    = [ref_p0, ref_p1],\n",
    "            full_output = True,\n",
    "        )\n",
    "    \n",
    "        srun_p0, srun_p1  = params\n",
    "        srun_delta_p0 = np.sqrt(pcov[0, 0])\n",
    "        srun_delta_p1 = np.sqrt(pcov[1, 1])\n",
    "        srun_chi2     = np.sum(info[\"fvec\"] ** 2)\n",
    "        srun_pvalue   = 1 - chi2.cdf(srun_chi2, sum(mask_dcheck_bins_fit))\n",
    "        \n",
    "        dict_results[\"chi2\"][iteration_step][srun]   = srun_chi2\n",
    "        dict_results[\"pvalue\"][iteration_step][srun] = srun_pvalue\n",
    "        dict_results[\"scaled\"][iteration_step][srun] = data_scale_factor\n",
    "    \n",
    "        data_p0.append(srun_p0)\n",
    "        data_p1.append(srun_p1)\n",
    "        data_delta_p0.append(srun_delta_p0)\n",
    "        data_delta_p1.append(srun_delta_p1)\n",
    "        data_chi2.append(srun_chi2)\n",
    "        data_pvalue.append(srun_pvalue)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    data_p0       = np.array(data_p0)\n",
    "    data_p1       = np.array(data_p1)\n",
    "    data_delta_p0 = np.array(data_delta_p0)\n",
    "    data_delta_p1 = np.array(data_delta_p1)\n",
    "    data_chi2     = np.array(data_chi2)\n",
    "    data_pvalue   = np.array(data_pvalue)\n",
    "    \n",
    "    # Zenith corrections to the parameters\n",
    "    data_corr_p0 = data_p0 * corr_factor_p0\n",
    "    data_corr_p1 = data_p1 + corr_factor_p1\n",
    "    \n",
    "    data_corr_delta_p0 = data_delta_p0 * corr_factor_p0\n",
    "    data_corr_delta_p1 = data_delta_p1\n",
    "    \n",
    "    # Calculating the needed light yield  \n",
    "    data_light_yield, data_delta_light_yield = geom.calc_light_yield(\n",
    "        p0_fit = data_corr_p0,\n",
    "        p1_fit = data_corr_p1, \n",
    "        sigma_p0_fit = data_corr_delta_p0, \n",
    "        sigma_p1_fit = data_corr_delta_p1, \n",
    "        p0_ref = ref_p0,\n",
    "    )\n",
    "    \n",
    "    # Scalings to apply\n",
    "    data_scaling       = 1 / data_light_yield\n",
    "    data_delta_scaling = 1 / data_light_yield ** 4 * data_delta_light_yield\n",
    "    \n",
    "    # In the case of the last subrun we use the last subrun statistics, because the lack of statistics\n",
    "    if last_subrun_statistics < statistics_threshold_last_srun:\n",
    "        data_scaling[-1]       = data_scaling[-2]\n",
    "        data_delta_scaling[-1] = data_delta_scaling[-2]\n",
    "    \n",
    "        logger.warning(f\"For last subrun using before subrun because of low statistics ({last_subrun_statistics}evs)\\n\")\n",
    "    \n",
    "    # The scaling in percentage\n",
    "    data_scaling_percent       = (data_scaling - 1) * 100\n",
    "    data_delta_scaling_percent = data_delta_scaling * 100\n",
    "    \n",
    "    # Adding to dictionary\n",
    "    for i, srun in enumerate(sruns_array):\n",
    "        dict_results[\"p0\"][iteration_step][srun]       = data_corr_p0[i]\n",
    "        dict_results[\"delta_p0\"][iteration_step][srun] = data_corr_delta_p0[i]\n",
    "        dict_results[\"p1\"][iteration_step][srun]       = data_corr_p1[i]\n",
    "        dict_results[\"delta_p1\"][iteration_step][srun] = data_corr_delta_p1[i]\n",
    "        \n",
    "        dict_results[\"light_yield\"][iteration_step][srun]       = data_light_yield[i]\n",
    "        dict_results[\"delta_light_yield\"][iteration_step][srun] = data_delta_light_yield[i]\n",
    "        dict_results[\"scaling\"][iteration_step][srun]           = data_scaling[i]\n",
    "        dict_results[\"delta_scaling\"][iteration_step][srun]     = data_delta_scaling[i]\n",
    "        dict_results[\"scaling_percent\"][iteration_step][srun]   = data_scaling_percent[i]\n",
    "        dict_results[\"delta_scaling_percent\"][iteration_step][srun] = data_delta_scaling_percent[i]\n",
    "\n",
    "    return dict_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00956a5-5068-4af3-86be-765e4dcd4bb8",
   "metadata": {},
   "source": [
    "### Applying the function over the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e595f-0944-44e9-b0ab-708f7153723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The main results dictionary\n",
    "dict_results = dict_results_empty.copy()\n",
    "# First filling the dictionary with ones in the scaled values\n",
    "for srun in sruns_array:\n",
    "    dict_results[\"scaled\"][\"original\"][srun] = 1.0\n",
    "\n",
    "# Then we read these files and perform the fits\n",
    "dict_results = find_scaling(iteration_step=\"original\", dict_results=dict_results)\n",
    "\n",
    "# Then filling the next step \"scaled\" with the calculated one\n",
    "for srun in sruns_array:\n",
    "    dict_results[\"scaled\"][\"upper\"][srun] = dict_results[\"scaling\"][\"original\"][srun]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319183a7",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><<-----------------------------------------------------------------------------------------------\\>></span>\n",
    "#### <span style=\"color:orange\"><<Some plots and checks that will not be included in the final script\\>></span>\n",
    "#### <span style=\"color:orange\"><<-----------------------------------------------------------------------------------------------\\>></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37892355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning for the plots ------------------\n",
    "bins_space = np.linspace(limits_intensity[0] - 100, limits_intensity[1] + 200, 100)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4.5))\n",
    "\n",
    "colors = plotting.get_colors_multiplot(sruns_array)\n",
    "for i in range(len(dcheck_rates)):\n",
    "    ax.plot(dcheck_intensity_binning_centers, dcheck_rates[i], color=colors[i], zorder=np.random.rand())\n",
    "\n",
    "cmap = plotting.create_cmap_from_colors(plotting.default_colors)\n",
    "plotting.plot_colorbar(fig, ax, range(len(dcheck_rates)), cmap=cmap, label=\"Subrun number\")\n",
    "\n",
    "intensity_sample = np.linspace(limits_intensity[0] / 1.3, limits_intensity[1] * 1.5, 100)\n",
    "ax.plot(bins_space, geom.powerlaw(bins_space / ref_p0, corr_ref_p0, corr_ref_p1), color=\"k\", ls=\"--\", label=\"Reference\")\n",
    "ax.axvspan(limits_intensity[0], limits_intensity[1], alpha=0.3, ls=\"-\", facecolor=\"none\", hatch=\"///\", edgecolor=\"k\", label=\"Fit region\")\n",
    "\n",
    "ax.set_xlabel(\"Intensity [p.e.]\")\n",
    "ax.set_ylabel(\"Rate [events / s / p.e.]\")\n",
    "ax.loglog()\n",
    "ax.set_xlim(1.5e1, 1e4)\n",
    "ax.set_ylim(1e-3, 1e2)\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "# plt.savefig(f\"plots/total.png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e7178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_corr_p0 = np.array([dict_results[\"p0\"][\"original\"][srun] for srun in sruns_array])\n",
    "original_corr_p1 = np.array([dict_results[\"p1\"][\"original\"][srun] for srun in sruns_array])\n",
    "original_corr_delta_p0 = np.array([dict_results[\"delta_p0\"][\"original\"][srun] for srun in sruns_array])\n",
    "original_corr_delta_p1 = np.array([dict_results[\"delta_p1\"][\"original\"][srun] for srun in sruns_array])\n",
    "original_scaling_percent = np.array([dict_results[\"scaling_percent\"][\"original\"][srun] for srun in sruns_array])\n",
    "original_delta_scaling_percent = np.array([dict_results[\"delta_scaling_percent\"][\"original\"][srun] for srun in sruns_array])\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 5), sharex=True)\n",
    "\n",
    "ax1.plot(sruns_array, original_corr_p0, color=\"mediumblue\")\n",
    "ax1.fill_between(\n",
    "    sruns_array, \n",
    "    original_corr_p0 - original_corr_delta_p0, \n",
    "    original_corr_p0 + original_corr_delta_p0,\n",
    "    color=\"mediumblue\", alpha=0.2, ls=\"\"\n",
    ")\n",
    "\n",
    "ax2.plot(sruns_array, original_corr_p1, color=\"crimson\")\n",
    "ax2.fill_between(\n",
    "    sruns_array, \n",
    "    original_corr_p1 - original_corr_delta_p1, \n",
    "    original_corr_p1 + original_corr_delta_p1,\n",
    "    color=\"crimson\", alpha=0.2, ls=\"\"\n",
    ")\n",
    "\n",
    "ax3.plot(sruns_array, original_scaling_percent, color=\"g\")\n",
    "ax3.fill_between(\n",
    "    sruns_array, \n",
    "    original_scaling_percent - original_delta_scaling_percent, \n",
    "    original_scaling_percent + original_delta_scaling_percent,\n",
    "    color=\"g\", alpha=0.2, ls=\"\",\n",
    ")\n",
    "\n",
    "ax1.axhline(ref_p0, color=\"k\", ls=\"--\", label=\"Reference\")\n",
    "ax2.axhline(ref_p1, color=\"k\", ls=\"--\")\n",
    "ax3.axhline(1, color=\"k\", ls=\"--\")\n",
    "\n",
    "ax1.set_ylabel(\"Normalization\")\n",
    "ax2.set_ylabel(\"Power index\")\n",
    "ax3.set_ylabel(\"Scaling factor [%]\")\n",
    "ax3.set_xlabel(\"# Subrun\")\n",
    "\n",
    "ax1.set_ylim(0.9, 2)\n",
    "ax2.set_ylim(-2.8, -1.5)\n",
    "ax3.set_ylim(-10, 100)\n",
    "ax1.legend(frameon=False)\n",
    "\n",
    "# plt.savefig(f\"plots/total.png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "del original_corr_p0, original_corr_p1, original_corr_delta_p0, original_corr_delta_p1\n",
    "del original_scaling_percent, original_delta_scaling_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13abbb8c-c047-46ad-809a-6cb4bdb9d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_chi2 = np.array([dict_results[\"chi2\"][\"original\"][srun] for srun in sruns_array])\n",
    "original_pvalue = np.array([dict_results[\"pvalue\"][\"original\"][srun] for srun in sruns_array])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 2), sharex=True)\n",
    "axt = ax.twinx()\n",
    "\n",
    "p1, = ax.plot(sruns_array, original_chi2, color=\"gray\")\n",
    "p2, = axt.plot(sruns_array, original_pvalue, ls=\"-\", color=\"cornflowerblue\")\n",
    "\n",
    "ax.set_ylabel(\"$\\chi^2$\")\n",
    "axt.set_ylabel(\"p-value\")\n",
    "ax.set_xlabel(\"# Subrun\")\n",
    "axt.set_yscale(\"log\")\n",
    "\n",
    "ax.yaxis.label.set_color(p1.get_color())\n",
    "axt.yaxis.label.set_color(p2.get_color())\n",
    "ax.tick_params(axis='y', colors=p1.get_color())\n",
    "axt.tick_params(axis='y', colors=p2.get_color())\n",
    "\n",
    "# plt.savefig(f\"plots/total.png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "del original_chi2, original_pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36993d84",
   "metadata": {},
   "source": [
    "### Performing the first scaling for first run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a356f9a4-8203-4cb9-a50a-1d14e6b5630d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dict_results = find_scaling(iteration_step=\"upper\", dict_results=dict_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94acdce6-579d-49c4-850a-6c3350199fdf",
   "metadata": {},
   "source": [
    "### Performing linear interpolation to calculate the new scaling factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cfdcc8-1785-4637-9b25-f03e8c048dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_scale_factors = []\n",
    "for srun in sruns_array:\n",
    "\n",
    "    # In the case of the last subrun we use the last subrun statistics, because the lack of statistics\n",
    "    if srun == sruns_array[-1] and last_subrun_statistics < statistics_threshold_last_srun:\n",
    "        linear_scale_factor = linear_scale_factor\n",
    "\n",
    "        logger.warning(f\"For last subrun using before subrun because of low statistics ({last_subrun_statistics}evs)\")\n",
    "    else:\n",
    "        # Now putting all together, upper and half\n",
    "        points_scaling     = np.array([1, dict_results[\"scaling\"][\"original\"][srun]])\n",
    "        points_light_yield = np.array([dict_results[\"light_yield\"][\"original\"][srun], dict_results[\"light_yield\"][\"upper\"][srun]])\n",
    "    \n",
    "        # Finding the final scaling as a line that pass trogh the two points we have\n",
    "        # Then we calculate where the light yield will be 1 in linear approximation\n",
    "        slope = (points_light_yield[1] - points_light_yield[0]) / (points_scaling[1] - points_scaling[0])\n",
    "        intercept = points_light_yield[0] - slope * points_scaling[0]\n",
    "        linear_scale_factor = 1 / slope - points_light_yield[0] / slope + points_scaling[0]\n",
    "\n",
    "    dict_results[\"scaled\"][\"linear\"][srun] = linear_scale_factor\n",
    "    linear_scale_factors.append(linear_scale_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e722a6f6-a225-4aeb-b4da-b9bc74991b58",
   "metadata": {},
   "source": [
    "### Applying the linear factor to the data and then re-calculating the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7d80bf-ee73-4212-83aa-fc644462f171",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dict_results = find_scaling(iteration_step=\"linear\", dict_results=dict_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe1b52-20ad-4f90-a4f8-b9ac57bac4b3",
   "metadata": {},
   "source": [
    "### Then calculate the final light yield and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f4aa2c-6e14-4fd8-bd75-5417457b1c40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_scale_factors = []\n",
    "for srun in sruns_array:\n",
    "\n",
    "    # In the case of the last subrun we use the last subrun statistics, because the lack of statistics\n",
    "    if srun == sruns_array[-1] and last_subrun_statistics < statistics_threshold_last_srun:\n",
    "        final_scale_factor = final_scale_factor\n",
    "\n",
    "        logger.warning(f\"For last subrun using before subrun because of low statistics ({last_subrun_statistics}evs)\")\n",
    "    else:\n",
    "        # Now putting all together, upper and half\n",
    "        points_scaling           = np.array([dict_results[\"scaled\"][key][srun]            for key in [\"original\", \"linear\", \"upper\"]])\n",
    "        points_light_yield       = np.array([dict_results[\"light_yield\"][key][srun]       for key in [\"original\", \"linear\", \"upper\"]])\n",
    "        points_delta_light_yield = np.array([dict_results[\"delta_light_yield\"][key][srun] for key in [\"original\", \"linear\", \"upper\"]]) \n",
    "        \n",
    "        # And fitting to a 2nd degree polynomial\n",
    "        pol2_scaling = np.poly1d(np.polyfit(points_scaling, points_light_yield, 2))\n",
    "    \n",
    "        # And finding the final scaling\n",
    "        final_scale_factor = optimize.root(pol2_scaling - 1, x0=linear_scale_factor).x[0]\n",
    "\n",
    "        ############################################\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(5, 3))\n",
    "        ax.plot(points_scaling, points_light_yield, \"o\", color=\"k\", label=\"Points\", zorder=10)\n",
    "        size_points = max(points_scaling) - min(points_scaling)\n",
    "\n",
    "        ax.plot([points_scaling[0], points_scaling[2]], [points_light_yield[0], points_light_yield[2]], \n",
    "            marker=\"\", ls=\"--\", lw=0.5, color=\"k\", label=\"\", zorder=10)\n",
    "\n",
    "        sample_scaling = np.linspace(min(points_scaling) - 0.1 * size_points, max(points_scaling) + 0.1 * size_points, 100)\n",
    "        ax.plot(sample_scaling, pol2_scaling(sample_scaling), color=\"crimson\", ls=\"-\", label=\"Polynomial fit\", zorder=5)\n",
    "        ax.axvline(final_scale_factor, color=\"k\", ls=\"--\", label=f\"Final scaling = {final_scale_factor:.2f}\")\n",
    "        ax.axhline(1, color=\"k\", ls=\":\", label=\"Reference\")\n",
    "        ax.set_xlabel(\"Scaling\")\n",
    "        ax.set_ylabel(\"Light yield\")\n",
    "        ax.legend(frameon=False)\n",
    "        ax.set_title(f\"Subrun {srun}\")\n",
    "        # plt.savefig(f\"plots/total.png\", bbox_inches=\"tight\", dpi=300)\n",
    "        plt.show()\n",
    "        ############################################\n",
    "\n",
    "\n",
    "    dict_results[\"final_scaling\"][srun] = final_scale_factor\n",
    "    final_scale_factors.append(linear_scale_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1d0e4d-a713-4a6d-b704-7c623de3c559",
   "metadata": {},
   "source": [
    "### Then we interpolate the lineal scalings with a linear behaviour trough the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce91664-7922-4ba1-9efc-459fe2fc23bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fit = np.cumsum(dict_dchecks[run_number][\"time\"][\"srunwise\"][\"telapsed\"])\n",
    "y_fit = np.array([dict_results[\"final_scaling\"][srun] for srun in dict_results[\"final_scaling\"].keys()])\n",
    "\n",
    "# Performing the fit\n",
    "params, pcov, info, _, _ = curve_fit(\n",
    "    f     = geom.straight_line,\n",
    "    xdata = x_fit,\n",
    "    ydata = y_fit,\n",
    "    p0    = [1, 0],\n",
    "    full_output = True,\n",
    ")\n",
    "    \n",
    "intercept       = params[0]\n",
    "slope           = params[1]\n",
    "delta_intercept = np.sqrt(pcov[0, 0])\n",
    "delta_slope     = np.sqrt(pcov[1, 1])\n",
    "_chi2           = np.sum(info['fvec'] ** 2)\n",
    "pvalue          = 1 - chi2.cdf(_chi2, len(x_fit))\n",
    "\n",
    "dict_results[\"interpolation\"] = {\n",
    "    \"chi2\" : _chi2,      \n",
    "    \"p_value\" : pvalue,         \n",
    "    \"slope\": slope,      \n",
    "    \"delta_slope\" : delta_slope,     \n",
    "    \"intercept\" : intercept, \n",
    "    \"delta_intercept\" : delta_intercept,\n",
    "}\n",
    "\n",
    "#############################################\n",
    "#############################################\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 2))\n",
    "\n",
    "ax.plot(x_fit/60, ((intercept + x_fit * slope) - 1) * 100, color=\"k\", ls=\"--\", zorder=10, label=\"Interpolation\")\n",
    "\n",
    "ax.plot(x_fit/60, (y_fit - 1) * 100, 'r', label='Data')\n",
    "\n",
    "ax.set_xlabel(\"Time elapsed [min]\")\n",
    "ax.set_ylabel(\"Scaling factor [%]\")\n",
    "ax.legend(ncols=2, frameon=False)\n",
    "\n",
    "plt.show()\n",
    "#############################################\n",
    "#############################################\n",
    "\n",
    "# Setting a interpolated scaling factor\n",
    "for srun in dict_results[\"final_scaling\"].keys():\n",
    "    \n",
    "    scaling_interpolated = intercept + slope * x_fit[srun]\n",
    "    \n",
    "    dict_results[\"final_scaling_interpolated\"][srun] = scaling_interpolated\n",
    "    dict_results[\"scaled\"][\"final\"][srun]            = scaling_interpolated\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba6c1f7-b50a-4ec8-bb56-9424395dadd4",
   "metadata": {},
   "source": [
    "### Then scale the full datasets by the interpolated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2872fa71-5cfe-4dcf-a59f-a56d57326cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing subrun 0/142\n",
      "Running lstchain_dl1ab... scale: 1.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pedestal cleaning\n",
      "Fraction of Cat_A pixel cleaning thresholds above Cat_A picture thr.:0.242\n",
      "Tailcut clean with pedestal threshold config used:{'picture_thresh': 8, 'boundary_thresh': 4, 'sigma': 2.5, 'keep_isolated_pixels': False, 'min_number_picture_neighbors': 2, 'use_only_main_island': False, 'delta_time': 2}\n",
      "Using dynamic cleaning for events with average size of the 3 most brighest pixels > 267 p.e\n",
      "Remove from image pixels which have charge below = 0.03 * average size\n",
      "Traceback (most recent call last):\n",
      "  File \"/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/bin/lstchain_dl1ab\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/fefs/aswg/workspace/juan.jimenez/softs/cta-lstchain/lstchain/scripts/lstchain_dl1ab.py\", line 342, in main\n",
      "    with tables.open_file(args.output_file, mode='a', filters=HDF5_ZSTD_FILTERS) as outfile:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/tables/file.py\", line 294, in open_file\n",
      "    return File(filename, mode, title, root_uep, filters, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/tables/file.py\", line 744, in __init__\n",
      "    self._g_new(filename, mode, **params)\n",
      "  File \"tables/hdf5extension.pyx\", line 397, in tables.hdf5extension.File._g_new\n",
      "  File \"/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/tables/utils.py\", line 172, in check_file_access\n",
      "    check_file_access(path, 'w')\n",
      "  File \"/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/tables/utils.py\", line 160, in check_file_access\n",
      "    raise FileNotFoundError(f\"``{path.parent}`` does not exist\")\n",
      "FileNotFoundError: ``/fefs/aswg/workspace/juan.jimenez/data/cherenkov_transparency_corrections/crab/dl1_scaled/06172`` does not exist\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "``/fefs/aswg/workspace/juan.jimenez/data/cherenkov_transparency_corrections/crab/dl1_scaled/06172/dl1_LST-1.Run06172.0000.h5`` does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "Cell \u001b[0;32mIn[10], line 75\u001b[0m, in \u001b[0;36mfind_scaling\u001b[0;34m(iteration_step, dict_results)\u001b[0m\n\u001b[1;32m     72\u001b[0m     dict_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilenames\u001b[39m\u001b[38;5;124m\"\u001b[39m][srun] \u001b[38;5;241m=\u001b[39m data_output_fname\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Reading the file\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m table_data \u001b[38;5;241m=\u001b[39m \u001b[43mtables\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_output_fname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m data_counts_intensity, _ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhistogram(\n\u001b[1;32m     77\u001b[0m     table_data\u001b[38;5;241m.\u001b[39mroot\u001b[38;5;241m.\u001b[39mdl1\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mtelescope\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mLST_LSTCam\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintensity\u001b[39m\u001b[38;5;124m\"\u001b[39m), \n\u001b[1;32m     78\u001b[0m     bins\u001b[38;5;241m=\u001b[39mdcheck_intensity_binning\n\u001b[1;32m     79\u001b[0m )\n\u001b[1;32m     80\u001b[0m table_data\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/tables/file.py:294\u001b[0m, in \u001b[0;36mopen_file\u001b[0;34m(filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    290\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is already opened.  Please \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose it before reopening in write mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m filename)\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# Finally, create the File instance, and return it\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_uep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/tables/file.py:744\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams \u001b[38;5;241m=\u001b[39m params\n\u001b[1;32m    743\u001b[0m \u001b[38;5;66;03m# Now, it is time to initialize the File extension\u001b[39;00m\n\u001b[0;32m--> 744\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_g_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;66;03m# Check filters and set PyTables format version for new files.\u001b[39;00m\n\u001b[1;32m    747\u001b[0m new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_v_new\n",
      "File \u001b[0;32m/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/tables/hdf5extension.pyx:397\u001b[0m, in \u001b[0;36mtables.hdf5extension.File._g_new\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/tables/utils.py:146\u001b[0m, in \u001b[0;36mcheck_file_access\u001b[0;34m(filename, mode)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# The file should be readable.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39maccess(path, os\u001b[38;5;241m.\u001b[39mF_OK):\n\u001b[0;32m--> 146\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m``\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`` does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIsADirectoryError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m``\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`` is not a regular file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: ``/fefs/aswg/workspace/juan.jimenez/data/cherenkov_transparency_corrections/crab/dl1_scaled/06172/dl1_LST-1.Run06172.0000.h5`` does not exist"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_results = find_scaling(iteration_step=\"final\", dict_results=dict_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5a9ff7-f47f-4446-b679-6a7cd9084517",
   "metadata": {},
   "source": [
    "## Storing the results of the fits in a file for each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a8ceecf-1d37-40c2-97b7-74a9b8f536b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_fname = root_results + f\"results_run_{run_number}.pkl\"\n",
    "\n",
    "# Saving the objects\n",
    "with open(dict_fname, 'wb') as f:\n",
    "    pickle.dump(dict_results, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# # Reading the object\n",
    "# with open(dict_fname, 'rb') as f:\n",
    "#     dict_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1458ffb",
   "metadata": {},
   "source": [
    "## Some plots for checking the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de79b6e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m original_scaling_percent \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([dict_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaling_percent\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m\"\u001b[39m][srun] \u001b[38;5;28;01mfor\u001b[39;00m srun \u001b[38;5;129;01min\u001b[39;00m sruns_array])\n\u001b[1;32m      6\u001b[0m original_delta_scaling_percent \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([dict_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta_scaling_percent\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m\"\u001b[39m][srun] \u001b[38;5;28;01mfor\u001b[39;00m srun \u001b[38;5;129;01min\u001b[39;00m sruns_array])\n\u001b[0;32m----> 7\u001b[0m final_corr_p0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43m[\u001b[49m\u001b[43mdict_results\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mp0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfinal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msrun\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msrun\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msruns_array\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      8\u001b[0m final_corr_p1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([dict_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp1\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal\u001b[39m\u001b[38;5;124m\"\u001b[39m][srun] \u001b[38;5;28;01mfor\u001b[39;00m srun \u001b[38;5;129;01min\u001b[39;00m sruns_array])\n\u001b[1;32m      9\u001b[0m final_corr_delta_p0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([dict_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta_p0\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal\u001b[39m\u001b[38;5;124m\"\u001b[39m][srun] \u001b[38;5;28;01mfor\u001b[39;00m srun \u001b[38;5;129;01min\u001b[39;00m sruns_array])\n",
      "Cell \u001b[0;32mIn[22], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m original_scaling_percent \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([dict_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaling_percent\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m\"\u001b[39m][srun] \u001b[38;5;28;01mfor\u001b[39;00m srun \u001b[38;5;129;01min\u001b[39;00m sruns_array])\n\u001b[1;32m      6\u001b[0m original_delta_scaling_percent \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([dict_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta_scaling_percent\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m\"\u001b[39m][srun] \u001b[38;5;28;01mfor\u001b[39;00m srun \u001b[38;5;129;01min\u001b[39;00m sruns_array])\n\u001b[0;32m----> 7\u001b[0m final_corr_p0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mdict_results\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mp0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfinal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msrun\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m srun \u001b[38;5;129;01min\u001b[39;00m sruns_array])\n\u001b[1;32m      8\u001b[0m final_corr_p1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([dict_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp1\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal\u001b[39m\u001b[38;5;124m\"\u001b[39m][srun] \u001b[38;5;28;01mfor\u001b[39;00m srun \u001b[38;5;129;01min\u001b[39;00m sruns_array])\n\u001b[1;32m      9\u001b[0m final_corr_delta_p0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([dict_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta_p0\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal\u001b[39m\u001b[38;5;124m\"\u001b[39m][srun] \u001b[38;5;28;01mfor\u001b[39;00m srun \u001b[38;5;129;01min\u001b[39;00m sruns_array])\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "original_corr_p0 = np.array([dict_results[\"p0\"][\"original\"][srun] for srun in sruns_array])\n",
    "original_corr_p1 = np.array([dict_results[\"p1\"][\"original\"][srun] for srun in sruns_array])\n",
    "original_corr_delta_p0 = np.array([dict_results[\"delta_p0\"][\"original\"][srun] for srun in sruns_array])\n",
    "original_corr_delta_p1 = np.array([dict_results[\"delta_p1\"][\"original\"][srun] for srun in sruns_array])\n",
    "original_scaling_percent = np.array([dict_results[\"scaling_percent\"][\"original\"][srun] for srun in sruns_array])\n",
    "original_delta_scaling_percent = np.array([dict_results[\"delta_scaling_percent\"][\"original\"][srun] for srun in sruns_array])\n",
    "final_corr_p0 = np.array([dict_results[\"p0\"][\"final\"][srun] for srun in sruns_array])\n",
    "final_corr_p1 = np.array([dict_results[\"p1\"][\"final\"][srun] for srun in sruns_array])\n",
    "final_corr_delta_p0 = np.array([dict_results[\"delta_p0\"][\"final\"][srun] for srun in sruns_array])\n",
    "final_corr_delta_p1 = np.array([dict_results[\"delta_p1\"][\"final\"][srun] for srun in sruns_array])\n",
    "final_scaling_percent = np.array([dict_results[\"scaling_percent\"][\"final\"][srun] for srun in sruns_array])\n",
    "final_delta_scaling_percent = np.array([dict_results[\"delta_scaling_percent\"][\"final\"][srun] for srun in sruns_array])\n",
    "\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(10, 7.8), sharex=True)\n",
    "\n",
    "ax1.plot(sruns_array, original_corr_p0, color=\"k\", alpha=0.7, label=\"Original\")\n",
    "ax1.fill_between(sruns_array, \n",
    "                 original_corr_p0 - original_corr_delta_p0, \n",
    "                 original_corr_p0 + original_corr_delta_p0,\n",
    "                 color=\"k\", alpha=0.2, ls=\"\")\n",
    "ax1.plot(sruns_array, final_corr_p0, color=\"mediumblue\", label=\"Scaled\")\n",
    "ax1.fill_between(sruns_array,\n",
    "                 final_corr_p0 - final_corr_delta_p0,\n",
    "                 final_corr_p0 + final_corr_delta_p0,\n",
    "                 color=\"mediumblue\", alpha=0.2, ls=\"\")\n",
    "\n",
    "ax2.plot(sruns_array, original_corr_p1, color=\"k\", alpha=0.7, label=\"Original\")\n",
    "ax2.fill_between(sruns_array, \n",
    "                 original_corr_p1 - original_corr_delta_p1, \n",
    "                 original_corr_p1 + original_corr_delta_p1,\n",
    "                 color=\"k\", alpha=0.2, ls=\"\")\n",
    "ax2.plot(sruns_array, final_corr_p1, color=\"crimson\", label=\"Scaled\")\n",
    "ax2.fill_between(sruns_array,\n",
    "                 final_corr_p1 - final_corr_delta_p1,\n",
    "                 final_corr_p1 + final_corr_delta_p1,\n",
    "                 color=\"crimson\", alpha=0.2, ls=\"\")\n",
    "\n",
    "ax3.plot(sruns_array, original_scaling_percent, color=\"k\", alpha=0.7, label=\"Original\")\n",
    "ax3.fill_between(sruns_array, \n",
    "                 original_scaling_percent - original_delta_scaling_percent, \n",
    "                 original_scaling_percent + original_delta_scaling_percent,\n",
    "                 color=\"k\", alpha=0.2, ls=\"\")\n",
    "ax3.plot(sruns_array, final_scaling_percent, color=\"g\", label=\"Scaled\")\n",
    "ax3.fill_between(sruns_array,\n",
    "                 final_scaling_percent - final_delta_scaling_percent,\n",
    "                 final_scaling_percent + final_delta_scaling_percent,\n",
    "                 color=\"g\", alpha=0.2, ls=\"\")\n",
    "\n",
    "ax4.plot(sruns_array, final_scaling_percent, color=\"g\", label=\"Scaled\")\n",
    "ax4.fill_between(sruns_array,\n",
    "                 final_scaling_percent - final_delta_scaling_percent,\n",
    "                 final_scaling_percent + final_delta_scaling_percent,\n",
    "                 color=\"g\", alpha=0.2, ls=\"\")\n",
    "\n",
    "\n",
    "ax1.axhline(ref_p0, color=\"k\", ls=\"--\", label=\"Reference\")\n",
    "ax2.axhline(ref_p1, color=\"k\", ls=\"--\")\n",
    "ax3.axhline(0, color=\"k\", ls=\"--\")\n",
    "ax4.axhline(0, color=\"k\", ls=\"--\")\n",
    "\n",
    "ax1.set_ylabel(\"Normalization\")\n",
    "ax2.set_ylabel(\"Power index\")\n",
    "ax3.set_ylabel(\"Scaling % needed\")\n",
    "ax4.set_ylabel(\"Scaling % needed\")\n",
    "ax4.set_xlabel(\"# Subrun\")\n",
    "\n",
    "ax1.set_ylim(0.9, 2)\n",
    "ax2.set_ylim(-2.8, -1.5)\n",
    "ax3.set_ylim(-10, 84)\n",
    "ax4.set_ylim(-5, 5)\n",
    "ax1.legend(frameon=False, ncols=3)\n",
    "ax3.legend(frameon=False)\n",
    "ax4.legend(frameon=False)\n",
    "\n",
    "# plt.savefig(f\"plots/total.png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9834b87e-7cd6-4412-8e4f-190103989e68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
