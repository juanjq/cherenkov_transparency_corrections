{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e70776f7-e2e1-4057-b7ef-878b2be0556d",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from datetime import datetime\n",
    "import pickle, json, sys, os, glob\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "# from scipy.optimize import minimize\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import chi2\n",
    "from scipy import optimize\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from traitlets.config.loader import Config\n",
    "from astropy.coordinates     import SkyCoord\n",
    "from lstchain.io.config      import get_standard_config\n",
    "from lstchain.io.io          import dl1_params_lstcam_key\n",
    "from ctapipe.io              import read_table\n",
    "import tables\n",
    "\n",
    "# location of the scripts\n",
    "sys.path.insert(0, os.getcwd() + \"/../scripts/\")\n",
    "import auxiliar as aux\n",
    "import geometry as geom\n",
    "import lstpipeline\n",
    "import plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cff6f04",
   "metadata": {},
   "source": [
    "# Some configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ff173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source specifications\n",
    "source_name = \"crab\"\n",
    "\n",
    "# Maybe we add a way to extract the run numbers we are interested in\n",
    "run_number = 6172 # [6172, 6242, 6194, 6193, 15337, 15272, 15339, 15340]\n",
    "\n",
    "# --- FIT PARAMETERS --- #\n",
    "# hist_intensity_binning = np.logspace(np.log10(4), np.log10(6000), 400)   # intensity binning\n",
    "limits_intensity = [316, 562]                  # limits for the fit\n",
    "limit_intensity_extended = 60 # limits for the subsets\n",
    "\n",
    "####################################\n",
    "# --- POWER LAW PARAMETERS REF --- #\n",
    "reference_normalization =  1.74 \n",
    "reference_powerindex    = -2.23\n",
    "####################################\n",
    "\n",
    "# Threshold in statistics for the last subrun\n",
    "statistics_threshold = 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ea7368",
   "metadata": {},
   "source": [
    "# Paths to data and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66854bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root path of this script\n",
    "root = os.getcwd() + \"/\"\n",
    "# Path to store the configuration file we are going to use\n",
    "config_file = root + \"config/standard_config.json\"\n",
    "# Path to store objects\n",
    "root_objects = root + f\"objects/\"\n",
    "# Data main directory\n",
    "root_data = root + f\"../../data/cherenkov_transparency_corrections/{source_name}/\"\n",
    "# Directory for the results of the fit of each run\n",
    "root_results = root_objects + \"results_fits/\"\n",
    "#Directory to store the scaled dl1 data\n",
    "root_dl1 = root_objects + f\"dl1_scaled/{run_number:05}/\"\n",
    "\n",
    "# STANDARD paths ---------\n",
    "dl1_root = \"/fefs/aswg/data/real/DL1/*/v0.*/tailcut84/\"\n",
    "rfs_root = \"/fefs/aswg/data/models/AllSky/20230901_v0.10.4_allsky_base_prod/\"\n",
    "mcs_root = \"/fefs/aswg/data/mc/DL2/AllSky/20230901_v0.10.4_allsky_base_prod/TestingDataset/\"\n",
    "\n",
    "# Create the paths that do not exist\n",
    "for path in [os.path.dirname(config_file), root_objects, root_results, root_dl1]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(os.path.join(path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb8d10f",
   "metadata": {},
   "source": [
    "# Opening and storing configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5030e785",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = get_standard_config()\n",
    "\n",
    "# We select the heuristic flatfield option in the standard configuration\n",
    "config_dict[\"source_config\"][\"LSTEventSource\"][\"use_flatfield_heuristic\"] = True\n",
    "\n",
    "with open(config_file, \"w\") as json_file:\n",
    "    json.dump(config_dict, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b32e12",
   "metadata": {},
   "source": [
    "# Finding the files that interest us\n",
    "#### Extracting dl1 files and dl1 datachecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95eb908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting coordinates of source\n",
    "source_coords = SkyCoord.from_name(source_name)\n",
    "\n",
    "dict_source = {\n",
    "    \"name\"   : source_name,\n",
    "    \"coords\" : source_coords,\n",
    "    \"ra\"     : source_coords.ra.deg  * u.deg, # ra in degrees\n",
    "    \"dec\"    : source_coords.dec.deg * u.deg, # dec in degrees\n",
    "}\n",
    "\n",
    "# We create a empty dictionary to store all the information needed inside\n",
    "DICT = {}\n",
    "for run in [run_number]:\n",
    "    DICT[run] = {\n",
    "        \"run_num\" : run,\n",
    "        \"errors\"  : \"\", # log of errors trough the analysis\n",
    "    }\n",
    "\n",
    "DICT = lstpipeline.add_dl1_paths_to_dict(DICT, dl1_root)\n",
    "DICT = lstpipeline.add_dl1_paths_to_dict(DICT, dl1_root, dchecking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac5fda0",
   "metadata": {},
   "source": [
    "#### Then we read the observations information and also the selected nodes for MC and RFs and we add it to the DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f1e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in [run_number]:\n",
    "\n",
    "    tab = read_table(DICT[run][\"dchecks\"][\"runwise\"], \"/dl1datacheck/cosmics\")\n",
    "    \n",
    "    # reading the variables\n",
    "    _zd = 90 - np.rad2deg(np.array(tab[\"mean_alt_tel\"]))\n",
    "    _az = np.rad2deg(np.array(tab[\"mean_az_tel\"]))\n",
    "    _t_start   = tab[\"dragon_time\"][0][0]\n",
    "    _t_elapsed = np.array(tab[\"elapsed_time\"])\n",
    "    \n",
    "    DICT[run][\"time\"] = {\n",
    "        \"tstart\"   : _t_start,            # datetime object\n",
    "        \"telapsed\" : np.sum(_t_elapsed),  # s\n",
    "        \"srunwise\" : {\n",
    "            \"telapsed\" : _t_elapsed,      # s      \n",
    "        },\n",
    "    }\n",
    "    DICT[run][\"pointing\"] = {\n",
    "        \"zd\" : np.mean(_zd),  # deg\n",
    "        \"az\" : np.mean(_az),  # deg\n",
    "        \"srunwise\" : {\n",
    "            \"zd\" : _zd,       # deg\n",
    "            \"az\" : _az,       # deg\n",
    "        },\n",
    "    }\n",
    "    \n",
    "# then we also select the RFs and MC files looking at the nodes available\n",
    "DICT, dict_nodes = lstpipeline.add_mc_and_rfs_nodes(DICT, rfs_root, mcs_root, dict_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261c0e68-ee3e-40b1-a11f-0eeaed2ca94b",
   "metadata": {},
   "source": [
    "### Read datacheck\n",
    "#### - The binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f3cc7f-95f6-405c-a8ad-19b8cbc8f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_intensity = 10 ** ((np.log10(limits_intensity[0]) + np.log10(limits_intensity[1]))/2)\n",
    "print(f\"The intensity in the middle of the intensity range is {reference_intensity:.1f} p.e.\")\n",
    "\n",
    "########################################################\n",
    "# Reading the binning from the datacheck ---------------\n",
    "# Opening the corresponding datacheck\n",
    "dcheck = DICT[run_number][\"dchecks\"][\"runwise\"]\n",
    "run_dcheck  = tables.open_file(dcheck)\n",
    "\n",
    "# Read the binning from the datacheck of the first subrun\n",
    "dcheck_intensity_binning = np.array(run_dcheck.root.dl1datacheck.histogram_binning.col(\"hist_intensity\")[0])\n",
    "# Calculating the logarithmic center of each bin\n",
    "dcheck_intensity_binning_centers = (dcheck_intensity_binning[:-1] * dcheck_intensity_binning[1:]) ** 0.5\n",
    "# Calculating the width of each bin\n",
    "dcheck_intensity_binning_widths = np.diff(dcheck_intensity_binning)\n",
    "run_dcheck.close()\n",
    "\n",
    "# Mask for the fitting region in the fits\n",
    "mask_dcheck_bins_fit = ((dcheck_intensity_binning_centers >= limits_intensity[0]) & (dcheck_intensity_binning_centers <= limits_intensity[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eca996a-e892-4e46-9a04-7b18fb9411cf",
   "metadata": {},
   "source": [
    "#### - The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7716a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# Reading the histogram from the datacheck ---------------\n",
    "# Opening the corresponding datacheck\n",
    "run_dcheck = tables.open_file(dcheck)\n",
    "dcheck_hist_intensities = np.array(run_dcheck.root.dl1datacheck.cosmics.col(\"hist_intensity\"))\n",
    "run_dcheck.close()\n",
    "\n",
    "# Converting from counts to rate per intensity unit (non-binning dependent quantity)\n",
    "hist_rates       = [] # Array of histogram of rates for each subrun\n",
    "hist_delta_rates = [] # The statistical error\n",
    "for srun, dcheck_hist_intensity in enumerate(dcheck_hist_intensities):\n",
    "\n",
    "    effective_time_srun = DICT[run_number][\"time\"][\"srunwise\"][\"telapsed\"][srun]\n",
    "    \n",
    "    hist_rates.append(              dcheck_hist_intensity  / effective_time_srun / dcheck_intensity_binning_widths)\n",
    "\n",
    "    hist_delta_rates.append(np.sqrt(dcheck_hist_intensity) / effective_time_srun / dcheck_intensity_binning_widths)\n",
    "    \n",
    "# Number of events in last subrun\n",
    "last_subrun_statistics = np.sum(dcheck_hist_intensities[-1])\n",
    "\n",
    "# Subruns array \n",
    "sruns_array = np.arange(len(hist_rates))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aa528a-a16b-4597-a186-db8416cf88a4",
   "metadata": {},
   "source": [
    "#### - Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a5d69f-85a3-4e5b-afda-e7a2a7e5cef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing the fit for each subrun\n",
    "normalization_fit       = []\n",
    "powerindex_fit          = []\n",
    "delta_normalization_fit = []\n",
    "delta_powerindex_fit    = []\n",
    "pvalue_fit              = []\n",
    "chi2_fit                = []\n",
    "for srun, hist in enumerate(hist_rates):\n",
    "\n",
    "    # Displacing the X-coordinates to the center of the fit, in order to decorrelate the fit\n",
    "    xfit = dcheck_intensity_binning_centers[mask_dcheck_bins_fit] / reference_intensity\n",
    "    yfit = hist_rates[srun][mask_dcheck_bins_fit]\n",
    "    \n",
    "    # Performing the fit\n",
    "    params, pcov, info, _, _ = curve_fit(\n",
    "        f     = geom.powerlaw,\n",
    "        xdata = xfit,\n",
    "        ydata = yfit,\n",
    "        sigma = hist_delta_rates[srun][mask_dcheck_bins_fit],\n",
    "        p0    = [reference_normalization, reference_powerindex],\n",
    "        full_output = True,\n",
    "    )\n",
    "    \n",
    "    normalization_fit.append(params[0])\n",
    "    powerindex_fit.append(params[1])\n",
    "    delta_normalization_fit.append(np.sqrt(pcov[0, 0]))\n",
    "    delta_powerindex_fit.append(np.sqrt(pcov[1, 1]))\n",
    "        \n",
    "    _chi2 = np.sum(info['fvec'] ** 2)\n",
    "    chi2_fit.append(_chi2)\n",
    "    pvalue_fit.append(1 - chi2.cdf(_chi2, sum(mask_dcheck_bins_fit)))\n",
    "\n",
    "    \n",
    "# Convert to numpy arrays\n",
    "normalization_fit       = np.array(normalization_fit)\n",
    "powerindex_fit          = np.array(powerindex_fit)\n",
    "delta_normalization_fit = np.array(delta_normalization_fit)\n",
    "delta_powerindex_fit    = np.array(delta_powerindex_fit)\n",
    "pvalue_fit              = np.array(pvalue_fit)\n",
    "chi2_fit                = np.array(chi2_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f4c9f3-b084-42d7-a877-a15319ac8d7b",
   "metadata": {},
   "source": [
    "#### - Zenith corrections and light yield\n",
    "\n",
    "Taking into account the logarithmic scale that is used, the light yield can be calculated with the following expression:\\\n",
    "\\\n",
    "$\\qquad\\qquad LY(A_{fit}, \\alpha; A_{ref}) = \\left(\\frac{A_{fit}}{A_{ref}}\\right)^{-\\frac{1}{1+\\alpha}}$\\\n",
    "\\\n",
    "The uncertainties have been calculated propagating the known ones.\\\n",
    "$\\sigma_{LY} = \\sqrt{\\sum_x\\left(\\frac{\\partial LY}{\\partial x}\\right)^2 \\sigma_x^2}$\\\n",
    "Where:\\\n",
    "$\\left(\\frac{\\partial LY}{\\partial A}\\right) =\\left(\\frac{A_{fit}}{A_{ref}}\\right)^{-\\frac{1}{1+\\alpha}} \\dfrac{-1}{\\left({\\alpha}+1\\right)A_{fit}}$\\\n",
    "$\\left(\\frac{\\partial LY}{\\partial \\alpha}\\right) = \\ln \\left(\\frac{A_{fit}}{A_{ref}}\\right) \\left(\\frac{A_{fit}}{A_{ref}}\\right)^{-\\frac{1}{1+\\alpha}} \\frac{1}{(1+\\alpha)^2}$\\\n",
    "\\\n",
    "So the final error will be:\n",
    "\\\n",
    "$\\sigma_{LY} = \\left(\\frac{A_{fit}}{A_{ref}}\\right)^{-\\frac{1}{1+\\alpha}} \\frac{1}{1+\\alpha}\\sqrt{A_{fit}^{-2}\\sigma_{A}^2+\\ln\\left(\\frac{A_{fit}}{A_{ref}}\\right)^2\\frac{\\sigma_{\\alpha}^2}{(1+\\alpha)^2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab94c3c4-990c-4cc1-abd7-302c814c9899",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# --- PARAMETERS RELATION --- #\n",
    "p0a, p0b, p0c = -0.44751321, 3.62502037, -1.43611437\n",
    "p1a, p1b, p1c = -2.89253919, 0.99443581, -0.34013068\n",
    "###############################\n",
    "\n",
    "# Zenith corrections to the parameters\n",
    "corr_normalization_fit = normalization_fit * geom.pol2(1, p0a, p0b, p0c) / geom.pol2(np.cos(np.deg2rad(DICT[run_number][\"pointing\"][\"zd\"])), p0a, p0b, p0c)\n",
    "corr_powerindex_fit    = powerindex_fit    + geom.pol2(1, p1a, p1b, p1c) - geom.pol2(np.cos(np.deg2rad(DICT[run_number][\"pointing\"][\"zd\"])), p1a, p1b, p1c)\n",
    "\n",
    "corr_delta_normalization_fit = delta_normalization_fit * geom.pol2(1, p0a, p0b, p0c) / geom.pol2(np.cos(np.deg2rad(DICT[run_number][\"pointing\"][\"zd\"])), p0a, p0b, p0c)\n",
    "corr_delta_powerindex_fit    = delta_powerindex_fit\n",
    "\n",
    "# Calculating the needed light yield\n",
    "initial_light_yield = geom.calc_light_yield(reference_normalization, corr_normalization_fit, corr_powerindex_fit)\n",
    "\n",
    "def sigma_LY(A_fit, A_ref, a_fit, sigma_A, sigma_a):\n",
    "\n",
    "    dlydA = A_fit ** (- 1 / (1 + a_fit)) * 1 / (1 + a_fit)    * (-1) / A_fit\n",
    "    dlyda = A_fit ** (- 1 / (1 + a_fit)) * 1 / (1 + a_fit)**2 * np.log(A_fit / A_ref) \n",
    "\n",
    "    return np.sqrt((dlydA) ** 2 * sigma_A ** 2 + (dlyda) ** 2 * sigma_a ** 2)\n",
    "    \n",
    "delta_initial_light_yield = sigma_LY(corr_normalization_fit, reference_normalization, \n",
    "                                     corr_powerindex_fit,\n",
    "                                     corr_delta_normalization_fit, corr_delta_powerindex_fit)\n",
    "\n",
    "# Zenith correction of the reference (putting the reference in the zenith of the determined subrun)\n",
    "corr_reference_normalization = reference_normalization / (geom.pol2(1, p0a, p0b, p0c) / geom.pol2(np.cos(np.deg2rad(DICT[run_number][\"pointing\"][\"zd\"])), p0a, p0b, p0c))\n",
    "corr_reference_powerindex    = reference_powerindex    - (geom.pol2(1, p1a, p1b, p1c) - geom.pol2(np.cos(np.deg2rad(DICT[run_number][\"pointing\"][\"zd\"])), p1a, p1b, p1c))\n",
    "\n",
    "print(f\"The initial mean scaling factor obtained is {1 / np.mean(initial_light_yield):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1f71db-c05a-4026-a0ca-4069ca3df17d",
   "metadata": {},
   "source": [
    "#### - Defining scaling\n",
    "In order to sample we will set two scalings for the conversion.\n",
    "\n",
    "* $1/LY$ The ideal scaling needed. The case where no new pixels emerge on the cleaning (unrealistic).\n",
    "\n",
    "* $\\frac{1/LY +1}{2}$ Half of the anterior scaling, just for sampling reasons.\n",
    "\n",
    "And the error is calculated now as:\n",
    "\\\n",
    "$\\sigma_{s} = \\frac{1}{LY^4}\\sigma_{LY}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ae712b-67d6-446c-b229-896e9719c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalings to apply\n",
    "upper_scaling       = 1 / initial_light_yield\n",
    "delta_upper_scaling = 1 / initial_light_yield ** 4 * delta_initial_light_yield\n",
    "\n",
    "# In the case of the last subrun we use the last subrun statistics, because the lack of statistics\n",
    "if last_subrun_statistics < statistics_threshold:\n",
    "    upper_scaling[-1] = upper_scaling[-2]\n",
    "    delta_upper_scaling[-1] = delta_upper_scaling[-2]\n",
    "\n",
    "    print(f\"For last subrun using before subrun because of low statistics ({last_subrun_statistics}evs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319183a7",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><<-----------------------------------------------------------------------------------------------\\>></span>\n",
    "#### <span style=\"color:orange\"><<Some plots and checks that will not be included in the final script\\>></span>\n",
    "#### <span style=\"color:orange\"><<-----------------------------------------------------------------------------------------------\\>></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37892355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning for the plots ------------------\n",
    "bins_space = np.linspace(limits_intensity[0] - 100, limits_intensity[1] + 200, 100)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4.5))\n",
    "\n",
    "colors = plotting.get_colors_multiplot(sruns_array)\n",
    "for i in range(len(hist_rates)):\n",
    "    ax.plot(dcheck_intensity_binning_centers, hist_rates[i], color=colors[i], zorder=np.random.rand())\n",
    "\n",
    "cmap = plotting.create_cmap_from_colors(plotting.default_colors)\n",
    "plotting.plot_colorbar(fig, ax, range(len(hist_rates)), cmap=cmap, label=\"Subrun number\")\n",
    "\n",
    "intensity_sample = np.linspace(limits_intensity[0] / 1.3, limits_intensity[1] * 1.5, 100)\n",
    "ax.plot(bins_space, geom.powerlaw(bins_space / reference_intensity, corr_reference_normalization, corr_reference_powerindex), color=\"k\", ls=\"--\", label=\"Reference\")\n",
    "ax.axvspan(limits_intensity[0], limits_intensity[1], alpha=0.3, ls=\"-\", facecolor=\"none\", hatch=\"///\", edgecolor=\"k\", label=\"Fit region\")\n",
    "\n",
    "ax.set_xlabel(\"Intensity [p.e.]\")\n",
    "ax.set_ylabel(\"Rate [events / s / p.e.]\")\n",
    "ax.loglog()\n",
    "ax.set_xlim(1.5e1, 1e4)\n",
    "ax.set_ylim(1e-3, 1e2)\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "# plt.savefig(f\"plots/total.png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e7178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 5), sharex=True)\n",
    "\n",
    "ax1.plot(sruns_array, corr_normalization_fit, color=\"mediumblue\")\n",
    "ax1.fill_between(sruns_array, corr_normalization_fit - corr_delta_normalization_fit, corr_normalization_fit + corr_delta_normalization_fit,\n",
    "                 color=\"mediumblue\", alpha=0.2, ls=\"\")\n",
    "\n",
    "\n",
    "ax2.plot(sruns_array, corr_powerindex_fit, color=\"crimson\")\n",
    "ax2.fill_between(sruns_array, corr_powerindex_fit - corr_delta_powerindex_fit, corr_powerindex_fit + corr_delta_powerindex_fit, \n",
    "                 color=\"crimson\", alpha=0.2, ls=\"\")\n",
    "\n",
    "ax3.plot(sruns_array, upper_scaling, color=\"gray\", label=\"$1/LY$\")\n",
    "ax3.fill_between(sruns_array, upper_scaling - delta_upper_scaling, upper_scaling + delta_upper_scaling, \n",
    "                 color=\"gray\", alpha=0.2, ls=\"\")\n",
    "\n",
    "ax1.axhline(reference_normalization, color=\"k\", ls=\"--\", label=\"Reference\")\n",
    "ax2.axhline(reference_powerindex, color=\"k\", ls=\"--\")\n",
    "ax3.axhline(1, color=\"k\", ls=\"--\")\n",
    "\n",
    "ax1.set_ylabel(\"Normalization\")\n",
    "ax2.set_ylabel(\"Power index\")\n",
    "ax3.set_ylabel(\"1 / LY\")\n",
    "ax3.set_xlabel(\"# Subrun\")\n",
    "\n",
    "ax1.set_ylim(0.9, 2)\n",
    "ax2.set_ylim(-2.8, -1.5)\n",
    "ax3.set_ylim(0.9, 2.1)\n",
    "ax1.legend(frameon=False)\n",
    "ax3.legend(frameon=False)\n",
    "\n",
    "# plt.savefig(f\"plots/total.png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13abbb8c-c047-46ad-809a-6cb4bdb9d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 2), sharex=True)\n",
    "axt = ax.twinx()\n",
    "\n",
    "p1, = ax.plot(sruns_array, chi2_fit, color=\"gray\")\n",
    "p2, = axt.plot(sruns_array, pvalue_fit, ls=\"-\", color=\"cornflowerblue\")\n",
    "\n",
    "ax.set_ylabel(\"$\\chi^2$\")\n",
    "axt.set_ylabel(\"p-value\")\n",
    "ax.set_xlabel(\"# Subrun\")\n",
    "axt.set_yscale(\"log\")\n",
    "\n",
    "ax.yaxis.label.set_color(p1.get_color())\n",
    "axt.yaxis.label.set_color(p2.get_color())\n",
    "ax.tick_params(axis='y', colors=p1.get_color())\n",
    "axt.tick_params(axis='y', colors=p2.get_color())\n",
    "\n",
    "# plt.savefig(f\"plots/total.png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528427e3-db8b-4ace-9d6e-551f1330c602",
   "metadata": {},
   "source": [
    "#### Create a dictionary to store all the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dece050-80e3-487f-8a4a-b59ac15d515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty dict to store the information\n",
    "dict_results = { \n",
    "    \"run\": run_number,\n",
    "    \"filenames\": {},\n",
    "    \"normalization\":        {\"default\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "    \"delta_normalization\":  {\"default\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "    \"powerindex\":           {\"default\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "    \"delta_powerindex\":     {\"default\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "    \"light_yield\":          {\"default\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "    \"delta_light_yield\":    {\"default\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "    \"scaling_needed\":       {\"default\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "    \"delta_scaling_needed\": {\"default\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "    \"chi2\":                 {\"default\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "    \"pvalue\":               {\"default\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "    \"final_scaling\": {},\n",
    "    \"final_scaling_interpolated\": {},\n",
    "    \"interpolation\" : None,\n",
    "}\n",
    "\n",
    "# Filling with the information we have\n",
    "for srun in sruns_array:\n",
    "    dict_results[\"normalization\"][\"default\"]        = corr_normalization_fit\n",
    "    dict_results[\"delta_normalization\"][\"default\"]  = corr_delta_normalization_fit\n",
    "    dict_results[\"powerindex\"][\"default\"]           = corr_powerindex_fit\n",
    "    dict_results[\"delta_powerindex\"][\"default\"]     = corr_delta_powerindex_fit\n",
    "    dict_results[\"light_yield\"][\"default\"]          = initial_light_yield\n",
    "    dict_results[\"light_yield\"][\"default\"]          = delta_initial_light_yield\n",
    "    dict_results[\"scaling_needed\"][\"default\"]       = upper_scaling\n",
    "    dict_results[\"delta_scaling_needed\"][\"default\"] = delta_upper_scaling\n",
    "    dict_results[\"chi2\"][\"default\"]                 = chi2_fit\n",
    "    dict_results[\"pvalue\"][\"default\"]               = pvalue_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36993d84",
   "metadata": {},
   "source": [
    "### Performing the first scaling for first run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4599153-804e-44a4-bd7d-dd06d357c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Processing subrun by subrun\n",
    "for srun in sruns_array:\n",
    "\n",
    "    print(f\"\\nProcessing subrun {srun}/{len(sruns_array)}\")\n",
    "    \n",
    "    input_fname = DICT[run_number][\"dl1a\"][\"srunwise\"][srun]\n",
    "\n",
    "    # Generating the scaled files\n",
    "    output_fname_upper = root_objects + f\"tmp_dl1_srunwise_upper_scaled.h5\"\n",
    "    scale = upper_scaling[srun]\n",
    "\n",
    "    # If the file already exists we delete it\n",
    "    if os.path.exists(output_fname_upper):\n",
    "        os.remove(output_fname_upper)\n",
    "    \n",
    "    # If scale is greater than 1 we select a range lower than the original one\n",
    "    # Otherwise we select a range higher than the original one\n",
    "    if scale > 1:\n",
    "        dl1_selected_range = f\"{limit_intensity_extended:.2f},{limits_intensity[1]:.2f}\"\n",
    "    else:\n",
    "        dl1_selected_range = f\"{limits_intensity[0]:.2f},inf\"\n",
    "\n",
    "    print(f\"Running lstchain_dl1ab... scale: {scale:.2f}\")\n",
    "\n",
    "    !lstchain_dl1ab \\\n",
    "    --input-file $input_fname \\\n",
    "    --output-file $output_fname_upper \\\n",
    "    --config $config_file \\\n",
    "    --no-image \\\n",
    "    --light-scaling $scale \\\n",
    "    --intensity-range $dl1_selected_range\n",
    "\n",
    "    # Reading the files\n",
    "    dl1b_file = tables.open_file(output_fname_upper)\n",
    "    _hist_intensity, _ = np.histogram(dl1b_file.root.dl1.event.telescope.parameters.LST_LSTCam.col(\"intensity\"), bins=dcheck_intensity_binning)\n",
    "    dl1b_file.close()\n",
    "\n",
    "    \n",
    "    # Calculating the non binning dependent transformation\n",
    "    effective_time_srun = DICT[run_number][\"time\"][\"srunwise\"][\"telapsed\"][srun]\n",
    "    rates       = np.array(_hist_intensity) / effective_time_srun / dcheck_intensity_binning_widths\n",
    "    delta_rates = np.sqrt(_hist_intensity)  / effective_time_srun / dcheck_intensity_binning_widths\n",
    "\n",
    "    # Displacing the X-coordinates to the center of the fit, in order to decorrelate the fit\n",
    "    xfit = dcheck_intensity_binning_centers[mask_dcheck_bins_fit] / reference_intensity\n",
    "    yfit = rates[mask_dcheck_bins_fit]\n",
    "    \n",
    "    # Performing the fit\n",
    "    params, pcov, info, _, _ = curve_fit(\n",
    "        f     = geom.powerlaw,\n",
    "        xdata = xfit,\n",
    "        ydata = yfit,\n",
    "        sigma = delta_rates[mask_dcheck_bins_fit],\n",
    "        p0    = [reference_normalization, reference_powerindex],\n",
    "        full_output = True,\n",
    "    )\n",
    "        \n",
    "    norm         = params[0]\n",
    "    pindex       = params[1]\n",
    "    delta_norm   = np.sqrt(pcov[0, 0])\n",
    "    delta_pindex = np.sqrt(pcov[1, 1])\n",
    "    _chi2        = np.sum(info['fvec'] ** 2)\n",
    "    pvalue       = 1 - chi2.cdf(_chi2, sum(mask_dcheck_bins_fit))\n",
    "\n",
    "    # Zenith corrections to the parameters\n",
    "    corr_norm         = norm       * geom.pol2(1, p0a, p0b, p0c) / geom.pol2(np.cos(np.deg2rad(DICT[run_number][\"pointing\"][\"zd\"])), p0a, p0b, p0c)\n",
    "    corr_pindex       = pindex     + geom.pol2(1, p1a, p1b, p1c) - geom.pol2(np.cos(np.deg2rad(DICT[run_number][\"pointing\"][\"zd\"])), p1a, p1b, p1c)\n",
    "    corr_delta_norm   = delta_norm * geom.pol2(1, p0a, p0b, p0c) / geom.pol2(np.cos(np.deg2rad(DICT[run_number][\"pointing\"][\"zd\"])), p0a, p0b, p0c)\n",
    "    corr_delta_pindex = delta_pindex\n",
    "    \n",
    "    # Calculating the needed light yield\n",
    "    light_yield = geom.calc_light_yield(reference_normalization, corr_norm, corr_pindex)\n",
    "    delta_light_yield = sigma_LY(corr_norm, reference_normalization, corr_pindex, corr_delta_norm, corr_delta_pindex)\n",
    "\n",
    "    # The scaling needed\n",
    "    scaling_needed       = 1 / light_yield\n",
    "    delta_scaling_needed = 1 / light_yield ** 4 * delta_light_yield    \n",
    "    \n",
    "    # Storing the results\n",
    "    dict_results[\"normalization\"][\"upper\"][srun]        = corr_norm\n",
    "    dict_results[\"delta_normalization\"][\"upper\"][srun]  = corr_delta_norm\n",
    "    dict_results[\"powerindex\"][\"upper\"][srun]           = corr_pindex\n",
    "    dict_results[\"delta_powerindex\"][\"upper\"][srun]     = corr_delta_pindex\n",
    "    dict_results[\"light_yield\"][\"upper\"][srun]          = light_yield\n",
    "    dict_results[\"delta_light_yield\"][\"upper\"][srun]    = delta_light_yield\n",
    "    dict_results[\"scaling_needed\"][\"upper\"][srun]       = scaling_needed\n",
    "    dict_results[\"delta_scaling_needed\"][\"upper\"][srun] = delta_scaling_needed\n",
    "    dict_results[\"chi2\"][\"upper\"][srun]                 = _chi2\n",
    "    dict_results[\"pvalue\"][\"upper\"][srun]               = pvalue\n",
    "        \n",
    "    # In the case of the last subrun we use the last subrun statistics, because the lack of statistics\n",
    "    if last_subrun_statistics < statistics_threshold:\n",
    "\n",
    "        linear_scaling = linear_scaling\n",
    "        print(f\"For last subrun using before subrun because of low statistics ({last_subrun_statistics}evs)\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Now putting all together, upper and half\n",
    "        points_scaling     = np.array([1, dict_results[\"scaling_needed\"][\"default\"][srun]])\n",
    "        points_light_yield = np.array([1/upper_scaling[srun], dict_results[\"light_yield\"][\"upper\"][srun]])\n",
    "    \n",
    "        # Finding the final scaling as a line that pass trogh the two points we have\n",
    "        # Then we calculate where the light yield will be 1 in linear approximation\n",
    "        slope = (points_light_yield[1] - points_light_yield[0]) / (points_scaling[1] - points_scaling[0])\n",
    "        intercept = points_light_yield[0] - slope * points_scaling[0]\n",
    "        linear_scaling = 1 / slope - points_light_yield[0] / slope + points_scaling[0]\n",
    "\n",
    "    output_fname_linear  = root_objects + f\"tmp_dl1_srunwise_linear_scaled.h5\"\n",
    "\n",
    "    # If the file already exists we delete it\n",
    "    if os.path.exists(output_fname_linear):\n",
    "        os.remove(output_fname_linear)\n",
    "        \n",
    "    !lstchain_dl1ab \\\n",
    "    --input-file $input_fname \\\n",
    "    --output-file $output_fname_linear \\\n",
    "    --config $config_file \\\n",
    "    --no-image \\\n",
    "    --light-scaling $linear_scaling \\\n",
    "    --intensity-range $dl1_selected_range\n",
    "\n",
    "\n",
    "    # Reading the files\n",
    "    dl1b_file = tables.open_file(output_fname_linear)\n",
    "    _hist_intensity, _ = np.histogram(dl1b_file.root.dl1.event.telescope.parameters.LST_LSTCam.col(\"intensity\"), bins=dcheck_intensity_binning)\n",
    "    dl1b_file.close()\n",
    "\n",
    "    \n",
    "    # Calculating the non binning dependent transformation\n",
    "    effective_time_srun = DICT[run_number][\"time\"][\"srunwise\"][\"telapsed\"][srun]\n",
    "    rates       = np.array(_hist_intensity) / effective_time_srun / dcheck_intensity_binning_widths\n",
    "    delta_rates = np.sqrt(_hist_intensity)  / effective_time_srun / dcheck_intensity_binning_widths\n",
    "\n",
    "    # Displacing the X-coordinates to the center of the fit, in order to decorrelate the fit\n",
    "    xfit = dcheck_intensity_binning_centers[mask_dcheck_bins_fit] / reference_intensity\n",
    "    yfit = rates[mask_dcheck_bins_fit]\n",
    "    \n",
    "    # Performing the fit\n",
    "    params, pcov, info, _, _ = curve_fit(\n",
    "        f     = geom.powerlaw,\n",
    "        xdata = xfit,\n",
    "        ydata = yfit,\n",
    "        sigma = delta_rates[mask_dcheck_bins_fit],\n",
    "        p0    = [reference_normalization, reference_powerindex],\n",
    "        full_output = True,\n",
    "    )\n",
    "        \n",
    "    norm         = params[0]\n",
    "    pindex       = params[1]\n",
    "    delta_norm   = np.sqrt(pcov[0, 0])\n",
    "    delta_pindex = np.sqrt(pcov[1, 1])\n",
    "    _chi2        = np.sum(info['fvec'] ** 2)\n",
    "    pvalue       = 1 - chi2.cdf(_chi2, sum(mask_dcheck_bins_fit))\n",
    "\n",
    "    # Zenith corrections to the parameters\n",
    "    corr_norm         = norm       * geom.pol2(1, p0a, p0b, p0c) / geom.pol2(np.cos(np.deg2rad(DICT[run_number][\"pointing\"][\"zd\"])), p0a, p0b, p0c)\n",
    "    corr_pindex       = pindex     + geom.pol2(1, p1a, p1b, p1c) - geom.pol2(np.cos(np.deg2rad(DICT[run_number][\"pointing\"][\"zd\"])), p1a, p1b, p1c)\n",
    "    corr_delta_norm   = delta_norm * geom.pol2(1, p0a, p0b, p0c) / geom.pol2(np.cos(np.deg2rad(DICT[run_number][\"pointing\"][\"zd\"])), p0a, p0b, p0c)\n",
    "    corr_delta_pindex = delta_pindex\n",
    "    \n",
    "    # Calculating the needed light yield\n",
    "    light_yield = geom.calc_light_yield(reference_normalization, corr_norm, corr_pindex)\n",
    "    delta_light_yield = sigma_LY(corr_norm, reference_normalization, corr_pindex, corr_delta_norm, corr_delta_pindex)\n",
    "\n",
    "    # The scaling needed\n",
    "    scaling_needed       = 1 / light_yield\n",
    "    delta_scaling_needed = 1 / light_yield ** 4 * delta_light_yield    \n",
    "    \n",
    "    # Storing the results\n",
    "    dict_results[\"normalization\"][\"linear\"][srun]        = corr_norm\n",
    "    dict_results[\"delta_normalization\"][\"linear\"][srun]  = corr_delta_norm\n",
    "    dict_results[\"powerindex\"][\"linear\"][srun]           = corr_pindex\n",
    "    dict_results[\"delta_powerindex\"][\"linear\"][srun]     = corr_delta_pindex\n",
    "    dict_results[\"light_yield\"][\"linear\"][srun]          = light_yield\n",
    "    dict_results[\"delta_light_yield\"][\"linear\"][srun]    = delta_light_yield\n",
    "    dict_results[\"scaling_needed\"][\"linear\"][srun]       = scaling_needed\n",
    "    dict_results[\"delta_scaling_needed\"][\"linear\"][srun] = delta_scaling_needed\n",
    "    dict_results[\"chi2\"][\"linear\"][srun]                 = _chi2\n",
    "    dict_results[\"pvalue\"][\"linear\"][srun]               = pvalue\n",
    "\n",
    "    # In the case of the last subrun we use the last subrun statistics, because the lack of statistics\n",
    "    if last_subrun_statistics < statistics_threshold:\n",
    "\n",
    "        final_scaling = final_scaling\n",
    "        print(f\"For last subrun using before subrun because of low statistics ({last_subrun_statistics}evs)\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Now putting all together, upper and linear\n",
    "        points_scaling     = np.array([1, linear_scaling, dict_results[\"scaling_needed\"][\"default\"][srun]])\n",
    "        points_light_yield = np.array([1/upper_scaling[srun], dict_results[\"light_yield\"][\"linear\"][srun], dict_results[\"light_yield\"][\"upper\"][srun]])\n",
    "    \n",
    "        # And fitting to a 2nd degree polynomial\n",
    "        pol2_scaling = np.poly1d(np.polyfit(points_scaling, points_light_yield, 2))\n",
    "    \n",
    "        # And finding the final scaling\n",
    "        final_scaling = optimize.root(pol2_scaling - 1, x0=half_scaling[srun]).x[0]\n",
    "\n",
    "        ############################################\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(5, 3))\n",
    "        ax.plot(points_scaling, points_light_yield, \"o\", color=\"k\", label=\"Points\", zorder=10)\n",
    "        size_points = max(points_scaling) - min(points_scaling)\n",
    "\n",
    "        ax.plot(\n",
    "            [points_scaling[0], points_scaling[2]], \n",
    "            [points_light_yield[0], points_light_yield[2]], \n",
    "            marker=\"\", ls=\"--\", lw=0.5, color=\"k\", label=\"\", zorder=10)\n",
    "\n",
    "        sample_scaling = np.linspace(min(points_scaling) - 0.1 * size_points, max(points_scaling) + 0.1 * size_points, 100)\n",
    "        ax.plot(sample_scaling, pol2_scaling(sample_scaling), color=\"crimson\", ls=\"-\", label=\"Polynomial fit\", zorder=5)\n",
    "        ax.axvline(final_scaling, color=\"k\", ls=\"--\", label=f\"Final scaling = {final_scaling:.2f}\")\n",
    "        ax.axhline(1, color=\"k\", ls=\":\", label=\"Reference\")\n",
    "        ax.set_xlabel(\"Scaling\")\n",
    "        ax.set_ylabel(\"Light yield\")\n",
    "        ax.legend(frameon=False)\n",
    "        ax.set_title(f\"Subrun {srun}\")\n",
    "        # plt.savefig(f\"plots/total.png\", bbox_inches=\"tight\", dpi=300)\n",
    "        plt.show()\n",
    "        ############################################\n",
    "        \n",
    "    dict_results[\"final_scaling\"][srun] = final_scaling\n",
    "    \n",
    "    # output_final_fname = root_data + f\"dl1_scaled/{run_number:05}/\" + os.path.basename(DICT[run_number][\"dl1a\"][\"srunwise\"][srun])\n",
    "    # dict_results[\"filenames\"][srun] = output_final_fname\n",
    "\n",
    "    # # If the file already exists we delete it\n",
    "    # if os.path.exists(output_final_fname):\n",
    "    #     os.remove(output_final_fname)\n",
    "        \n",
    "    # !lstchain_dl1ab \\\n",
    "    # --input-file $input_fname \\\n",
    "    # --output-file $output_final_fname \\\n",
    "    # --config $config_file \\\n",
    "    # --no-image \\\n",
    "    # --light-scaling $final_scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1d0e4d-a713-4a6d-b704-7c623de3c559",
   "metadata": {},
   "source": [
    "### Then we interpolate the lineal scalings with a linear behaviour trough the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c6fe63-e042-4a53-b3c5-8cc0deaecba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fit = np.cumsum(DICT[run_number][\"time\"][\"srunwise\"][\"telapsed\"])\n",
    "y_fit = np.array([dict_results[\"scaling_needed\"][\"final\"][srun][\"final_scaling\"] for srun in dict_results[\"scaling_needed\"][\"final\"].keys()])\n",
    "yerr_fit = np.array([dict_results[\"delta_scaling_needed\"][\"final\"][srun][\"final_scaling\"] for srun in dict_results[\"delta_scaling_needed\"][\"final\"].keys()])\n",
    "\n",
    "# Performing the fit\n",
    "params, pcov, info, _, _ = curve_fit(\n",
    "    f     = geom.straight_line,\n",
    "    xdata = x_fit,\n",
    "    ydata = y_fit,\n",
    "    sigma = yerr_fit,\n",
    "    p0    = [1, 0],\n",
    "    full_output = True,\n",
    ")\n",
    "    \n",
    "intercept       = params[0]\n",
    "slope           = params[1]\n",
    "delta_slope     = np.sqrt(pcov[0, 0])\n",
    "delta_intercept = np.sqrt(pcov[1, 1])\n",
    "_chi2           = np.sum(info['fvec'] ** 2)\n",
    "pvalue          = 1 - chi2.cdf(_chi2, len(xfit))\n",
    "\n",
    "dict_results[\"interpolation\"] = {\n",
    "    \"chi2\" : _chi2,      \n",
    "    \"p_value\" : pvalue,         \n",
    "    \"slope\": slope,      \n",
    "    \"delta_slope\" : delta_slope,     \n",
    "    \"intercept\" : intercept, \n",
    "    \"delta_intercept\" : delta_intercept,\n",
    "}\n",
    "\n",
    "#############################################\n",
    "#############################################\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8,1.5))\n",
    "\n",
    "ax.plot(x_fit/60, ((intercept + x_fit * slope) - 1) * 100, color=\"k\", ls=\"--\", zorder=10, label=\"Interpolation\")\n",
    "\n",
    "ax.plot(x_fit/60, (y_fit - 1) * 100, 'r', label='Data')\n",
    "ax.fill_between(x_fit / 60, (y_fit - 1) * 100 - yerr_fit * 100, (y_fit - 1) * 100 + yerr_fit * 100, color=\"r\", alpha=0.2, ls=\"\")\n",
    "\n",
    "ax.set_xlabel(\"Time elapsed [min]\")\n",
    "ax.set_ylabel(\"Scaling factor [%]\")\n",
    "ax.legend(ncols=2, frameon=False)\n",
    "\n",
    "plt.show()\n",
    "#############################################\n",
    "#############################################\n",
    "\n",
    "# Setting a interpolated scaling factor\n",
    "# for srun in dict_results[\"final_scaling\"].keys():\n",
    "\n",
    "    scaling_not_interpolated = dict_results[\"final_scaling\"][srun]\n",
    "    \n",
    "    scaling_interpolated = intercept + slope * scaling_not_interpolated\n",
    "    \n",
    "    dict_results[\"final_scaling_interpolated\"][srun] = scaling_interpolated\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba6c1f7-b50a-4ec8-bb56-9424395dadd4",
   "metadata": {},
   "source": [
    "### Then scale the full datasets by the interpolated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2872fa71-5cfe-4dcf-a59f-a56d57326cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Processing subrun by subrun\n",
    "for srun in sruns_array:\n",
    "\n",
    "    print(f\"\\nProcessing subrun {srun}/{len(sruns_array)}\")\n",
    "    \n",
    "    input_fname = DICT[run_number][\"dl1a\"][\"srunwise\"][srun]\n",
    "\n",
    "    final_scaling = dict_results[\"final_scaling_interpolated\"][srun]\n",
    "    \n",
    "    output_final_fname = root_data + f\"dl1_scaled/{run_number:05}/\" + os.path.basename(DICT[run_number][\"dl1a\"][\"srunwise\"][srun])\n",
    "    dict_results[\"filenames\"][srun] = output_final_fname\n",
    "\n",
    "    # If the file already exists we delete it\n",
    "    if os.path.exists(output_final_fname):\n",
    "        os.remove(output_final_fname)\n",
    "        \n",
    "    !lstchain_dl1ab \\\n",
    "    --input-file $input_fname \\\n",
    "    --output-file $output_final_fname \\\n",
    "    --config $config_file \\\n",
    "    --no-image \\\n",
    "    --light-scaling $final_scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251da006-01bc-4813-8413-57b3964a4aad",
   "metadata": {},
   "source": [
    "#### The crosscheck\n",
    "Reading now the already processed runs and run the fit in order to see if the correction lead to a almost zero light yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22187484-151e-4406-bc0d-4c4ce77ef98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dl1b_scaled = root_data + f\"dl1_scaled/{run_number:05}/dl1_LST-1.Run?????.????.h5\"\n",
    "dl1_files = np.sort(glob.glob(root_dl1b_scaled))\n",
    "\n",
    "# Converting from counts to rate per intensity unit (non-binning dependent quantity)\n",
    "final_hist_rates       = [] # Array of histogram of rates for each subrun\n",
    "final_hist_delta_rates = [] # The statistical error\n",
    "\n",
    "for srun, file in enumerate(dl1_files):\n",
    "    dl1b_file = tables.open_file(file)\n",
    "    final_hist_intensity, _ = np.histogram(dl1b_file.root.dl1.event.telescope.parameters.LST_LSTCam.col(\"intensity\"), \n",
    "                                           bins=dcheck_intensity_binning)\n",
    "    dl1b_file.close()\n",
    "    \n",
    "    effective_time_srun = DICT[run_number][\"time\"][\"srunwise\"][\"telapsed\"][srun]\n",
    "\n",
    "    rate_srun       =         final_hist_intensity  / effective_time_srun / dcheck_intensity_binning_widths\n",
    "    delta_rate_srun = np.sqrt(final_hist_intensity) / effective_time_srun / dcheck_intensity_binning_widths\n",
    "    \n",
    "    final_hist_rates.append(rate_srun)\n",
    "    final_hist_delta_rates.append(delta_rate_srun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70da46d-b811-4b04-a73f-b63e0cebd76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing the fit for each subrun\n",
    "final_normalization_fit       = []\n",
    "final_powerindex_fit          = []\n",
    "final_delta_normalization_fit = []\n",
    "final_delta_powerindex_fit    = []\n",
    "final_pvalue_fit              = []\n",
    "final_chi2_fit                = []\n",
    "for srun, hist in enumerate(hist_rates):\n",
    "\n",
    "    # Displacing the X-coordinates to the center of the fit, in order to decorrelate the fit\n",
    "    xfit = dcheck_intensity_binning_centers[mask_dcheck_bins_fit] / reference_intensity\n",
    "    yfit = final_hist_rates[srun][mask_dcheck_bins_fit]\n",
    "    \n",
    "    # Performing the fit\n",
    "    params, pcov, info, _, _ = curve_fit(\n",
    "        f     = geom.powerlaw,\n",
    "        xdata = xfit,\n",
    "        ydata = yfit,\n",
    "        sigma = final_hist_delta_rates[srun][mask_dcheck_bins_fit],\n",
    "        p0    = [reference_normalization, reference_powerindex],\n",
    "        full_output = True,\n",
    "    )\n",
    "    \n",
    "    final_normalization_fit.append(params[0])\n",
    "    final_powerindex_fit.append(params[1])\n",
    "    final_delta_normalization_fit.append(np.sqrt(pcov[0, 0]))\n",
    "    final_delta_powerindex_fit.append(np.sqrt(pcov[1, 1]))\n",
    "            \n",
    "    _chi2 = np.sum(info['fvec'] ** 2)\n",
    "    final_chi2_fit.append(_chi2)\n",
    "    final_pvalue_fit.append(1 - chi2.cdf(_chi2, sum(mask_dcheck_bins_fit)))\n",
    "\n",
    "    \n",
    "# Convert to numpy arrays\n",
    "final_normalization_fit       = np.array(final_normalization_fit)\n",
    "final_powerindex_fit          = np.array(final_powerindex_fit)\n",
    "final_delta_normalization_fit = np.array(final_delta_normalization_fit)\n",
    "final_delta_powerindex_fit    = np.array(final_delta_powerindex_fit)\n",
    "final_pvalue_fit              = np.array(final_pvalue_fit)\n",
    "final_chi2_fit                = np.array(final_chi2_fit)\n",
    "\n",
    "# Zenith corrections to the parameters\n",
    "corr_final_normalization_fit = final_normalization_fit * geom.pol2(1, p0a, p0b, p0c) / geom.pol2(np.cos(np.deg2rad(DICT[run_number][\"pointing\"][\"zd\"])), p0a, p0b, p0c)\n",
    "corr_final_powerindex_fit    = final_powerindex_fit    + geom.pol2(1, p1a, p1b, p1c) - geom.pol2(np.cos(np.deg2rad(DICT[run_number][\"pointing\"][\"zd\"])), p1a, p1b, p1c)\n",
    "\n",
    "corr_final_delta_normalization_fit = final_delta_normalization_fit * geom.pol2(1, p0a, p0b, p0c) / geom.pol2(np.cos(np.deg2rad(DICT[run_number][\"pointing\"][\"zd\"])), p0a, p0b, p0c)\n",
    "corr_final_delta_powerindex_fit    = final_delta_powerindex_fit\n",
    "\n",
    "# Calculating the needed light yield\n",
    "final_light_yield = geom.calc_light_yield(reference_normalization, corr_final_normalization_fit, corr_final_powerindex_fit)\n",
    "final_delta_light_yield = sigma_LY(corr_final_normalization_fit, reference_normalization, corr_final_powerindex_fit, \n",
    "                                   corr_final_delta_normalization_fit, corr_final_delta_powerindex_fit)\n",
    "\n",
    "# The scaling needed\n",
    "final_scalings       = 1 / final_light_yield\n",
    "delta_final_scalings = 1 / final_light_yield ** 4 * final_delta_light_yield  \n",
    "\n",
    "for srun, hist in enumerate(hist_rates):\n",
    "    \n",
    "    # Storing the results\n",
    "    dict_results[\"normalization\"][\"final\"][srun]        = corr_final_normalization_fit[srun]\n",
    "    dict_results[\"delta_normalization\"][\"final\"][srun]  = corr_final_delta_normalization_fit[srun]\n",
    "    dict_results[\"powerindex\"][\"final\"][srun]           = corr_final_powerindex_fit[srun]\n",
    "    dict_results[\"delta_powerindex\"][\"final\"][srun]     = corr_final_delta_powerindex_fit[srun]\n",
    "    dict_results[\"light_yield\"][\"final\"][srun]          = final_light_yield[srun]\n",
    "    dict_results[\"delta_light_yield\"][\"final\"][srun]    = final_delta_light_yield[srun]\n",
    "    dict_results[\"scaling_needed\"][\"final\"][srun]       = final_scalings[srun]\n",
    "    dict_results[\"delta_scaling_needed\"][\"final\"][srun] = delta_final_scalings[srun]\n",
    "    dict_results[\"chi2\"][\"final\"][srun]                 = final_chi2_fit[srun]\n",
    "    dict_results[\"pvalue\"][\"final\"][srun]               = final_pvalue_fit[srun]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5a9ff7-f47f-4446-b679-6a7cd9084517",
   "metadata": {},
   "source": [
    "## Storing the results of the fits in a file for each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8ceecf-1d37-40c2-97b7-74a9b8f536b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_fname = root_results + f\"results_run_{run_number}.pkl\"\n",
    "\n",
    "# Saving the objects\n",
    "with open(dict_fname, 'wb') as f:\n",
    "    pickle.dump(dict_results, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# Reading the object\n",
    "# with open(dict_fname, 'rb') as f:\n",
    "#     dict_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1458ffb",
   "metadata": {},
   "source": [
    "## Some plots for checking the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134aecf4-a312-41fd-ab70-1bd881fae598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning for the plots ------------------\n",
    "bins_space = np.linspace(limits_intensity[0] - 100, limits_intensity[1] + 200, 100)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(9, 4.5))\n",
    "\n",
    "for i in range(len(hist_rates)):\n",
    "    ax.plot(dcheck_intensity_binning_centers, hist_rates[i], color=\"lightgray\", label=\"Original data\" if i == 0 else \"\")\n",
    "\n",
    "for i in range(len(final_hist_rates)):\n",
    "    ax.plot(dcheck_intensity_binning_centers, final_hist_rates[i], color=\"crimson\", label=\"Scaled data\" if i == 0 else \"\")\n",
    "\n",
    "ax.plot(bins_space, geom.powerlaw(bins_space / reference_intensity, corr_reference_normalization, corr_reference_powerindex), color=\"k\", ls=\"--\", label=\"Reference\")\n",
    "ax.axvspan(limits_intensity[0], limits_intensity[1], alpha=0.3, ls=\"-\", facecolor=\"none\", hatch=\"///\", edgecolor=\"k\", label=\"Fit region\")\n",
    "\n",
    "ax.set_xlabel(\"Intensity [p.e.]\")\n",
    "ax.set_ylabel(\"Rate [events / s / p.e.]\")\n",
    "ax.loglog()\n",
    "ax.set_xlim(1.5e1, 1.5e4)\n",
    "ax.set_ylim(1e-4, 1e2)\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "# plt.savefig(f\"plots/total.png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de79b6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(10, 7.8), sharex=True)\n",
    "\n",
    "ax1.plot(sruns_array, corr_normalization_fit, color=\"k\", alpha=0.7, label=\"Original\")\n",
    "ax1.fill_between(sruns_array, \n",
    "                 corr_normalization_fit - corr_delta_normalization_fit, \n",
    "                 corr_normalization_fit + corr_delta_normalization_fit,\n",
    "                 color=\"k\", alpha=0.2, ls=\"\")\n",
    "ax1.plot(sruns_array, corr_final_normalization_fit, color=\"mediumblue\", label=\"Scaled\")\n",
    "ax1.fill_between(sruns_array,\n",
    "                 corr_final_normalization_fit - corr_final_delta_normalization_fit,\n",
    "                 corr_final_normalization_fit + corr_final_delta_normalization_fit,\n",
    "                 color=\"mediumblue\", alpha=0.2, ls=\"\")\n",
    "\n",
    "ax2.plot(sruns_array, corr_powerindex_fit, color=\"k\", alpha=0.7)\n",
    "ax2.plot(sruns_array, corr_final_powerindex_fit, color=\"crimson\",)\n",
    "ax2.fill_between(sruns_array,\n",
    "                 corr_final_powerindex_fit - corr_final_delta_powerindex_fit,\n",
    "                 corr_final_powerindex_fit + corr_final_delta_powerindex_fit,\n",
    "                 color=\"crimson\", alpha=0.2, ls=\"\")\n",
    "\n",
    "ax3.plot(sruns_array, (upper_scaling-1) * 100, color=\"gray\", label=\"$1/LY$\")\n",
    "ax3.fill_between(sruns_array, \n",
    "                 (upper_scaling-1) * 100 - delta_upper_scaling * 100, \n",
    "                 (upper_scaling-1) * 100 + delta_upper_scaling * 100, \n",
    "                 color=\"gray\", alpha=0.2, ls=\"\")\n",
    "\n",
    "ax3.plot(sruns_array, (1/final_light_yield-1) * 100, color=\"g\")\n",
    "ax4.plot(sruns_array, (1/final_light_yield-1) * 100, color=\"g\", label=\"Scaled data\")\n",
    "\n",
    "scale   = np.array([dict_results[\"scaling_needed\"][\"final\"][_srun] for _srun in dict_results[\"scaling_needed\"][\"final\"].keys()])\n",
    "d_scale = np.array([dict_results[\"delta_scaling_needed\"][\"final\"][_srun] for _srun in dict_results[\"scaling_needed\"][\"final\"].keys()])\n",
    "ax3.fill_between(sruns_array, \n",
    "                 (scale-1) * 100 - d_scale * 100, \n",
    "                 (scale-1) * 100 + d_scale * 100, \n",
    "                 color=\"g\", alpha=0.2, ls=\"\")\n",
    "ax4.fill_between(sruns_array, \n",
    "                 (scale-1) * 100 - d_scale * 100, \n",
    "                 (scale-1) * 100 + d_scale * 100, \n",
    "                 color=\"g\", alpha=0.2, ls=\"\")\n",
    "ax1.axhline(reference_normalization, color=\"k\", ls=\"--\", label=\"Reference\")\n",
    "ax2.axhline(reference_powerindex, color=\"k\", ls=\"--\")\n",
    "ax3.axhline(0, color=\"k\", ls=\"--\")\n",
    "ax4.axhline(0, color=\"k\", ls=\"--\")\n",
    "\n",
    "ax1.set_ylabel(\"Normalization\")\n",
    "ax2.set_ylabel(\"Power index\")\n",
    "ax3.set_ylabel(\"Scaling % needed\")\n",
    "ax4.set_ylabel(\"Scaling % needed\")\n",
    "ax4.set_xlabel(\"# Subrun\")\n",
    "\n",
    "ax1.set_ylim(0.9, 2)\n",
    "ax2.set_ylim(-2.8, -1.5)\n",
    "ax3.set_ylim(-10, 84)\n",
    "ax4.set_ylim(-5, 5)\n",
    "ax1.legend(frameon=False, ncols=3)\n",
    "ax3.legend(frameon=False)\n",
    "ax4.legend(frameon=False)\n",
    "\n",
    "# plt.savefig(f\"plots/total.png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
