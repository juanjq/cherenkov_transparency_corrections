{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e70776f7-e2e1-4057-b7ef-878b2be0556d",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from datetime import datetime\n",
    "import pickle, json, sys, os\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import chi2\n",
    "from scipy import optimize\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from traitlets.config.loader import Config\n",
    "from astropy.coordinates     import SkyCoord\n",
    "from lstchain.io.config      import get_standard_config\n",
    "from ctapipe.io              import read_table\n",
    "import tables\n",
    "\n",
    "# Other auxiliar scripts\n",
    "sys.path.insert(0, os.getcwd() + \"/../scripts/\")\n",
    "import auxiliar as aux\n",
    "import geometry as geom\n",
    "import lstpipeline\n",
    "import plotting\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cff6f04",
   "metadata": {},
   "source": [
    "# Some configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ff173",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Source name in order to just complete the results file, and\n",
    "in order to improve run organization.\"\"\"\n",
    "source_name = \"crab\"\n",
    "\n",
    "\"\"\" The run number that we are interested in apply the corrections.\n",
    "The process is done run-wise, so the input will be an individual run.\"\"\"\n",
    "# run_number  = 6172\n",
    "# srun_number = 0\n",
    "input_str = \"6172_0_1_25\"\n",
    "run_number   = int(input_str.split(\"_\")[0])\n",
    "srun_numbers = [int(s) for s in input_str.split(\"_\")[1:]]\n",
    "\n",
    "\"\"\" Fit parameters\n",
    "Chosen limits in intensity (p.e.) for applying the fit i.e. the\n",
    "power law will be fitted only with the points within this range.\"\"\"\n",
    "limits_intensity = [316, 562]\n",
    "\"\"\" For the positive scaling cases (most of them), we need to have a lower \n",
    "limit in intensity. Thi slimit is used for the subset of events that are \n",
    "scaled just to find which is the scaling value. We use a very low limit by\n",
    "default 60 p.e. compared to the lower limit of the fit 316 p.e. because in \n",
    "the worst cases we will have a very non-linear scaling that will displace \n",
    "significantly the events intensities.\"\"\"\n",
    "limits_intensity_extended = 60\n",
    "\n",
    "\"\"\" Power law parameters for the reference\n",
    "All these parameters are taken from a common analysis of the full dataset\n",
    "Where the period of end of 2022 and start 2023 is taken as reference for good \n",
    "runs. Then we take as reference the mean power law parameters in that period.\n",
    "p0 is the normalization factor and p1 is the slope.\"\"\"\n",
    "ref_p0 =  1.74 \n",
    "ref_p1 = -2.23\n",
    "\n",
    "\"\"\" Threshold in statistics for the last subrun\n",
    "The limit in number of events after cleaning that we need to consider the last\n",
    "subrun has enough statistics to perform the analysis over it. Otherwise the \n",
    "values of the scaling that will be applied to this last rubrun are the same \n",
    "that are applied to the last last subrun.\"\"\"\n",
    "statistics_threshold_last_srun = 15000\n",
    "\n",
    "\"\"\" Parameters for the empyrical fits for Zenith Distance corrections\n",
    "Are simply two 2 degree polynomials for each variable of the power law.\"\"\"\n",
    "p0a, p0b, p0c = -0.44751321, 3.62502037, -1.43611437\n",
    "p1a, p1b, p1c = -2.89253919, 0.99443581, -0.34013068\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ea7368",
   "metadata": {},
   "source": [
    "# Paths to data and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66854bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root path of this script\n",
    "root = os.getcwd() + \"/\"\n",
    "# Path to store the configuration file we are going to use\n",
    "config_file = root + \"config/standard_config.json\"\n",
    "# Path to store objects\n",
    "root_objects = root + f\"objects/\"\n",
    "# Data main directory\n",
    "root_data = root + f\"../../data/cherenkov_transparency_corrections/{source_name}/\"\n",
    "# Directory for the results of the fit of each run\n",
    "root_final_results = root_objects + \"final_results_fits/\"\n",
    "\n",
    "# STANDARD paths ---------\n",
    "root_dl1 = \"/fefs/aswg/data/real/DL1/*/v0.*/tailcut84/\"\n",
    "root_rfs = \"/fefs/aswg/data/models/AllSky/20230901_v0.10.4_allsky_base_prod/\"\n",
    "root_mcs = \"/fefs/aswg/data/mc/DL2/AllSky/20230901_v0.10.4_allsky_base_prod/TestingDataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb8d10f",
   "metadata": {},
   "source": [
    "# Opening and storing configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4351371c-f83f-49fe-bcc6-b7a1bdadbfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_config = get_standard_config()\n",
    "\n",
    "# We select the heuristic flatfield option in the standard configuration\n",
    "dict_config[\"source_config\"][\"LSTEventSource\"][\"use_flatfield_heuristic\"] = True\n",
    "\n",
    "with open(config_file, \"w\") as json_file:\n",
    "    json.dump(dict_config, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b32e12",
   "metadata": {},
   "source": [
    "# Finding the files that interest us\n",
    "#### Extracting dl1 files and dl1 datachecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95eb908",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Getting coordinates of source\n",
    "source_coords = SkyCoord.from_name(source_name)\n",
    "\n",
    "dict_source = {\n",
    "    \"name\"   : source_name,\n",
    "    \"coords\" : source_coords,\n",
    "    \"ra\"     : source_coords.ra.deg  * u.deg, # ra in degrees\n",
    "    \"dec\"    : source_coords.dec.deg * u.deg, # dec in degrees\n",
    "}\n",
    "\n",
    "# We create a empty dictionary to store all the information needed inside\n",
    "dict_dchecks = {}\n",
    "for run in [run_number]:\n",
    "    dict_dchecks[run] = {\n",
    "        \"run_num\" : run,\n",
    "    }\n",
    "\n",
    "dict_dchecks = lstpipeline.add_dl1_paths_to_dict(dict_dchecks, root_dl1)\n",
    "dict_dchecks = lstpipeline.add_dl1_paths_to_dict(dict_dchecks, root_dl1, dchecking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac5fda0",
   "metadata": {},
   "source": [
    "#### Then we read the observations information and also the selected nodes for MC and RFs and we add it to the DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f1e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_dcheck_run = read_table(dict_dchecks[run_number][\"dchecks\"][\"runwise\"], \"/dl1datacheck/cosmics\")\n",
    "\n",
    "# reading the variables\n",
    "dcheck_zd = 90 - np.rad2deg(np.array(tab_dcheck_run[\"mean_alt_tel\"]))\n",
    "dcheck_az = np.rad2deg(np.array(tab_dcheck_run[\"mean_az_tel\"]))\n",
    "dcheck_tstart   = tab_dcheck_run[\"dragon_time\"][0][0]\n",
    "dcheck_telapsed = np.array(tab_dcheck_run[\"elapsed_time\"])\n",
    "\n",
    "dict_dchecks[run_number][\"time\"] = {\n",
    "    \"tstart\"   : dcheck_tstart,            # datetime object\n",
    "    \"telapsed\" : np.sum(dcheck_telapsed),  # s\n",
    "    \"srunwise\" : {\n",
    "        \"telapsed\" : dcheck_telapsed,      # s      \n",
    "    },\n",
    "}\n",
    "dict_dchecks[run_number][\"pointing\"] = {\n",
    "    \"zd\" : np.mean(dcheck_zd),  # deg\n",
    "    \"az\" : np.mean(dcheck_az),  # deg\n",
    "    \"srunwise\" : {\n",
    "        \"zd\" : dcheck_zd, # deg\n",
    "        \"az\" : dcheck_az, # deg\n",
    "    },\n",
    "}\n",
    "    \n",
    "# then we also select the RFs and MC files looking at the nodes available\n",
    "dict_dchecks, dict_nodes = lstpipeline.add_mc_and_rfs_nodes(dict_dchecks, root_rfs, root_mcs, dict_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261c0e68-ee3e-40b1-a11f-0eeaed2ca94b",
   "metadata": {},
   "source": [
    "### Read datacheck\n",
    "#### - The binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f3cc7f-95f6-405c-a8ad-19b8cbc8f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_intensity = (limits_intensity[0] * limits_intensity[1]) ** 0.5\n",
    "logger.info(f\"The intensity in the middle of the intensity range is {ref_intensity:.1f} p.e.\")\n",
    "\n",
    "########################################################\n",
    "# Reading the binning from the datacheck ---------------\n",
    "# Opening the corresponding datacheck\n",
    "fname_dcheck = dict_dchecks[run_number][\"dchecks\"][\"runwise\"]\n",
    "tab_dcheck_run = tables.open_file(fname_dcheck)\n",
    "\n",
    "# Read the binning from the datacheck of the first subrun\n",
    "dcheck_intensity_binning = np.array(tab_dcheck_run.root.dl1datacheck.histogram_binning.col(\"hist_intensity\")[0])\n",
    "# Calculating the logarithmic center of each bin\n",
    "dcheck_intensity_binning_centers = (dcheck_intensity_binning[:-1] * dcheck_intensity_binning[1:]) ** 0.5\n",
    "# Calculating the width of each bin\n",
    "dcheck_intensity_binning_widths = np.diff(dcheck_intensity_binning)\n",
    "tab_dcheck_run.close()\n",
    "\n",
    "# Mask for the fitting region in the fits\n",
    "mask_dcheck_bins_fit = (\n",
    "    (dcheck_intensity_binning_centers >= limits_intensity[0]) &\n",
    "    (dcheck_intensity_binning_centers <= limits_intensity[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eca996a-e892-4e46-9a04-7b18fb9411cf",
   "metadata": {},
   "source": [
    "#### - The intensity data from the datacheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7716a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# Reading the histogram from the datacheck ---------------\n",
    "# Opening the corresponding datacheck\n",
    "tab_dcheck_run = tables.open_file(fname_dcheck)\n",
    "dcheck_hist_intensities = np.array(tab_dcheck_run.root.dl1datacheck.cosmics.col(\"hist_intensity\"))\n",
    "tab_dcheck_run.close()\n",
    "\n",
    "# Converting from counts to rate per intensity unit (non-binning dependent quantity)\n",
    "dcheck_rates       = [] # Array of histogram of rates for each subrun\n",
    "dcheck_delta_rates = [] # The statistical error\n",
    "for srun, dcheck_hist_intensity in enumerate(dcheck_hist_intensities):\n",
    "\n",
    "    effective_time_srun = dict_dchecks[run_number][\"time\"][\"srunwise\"][\"telapsed\"][srun]\n",
    "    \n",
    "    dcheck_rates.append(              dcheck_hist_intensity  / effective_time_srun / dcheck_intensity_binning_widths)\n",
    "\n",
    "    dcheck_delta_rates.append(np.sqrt(dcheck_hist_intensity) / effective_time_srun / dcheck_intensity_binning_widths)\n",
    "    \n",
    "# Number of events in last subrun\n",
    "last_subrun_statistics = np.sum(dcheck_hist_intensities[-1])\n",
    "\n",
    "# Subruns array \n",
    "sruns_array = np.sort([int(f[-7:-3]) for f in dict_dchecks[run_number][\"dl1a\"][\"srunwise\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1da3d41-9c8c-43a7-807f-d0a9df93f8b4",
   "metadata": {},
   "source": [
    "### Correction factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdc14ca-4311-439a-bfdb-e447ae9c4735",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_factor_p0 = geom.pol2(1, p0a, p0b, p0c) / geom.pol2(np.cos(np.deg2rad(dict_dchecks[run_number][\"pointing\"][\"zd\"])), p0a, p0b, p0c)\n",
    "corr_factor_p1 = geom.pol2(1, p1a, p1b, p1c) - geom.pol2(np.cos(np.deg2rad(dict_dchecks[run_number][\"pointing\"][\"zd\"])), p1a, p1b, p1c)\n",
    "\n",
    "# Zenith correction of the reference (putting the reference in the zenith of the determined subrun)\n",
    "corr_ref_p0 = ref_p0 / corr_factor_p0\n",
    "corr_ref_p1 = ref_p1 - corr_factor_p1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ff66a0-303c-4ee9-8461-33cb3b779791",
   "metadata": {},
   "source": [
    "## Function to perform all the scaling and then the reading\n",
    "#### - Zenith corrections and light yield\n",
    "\n",
    "Taking into account the logarithmic scale that is used, the light yield can be calculated with the following expression:\\\n",
    "\\\n",
    "$\\qquad\\qquad LY(A_{fit}, \\alpha; A_{ref}) = \\left(\\frac{A_{fit}}{A_{ref}}\\right)^{-\\frac{1}{1+\\alpha}}$\\\n",
    "\\\n",
    "The uncertainties have been calculated propagating the known ones.\\\n",
    "$\\sigma_{LY} = \\sqrt{\\sum_x\\left(\\frac{\\partial LY}{\\partial x}\\right)^2 \\sigma_x^2}$\\\n",
    "Where:\\\n",
    "$\\left(\\frac{\\partial LY}{\\partial A}\\right) =\\left(\\frac{A_{fit}}{A_{ref}}\\right)^{-\\frac{1}{1+\\alpha}} \\dfrac{-1}{\\left({\\alpha}+1\\right)A_{fit}}$\\\n",
    "$\\left(\\frac{\\partial LY}{\\partial \\alpha}\\right) = \\ln \\left(\\frac{A_{fit}}{A_{ref}}\\right) \\left(\\frac{A_{fit}}{A_{ref}}\\right)^{-\\frac{1}{1+\\alpha}} \\frac{1}{(1+\\alpha)^2}$\\\n",
    "\\\n",
    "So the final error will be:\n",
    "\\\n",
    "$\\sigma_{LY} = \\left(\\frac{A_{fit}}{A_{ref}}\\right)^{-\\frac{1}{1+\\alpha}} \\frac{1}{1+\\alpha}\\sqrt{A_{fit}^{-2}\\sigma_{A}^2+\\ln\\left(\\frac{A_{fit}}{A_{ref}}\\right)^2\\frac{\\sigma_{\\alpha}^2}{(1+\\alpha)^2}}$\n",
    "\n",
    "#### - Defining scaling\n",
    "In order to sample we will set two scalings for the conversion.\n",
    "\n",
    "* $1/LY$ The ideal scaling needed. The case where no new pixels emerge on the cleaning (unrealistic).\n",
    "\n",
    "And the error is calculated now as:\n",
    "\\\n",
    "$\\sigma_{s} = \\frac{1}{LY^4}\\sigma_{LY}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c93261-d9f7-427c-8c6c-bd53930d0ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_scaling(iteration_step, dict_results):\n",
    "    \"\"\"\n",
    "    A function to perform scaling and evaluating the results. Returning everything in a updated dictionary\n",
    "\n",
    "    Input:\n",
    "    - iteration_step: (str) \n",
    "        The iteration step you are in, that can be \"original\" for the original data, \"upper\" for the upper\n",
    "        limit on the scale factor, \"linear\" for the linear intepolation factor and \"final\" for the final scaling \n",
    "        and results.\n",
    "    - dict_results: (dict)\n",
    "        Dictionary with the results of the before step.\n",
    "    \"\"\"\n",
    "\n",
    "    # Creating a arrray of subruns looking at the datachecks and also extracting the run number\n",
    "    run_number  = dict_results[\"run\"]\n",
    "    sruns_array = np.sort([int(f[-7:-3]) for f in dict_dchecks[run_number][\"dl1a\"][\"srunwise\"]])\n",
    "    \n",
    "    # Empty arrays to store the fit information\n",
    "    data_p0, data_delta_p0  = [], []\n",
    "    data_p1, data_delta_p1  = [], []\n",
    "    data_chi2, data_pvalue  = [], []\n",
    "    \n",
    "    # Processing subrun by subrun\n",
    "    for srun in srun_numbers:    \n",
    "\n",
    "        input_fname = dict_dchecks[run_number][\"dl1a\"][\"srunwise\"][srun]   # Input dl1a        \n",
    "        data_scale_factor = dict_results[\"scaled\"][iteration_step][srun]   # Reading the scaling factor\n",
    "\n",
    "        # Here we do different things depending on the iteration step\n",
    "        # If is the first one i.e. == \"original\"\n",
    "        # We do not run lstchain_dl1ab because the data is already scaled\n",
    "        if iteration_step == \"original\":\n",
    "            data_output_fname = input_fname\n",
    "\n",
    "        # If is the second or third: \"upper\" or \"linear\"\n",
    "        # We perform lstchain_dl1ab but over a subset of the data only to keep it shorter\n",
    "        elif iteration_step in [\"upper\", \"linear\"]:\n",
    "\n",
    "            # Temporal dl1 file that will be overwritten in the next iteration / subrun\n",
    "            data_output_fname = root_objects + f\"tmp_dl1_srunwise_{iteration_step}_scaled.h5\" \n",
    "\n",
    "            logger.info(f\"\\nProcessing subrun {srun}\")\n",
    "            logger.info(f\"Running lstchain_dl1ab... scale: {data_scale_factor:.2f}\")\n",
    "\n",
    "            # If scale is greater than 1 we select a range lower than the upper one\n",
    "            # otherwise we select a range higher than the upper one\n",
    "            if data_scale_factor > 1:\n",
    "                dl1_selected_range = f\"{limits_intensity_extended:.2f},{limits_intensity[1]:.2f}\"\n",
    "            else:\n",
    "                dl1_selected_range = f\"{limits_intensity[0]:.2f},inf\"\n",
    "                \n",
    "            # # If the file already exists we delete it\n",
    "            # if os.path.exists(data_output_fname):\n",
    "            #     os.remove(data_output_fname)\n",
    "        \n",
    "            # !lstchain_dl1ab \\\n",
    "            # --input-file $input_fname \\\n",
    "            # --output-file $data_output_fname \\\n",
    "            # --config $config_file \\\n",
    "            # --no-image \\\n",
    "            # --light-scaling $data_scale_factor \\\n",
    "            # --intensity-range $dl1_selected_range\n",
    "\n",
    "        # If is the last step i.e. \"final\"\n",
    "        # The lstchain_dl1ab script is run over all thedataset to generate the final file\n",
    "        elif iteration_step == \"final\":\n",
    "\n",
    "            data_output_fname = root_data + f\"dl1_scaled/{run_number:05}/\" + os.path.basename(dict_dchecks[run_number][\"dl1a\"][\"srunwise\"][srun])\n",
    "            logger.info(f\"\\nProcessing subrun {srun}\")\n",
    "            logger.info(f\"Running lstchain_dl1ab... scale: {data_scale_factor:.2f}\")\n",
    "                \n",
    "            # # If the file already exists we delete it\n",
    "            # if os.path.exists(data_output_fname):\n",
    "            #     os.remove(data_output_fname)\n",
    "        \n",
    "            # !lstchain_dl1ab \\\n",
    "            # --input-file $input_fname \\\n",
    "            # --output-file $data_output_fname \\\n",
    "            # --config $config_file \\\n",
    "            # --no-image \\\n",
    "            # --light-scaling $data_scale_factor\n",
    "\n",
    "            # We store this info also in the dictionary in the final case\n",
    "            dict_results[\"filenames\"][srun] = data_output_fname\n",
    "    \n",
    "        # Reading the file\n",
    "        # table_data = tables.open_file(data_output_fname)\n",
    "        # data_counts_intensity, _ = np.histogram(\n",
    "        #     table_data.root.dl1.event.telescope.parameters.LST_LSTCam.col(\"intensity\"), \n",
    "        #     bins=dcheck_intensity_binning\n",
    "        # )\n",
    "        # table_data.close()\n",
    "        data_counts_intensity = dcheck_hist_intensities[0] + np.random.rand(100) * 100\n",
    "        \n",
    "        # Calculating the non binning dependent transformation\n",
    "        effective_time_srun = dict_dchecks[run_number][\"time\"][\"srunwise\"][\"telapsed\"][srun]\n",
    "        data_rates       = np.array(data_counts_intensity) / effective_time_srun / dcheck_intensity_binning_widths\n",
    "        data_delta_rates = np.sqrt(data_counts_intensity)  / effective_time_srun / dcheck_intensity_binning_widths\n",
    "    \n",
    "        # Displacing the X-coordinates to the center of the fit, in order to decorrelate the fit\n",
    "        x_fit    = dcheck_intensity_binning_centers[mask_dcheck_bins_fit] / ref_intensity\n",
    "        y_fit    = data_rates[mask_dcheck_bins_fit]\n",
    "        yerr_fit = data_delta_rates[mask_dcheck_bins_fit]\n",
    "    \n",
    "        # Performing the fit\n",
    "        params, pcov, info, _, _ = curve_fit(\n",
    "            f     = geom.powerlaw,\n",
    "            xdata = x_fit,\n",
    "            ydata = y_fit,\n",
    "            sigma = yerr_fit,\n",
    "            p0    = [ref_p0, ref_p1],\n",
    "            full_output = True,\n",
    "        )\n",
    "    \n",
    "        srun_p0, srun_p1  = params\n",
    "        srun_delta_p0 = np.sqrt(pcov[0, 0])\n",
    "        srun_delta_p1 = np.sqrt(pcov[1, 1])\n",
    "        srun_chi2     = np.sum(info[\"fvec\"] ** 2)\n",
    "        srun_pvalue   = 1 - chi2.cdf(srun_chi2, sum(mask_dcheck_bins_fit))\n",
    "        \n",
    "        dict_results[\"chi2\"][iteration_step][srun]   = srun_chi2\n",
    "        dict_results[\"pvalue\"][iteration_step][srun] = srun_pvalue\n",
    "        dict_results[\"scaled\"][iteration_step][srun] = data_scale_factor\n",
    "    \n",
    "        data_p0.append(srun_p0)\n",
    "        data_p1.append(srun_p1)\n",
    "        data_delta_p0.append(srun_delta_p0)\n",
    "        data_delta_p1.append(srun_delta_p1)\n",
    "        data_chi2.append(srun_chi2)\n",
    "        data_pvalue.append(srun_pvalue)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    data_p0       = np.array(data_p0)\n",
    "    data_p1       = np.array(data_p1)\n",
    "    data_delta_p0 = np.array(data_delta_p0)\n",
    "    data_delta_p1 = np.array(data_delta_p1)\n",
    "    data_chi2     = np.array(data_chi2)\n",
    "    data_pvalue   = np.array(data_pvalue)\n",
    "    \n",
    "    # Zenith corrections to the parameters\n",
    "    data_corr_p0 = data_p0 * corr_factor_p0\n",
    "    data_corr_p1 = data_p1 + corr_factor_p1\n",
    "    \n",
    "    data_corr_delta_p0 = data_delta_p0 * corr_factor_p0\n",
    "    data_corr_delta_p1 = data_delta_p1\n",
    "    \n",
    "    # Calculating the needed light yield  \n",
    "    data_light_yield, data_delta_light_yield = geom.calc_light_yield(\n",
    "        p0_fit = data_corr_p0,\n",
    "        p1_fit = data_corr_p1, \n",
    "        sigma_p0_fit = data_corr_delta_p0, \n",
    "        sigma_p1_fit = data_corr_delta_p1, \n",
    "        p0_ref = ref_p0,\n",
    "    )\n",
    "    \n",
    "    # Scalings to apply\n",
    "    data_scaling       = 1 / data_light_yield\n",
    "    data_delta_scaling = 1 / data_light_yield ** 4 * data_delta_light_yield\n",
    "    \n",
    "    # In the case of the last subrun we use the last subrun statistics, because the lack of statistics\n",
    "    if last_subrun_statistics < statistics_threshold_last_srun:\n",
    "        data_scaling[-1]       = data_scaling[-2]\n",
    "        data_delta_scaling[-1] = data_delta_scaling[-2]\n",
    "    \n",
    "        logger.warning(f\"For last subrun using before subrun because of low statistics ({last_subrun_statistics}evs)\\n\")\n",
    "    \n",
    "    # The scaling in percentage\n",
    "    data_scaling_percent       = (data_scaling - 1) * 100\n",
    "    data_delta_scaling_percent = data_delta_scaling * 100\n",
    "    \n",
    "    # Adding to dictionary\n",
    "    for i, srun in enumerate(srun_numbers):\n",
    "        dict_results[\"p0\"][iteration_step][srun]       = data_corr_p0[i]\n",
    "        dict_results[\"delta_p0\"][iteration_step][srun] = data_corr_delta_p0[i]\n",
    "        dict_results[\"p1\"][iteration_step][srun]       = data_corr_p1[i]\n",
    "        dict_results[\"delta_p1\"][iteration_step][srun] = data_corr_delta_p1[i]\n",
    "        \n",
    "        dict_results[\"light_yield\"][iteration_step][srun]       = data_light_yield[i]\n",
    "        dict_results[\"delta_light_yield\"][iteration_step][srun] = data_delta_light_yield[i]\n",
    "        dict_results[\"scaling\"][iteration_step][srun]           = data_scaling[i]\n",
    "        dict_results[\"delta_scaling\"][iteration_step][srun]     = data_delta_scaling[i]\n",
    "        dict_results[\"scaling_percent\"][iteration_step][srun]   = data_scaling_percent[i]\n",
    "        dict_results[\"delta_scaling_percent\"][iteration_step][srun] = data_delta_scaling_percent[i]\n",
    "\n",
    "    return dict_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba6c1f7-b50a-4ec8-bb56-9424395dadd4",
   "metadata": {},
   "source": [
    "### Then scale the full datasets by the interpolated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2872fa71-5cfe-4dcf-a59f-a56d57326cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dict_fname = root_final_results + f\"results_job_{run_number}.pkl\"\n",
    "\n",
    "# Reading the object\n",
    "with open(dict_fname, 'rb') as f:\n",
    "    dict_results = pickle.load(f)\n",
    "\n",
    "dict_results = find_scaling(iteration_step=\"final\", dict_results=dict_results)\n",
    "\n",
    "# Saving the object again\n",
    "with open(dict_fname, 'wb') as f:\n",
    "    pickle.dump(dict_results, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1458ffb",
   "metadata": {},
   "source": [
    "## Some plots for checking the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de79b6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_corr_p0 = np.array([dict_results[\"p0\"][\"original\"][srun] for srun in sruns_array])\n",
    "original_corr_p1 = np.array([dict_results[\"p1\"][\"original\"][srun] for srun in sruns_array])\n",
    "original_corr_delta_p0 = np.array([dict_results[\"delta_p0\"][\"original\"][srun] for srun in sruns_array])\n",
    "original_corr_delta_p1 = np.array([dict_results[\"delta_p1\"][\"original\"][srun] for srun in sruns_array])\n",
    "original_scaling_percent = np.array([dict_results[\"scaling_percent\"][\"original\"][srun] for srun in sruns_array])\n",
    "original_delta_scaling_percent = np.array([dict_results[\"delta_scaling_percent\"][\"original\"][srun] for srun in sruns_array])\n",
    "final_corr_p0 = np.array([dict_results[\"p0\"][\"final\"][srun] for srun in sruns_array])\n",
    "final_corr_p1 = np.array([dict_results[\"p1\"][\"final\"][srun] for srun in sruns_array])\n",
    "final_corr_delta_p0 = np.array([dict_results[\"delta_p0\"][\"final\"][srun] for srun in sruns_array])\n",
    "final_corr_delta_p1 = np.array([dict_results[\"delta_p1\"][\"final\"][srun] for srun in sruns_array])\n",
    "final_scaling_percent = np.array([dict_results[\"scaling_percent\"][\"final\"][srun] for srun in sruns_array])\n",
    "final_delta_scaling_percent = np.array([dict_results[\"delta_scaling_percent\"][\"final\"][srun] for srun in sruns_array])\n",
    "\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(10, 7.8), sharex=True)\n",
    "\n",
    "ax1.plot(sruns_array, original_corr_p0, color=\"k\", alpha=0.7, label=\"Original\")\n",
    "ax1.fill_between(sruns_array, \n",
    "                 original_corr_p0 - original_corr_delta_p0, \n",
    "                 original_corr_p0 + original_corr_delta_p0,\n",
    "                 color=\"k\", alpha=0.2, ls=\"\")\n",
    "ax1.plot(sruns_array, final_corr_p0, color=\"mediumblue\", label=\"Scaled\")\n",
    "ax1.fill_between(sruns_array,\n",
    "                 final_corr_p0 - final_corr_delta_p0,\n",
    "                 final_corr_p0 + final_corr_delta_p0,\n",
    "                 color=\"mediumblue\", alpha=0.2, ls=\"\")\n",
    "\n",
    "ax2.plot(sruns_array, original_corr_p1, color=\"k\", alpha=0.7, label=\"Original\")\n",
    "ax2.fill_between(sruns_array, \n",
    "                 original_corr_p1 - original_corr_delta_p1, \n",
    "                 original_corr_p1 + original_corr_delta_p1,\n",
    "                 color=\"k\", alpha=0.2, ls=\"\")\n",
    "ax2.plot(sruns_array, final_corr_p1, color=\"crimson\", label=\"Scaled\")\n",
    "ax2.fill_between(sruns_array,\n",
    "                 final_corr_p1 - final_corr_delta_p1,\n",
    "                 final_corr_p1 + final_corr_delta_p1,\n",
    "                 color=\"crimson\", alpha=0.2, ls=\"\")\n",
    "\n",
    "ax3.plot(sruns_array, original_scaling_percent, color=\"k\", alpha=0.7, label=\"Original\")\n",
    "ax3.fill_between(sruns_array, \n",
    "                 original_scaling_percent - original_delta_scaling_percent, \n",
    "                 original_scaling_percent + original_delta_scaling_percent,\n",
    "                 color=\"k\", alpha=0.2, ls=\"\")\n",
    "ax3.plot(sruns_array, final_scaling_percent, color=\"g\", label=\"Scaled\")\n",
    "ax3.fill_between(sruns_array,\n",
    "                 final_scaling_percent - final_delta_scaling_percent,\n",
    "                 final_scaling_percent + final_delta_scaling_percent,\n",
    "                 color=\"g\", alpha=0.2, ls=\"\")\n",
    "\n",
    "ax4.plot(sruns_array, final_scaling_percent, color=\"g\", label=\"Scaled\")\n",
    "ax4.fill_between(sruns_array,\n",
    "                 final_scaling_percent - final_delta_scaling_percent,\n",
    "                 final_scaling_percent + final_delta_scaling_percent,\n",
    "                 color=\"g\", alpha=0.2, ls=\"\")\n",
    "\n",
    "\n",
    "ax1.axhline(ref_p0, color=\"k\", ls=\"--\", label=\"Reference\")\n",
    "ax2.axhline(ref_p1, color=\"k\", ls=\"--\")\n",
    "ax3.axhline(0, color=\"k\", ls=\"--\")\n",
    "ax4.axhline(0, color=\"k\", ls=\"--\")\n",
    "\n",
    "ax1.set_ylabel(\"Normalization\")\n",
    "ax2.set_ylabel(\"Power index\")\n",
    "ax3.set_ylabel(\"Scaling % needed\")\n",
    "ax4.set_ylabel(\"Scaling % needed\")\n",
    "ax4.set_xlabel(\"# Subrun\")\n",
    "\n",
    "# ax1.set_ylim(0.9, 2)\n",
    "# ax2.set_ylim(-2.8, -1.5)\n",
    "# ax3.set_ylim(-10, 84)\n",
    "# ax4.set_ylim(-5, 5)\n",
    "ax1.legend(frameon=False, ncols=3)\n",
    "ax3.legend(frameon=False)\n",
    "ax4.legend(frameon=False)\n",
    "\n",
    "# plt.savefig(f\"plots/total.png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
