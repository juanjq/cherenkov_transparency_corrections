{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e70776f7-e2e1-4057-b7ef-878b2be0556d",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b3b4638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from datetime import datetime\n",
    "import pickle, json, sys, os, glob\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import chi2\n",
    "from scipy import optimize\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from traitlets.config.loader import Config\n",
    "from astropy.coordinates     import SkyCoord\n",
    "from lstchain.io.config      import get_standard_config\n",
    "from ctapipe.io              import read_table\n",
    "import tables\n",
    "\n",
    "# Other auxiliar scripts\n",
    "sys.path.insert(0, os.getcwd() + \"/../scripts/\")\n",
    "import auxiliar as aux\n",
    "import geometry as geom\n",
    "import lstpipeline\n",
    "import plotting\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cff6f04",
   "metadata": {},
   "source": [
    "# Some configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d56ff173",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Source name in order to just complete the results file, and\n",
    "in order to improve run organization.\"\"\"\n",
    "source_name = \"crab\"\n",
    "\n",
    "\"\"\" The run number that we are interested in apply the corrections.\n",
    "The process is done run-wise, so the input will be an individual run.\"\"\"\n",
    "# run_number  = 6172\n",
    "# srun_number = 0\n",
    "# input_str = \"6172_0_1_25\"\n",
    "# run_number   = int(input_str.split(\"_\")[0])\n",
    "# srun_numbers = [int(s) for s in input_str.split(\"_\")[1:]]\n",
    "\n",
    "\"\"\" Fit parameters\n",
    "Chosen limits in intensity (p.e.) for applying the fit i.e. the\n",
    "power law will be fitted only with the points within this range.\"\"\"\n",
    "limits_intensity = [316, 562]\n",
    "\"\"\" For the positive scaling cases (most of them), we need to have a lower \n",
    "limit in intensity. Thi slimit is used for the subset of events that are \n",
    "scaled just to find which is the scaling value. We use a very low limit by\n",
    "default 60 p.e. compared to the lower limit of the fit 316 p.e. because in \n",
    "the worst cases we will have a very non-linear scaling that will displace \n",
    "significantly the events intensities.\"\"\"\n",
    "limits_intensity_extended = 60\n",
    "\n",
    "\"\"\" Power law parameters for the reference\n",
    "All these parameters are taken from a common analysis of the full dataset\n",
    "Where the period of end of 2022 and start 2023 is taken as reference for good \n",
    "runs. Then we take as reference the mean power law parameters in that period.\n",
    "p0 is the normalization factor and p1 is the slope.\"\"\"\n",
    "ref_p0 =  1.74 \n",
    "ref_p1 = -2.23\n",
    "\n",
    "\"\"\" Threshold in statistics for the last subrun\n",
    "The limit in number of events after cleaning that we need to consider the last\n",
    "subrun has enough statistics to perform the analysis over it. Otherwise the \n",
    "values of the scaling that will be applied to this last rubrun are the same \n",
    "that are applied to the last last subrun.\"\"\"\n",
    "statistics_threshold_last_srun = 15000\n",
    "\n",
    "\"\"\" Parameters for the empyrical fits for Zenith Distance corrections\n",
    "Are simply two 2 degree polynomials for each variable of the power law.\"\"\"\n",
    "p0a, p0b, p0c = -0.44751321, 3.62502037, -1.43611437\n",
    "p1a, p1b, p1c = -2.89253919, 0.99443581, -0.34013068\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ea7368",
   "metadata": {},
   "source": [
    "# Paths to data and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66854bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root path of this script\n",
    "root = os.getcwd() + \"/\"\n",
    "# Path to store the configuration file we are going to use\n",
    "config_file = root + \"config/standard_config.json\"\n",
    "# Path to store objects\n",
    "root_objects = root + f\"objects/\"\n",
    "# Data main directory\n",
    "root_data = root + f\"../../data/cherenkov_transparency_corrections/{source_name}/\"\n",
    "# Directory for the results of the fit of each run\n",
    "root_results = root_objects + \"results_fits/\"\n",
    "root_final_results = root_objects + \"final_results_fits/\"\n",
    "\n",
    "# STANDARD paths ---------\n",
    "root_dl1 = \"/fefs/aswg/data/real/DL1/*/v0.*/tailcut84/\"\n",
    "root_rfs = \"/fefs/aswg/data/models/AllSky/20230901_v0.10.4_allsky_base_prod/\"\n",
    "root_mcs = \"/fefs/aswg/data/mc/DL2/AllSky/20230901_v0.10.4_allsky_base_prod/TestingDataset/\"\n",
    "\n",
    "# Create the paths that do not exist\n",
    "for path in [root_final_results]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(os.path.join(path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9097b0-d17b-4cee-bfa7-7eef02a43982",
   "metadata": {},
   "source": [
    "### Opening and merging the dictionaries stored in past jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d78b4c0f-acbd-4740-ae6c-75e0a5d1789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_files = np.sort(glob.glob(root_results + \"*.pkl\"))\n",
    "\n",
    "total_runs = []\n",
    "dictionaries = []\n",
    "for file in dict_files:\n",
    "\n",
    "    fname = file.split(\"/\")[-1]\n",
    "    run   = int(fname.split(\"_\")[2])\n",
    "    sruns = [int(sr) for sr in fname.split(\".\")[0].split(\"_\")[3:]]\n",
    "\n",
    "    total_runs.append(run)\n",
    "\n",
    "    # Also we store the dictionaries\n",
    "    # Reading the object\n",
    "    with open(file, 'rb') as f:\n",
    "        tmp_dict = pickle.load(f)\n",
    "    dictionaries.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "096c9b61-c2f7-4d2d-9e99-6cdea446d9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only non-repeated runs\n",
    "total_runs = np.unique(total_runs)\n",
    "\n",
    "dict_runs = {}\n",
    "for run in total_runs:\n",
    "    tmp = { \n",
    "        \"run\": run, \"filenames\": {}, \"statistics\": {},\n",
    "        \"scaled\" :           {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"p0\":                {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"delta_p0\":          {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"p1\":                {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"delta_p1\":          {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"chi2\":              {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"pvalue\":            {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"light_yield\":       {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"delta_light_yield\": {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"scaling\":           {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"delta_scaling\":     {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"scaling_percent\":       {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"delta_scaling_percent\": {\"original\": {}, \"upper\": {}, \"linear\": {}, \"final\": {}},\n",
    "        \"final_scaling\": {}, \"final_scaling_interpolated\": {}, \"interpolation\" : {},\n",
    "    }\n",
    "    dict_runs[run] = tmp\n",
    "\n",
    "def merge_dicts(dict1, dict2):\n",
    "    \"\"\"\n",
    "    Recursively merge two dictionaries with nested structures.\n",
    "\n",
    "    Args:\n",
    "    - dict1: First dictionary\n",
    "    - dict2: Second dictionary\n",
    "\n",
    "    Returns:\n",
    "    - Merged dictionary\n",
    "    \"\"\"\n",
    "    merged = dict1.copy()\n",
    "\n",
    "    for key, value in dict2.items():\n",
    "        if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):\n",
    "            # If both values are dictionaries, recursively merge them\n",
    "            merged[key] = merge_dicts(merged[key], value)\n",
    "        else:\n",
    "            # Otherwise, just update or add the key-value pair\n",
    "            merged[key] = value\n",
    "    return merged\n",
    "    \n",
    "# Now we fill this dicts with the info we have\n",
    "for d in dictionaries:\n",
    "    run = d[\"run\"]\n",
    "\n",
    "    dict_runs[run] = merge_dicts(dict_runs[run], d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6a07a95-539c-4bb3-ab1b-40e4a6798f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last subrun check\n",
    "for run in dict_runs.keys():\n",
    "    d = dict_runs[run]\n",
    "    stats = dict_runs[run][\"statistics\"]\n",
    "    last_srun = max(stats.keys())\n",
    "    last_last_srun = last_srun - 1\n",
    "\n",
    "    last_srun_stats = stats[last_srun]\n",
    "\n",
    "    # If we have less events we modify the values\n",
    "    if last_srun_stats < statistics_threshold_last_srun:\n",
    "\n",
    "        dict_runs[run][\"final_scaling\"][last_srun] = dict_runs[run][\"final_scaling\"][last_last_srun]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b32e12",
   "metadata": {},
   "source": [
    "# Finding the files that interest us\n",
    "#### Extracting dl1 files and dl1 datachecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e95eb908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding dl1  data to dictionary...\n",
      "...Finished adding dl1 data to dictionary\n",
      "\n",
      "Adding dl1 datacheck data to dictionary...\n",
      "...Finished adding dl1 data to dictionary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.5 s, sys: 6.93 s, total: 19.5 s\n",
      "Wall time: 31.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Getting coordinates of source\n",
    "source_coords = SkyCoord.from_name(source_name)\n",
    "\n",
    "dict_source = {\n",
    "    \"name\"   : source_name,\n",
    "    \"coords\" : source_coords,\n",
    "    \"ra\"     : source_coords.ra.deg  * u.deg, # ra in degrees\n",
    "    \"dec\"    : source_coords.dec.deg * u.deg, # dec in degrees\n",
    "}\n",
    "\n",
    "# We create a empty dictionary to store all the information needed inside\n",
    "dict_dchecks = {}\n",
    "for run in total_runs:\n",
    "    dict_dchecks[run] = {\n",
    "        \"run_num\" : run,\n",
    "    }\n",
    "\n",
    "dict_dchecks = lstpipeline.add_dl1_paths_to_dict(dict_dchecks, root_dl1)\n",
    "dict_dchecks = lstpipeline.add_dl1_paths_to_dict(dict_dchecks, root_dl1, dchecking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac5fda0",
   "metadata": {},
   "source": [
    "#### Then we read the observations information and also the selected nodes for MC and RFs and we add it to the DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91f1e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for run_number in total_runs:\n",
    "    tab_dcheck_run = read_table(dict_dchecks[run_number][\"dchecks\"][\"runwise\"], \"/dl1datacheck/cosmics\")\n",
    "    \n",
    "    # reading the variables\n",
    "    dcheck_zd = 90 - np.rad2deg(np.array(tab_dcheck_run[\"mean_alt_tel\"]))\n",
    "    dcheck_az = np.rad2deg(np.array(tab_dcheck_run[\"mean_az_tel\"]))\n",
    "    dcheck_tstart = tab_dcheck_run[\"dragon_time\"][0][0]\n",
    "    dcheck_telapsed = np.array(tab_dcheck_run[\"elapsed_time\"])\n",
    "\n",
    "    dict_dchecks[run_number][\"time\"] = {\n",
    "        \"tstart\"   : dcheck_tstart,            # datetime object\n",
    "        \"telapsed\" : np.sum(dcheck_telapsed),  # s\n",
    "        \"srunwise\" : {\n",
    "            \"telapsed\" : dcheck_telapsed,      # s      \n",
    "        },\n",
    "    }\n",
    "    dict_dchecks[run_number][\"pointing\"] = {\n",
    "        \"zd\" : np.mean(dcheck_zd),  # deg\n",
    "        \"az\" : np.mean(dcheck_az),  # deg\n",
    "        \"srunwise\" : {\n",
    "            \"zd\" : dcheck_zd, # deg\n",
    "            \"az\" : dcheck_az, # deg\n",
    "        },\n",
    "    }\n",
    "    \n",
    "# then we also select the RFs and MC files looking at the nodes available\n",
    "dict_dchecks, dict_nodes = lstpipeline.add_mc_and_rfs_nodes(dict_dchecks, root_rfs, root_mcs, dict_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1d0e4d-a713-4a6d-b704-7c623de3c559",
   "metadata": {},
   "source": [
    "### Then we interpolate the lineal scalings with a linear behaviour trough the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf97dc25-4ea6-4495-9603-428e6718e73f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (70,) (2,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m y_fit \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([dict_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_scaling\u001b[39m\u001b[38;5;124m\"\u001b[39m][srun] \u001b[38;5;28;01mfor\u001b[39;00m srun \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39msort(\u001b[38;5;28mlist\u001b[39m(dict_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_scaling\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys()))])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Performing the fit\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m params, pcov, info, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcurve_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgeom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstraight_line\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_fit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mydata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_fit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp0\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m intercept       \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     18\u001b[0m slope           \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/scipy/optimize/_minpack_py.py:963\u001b[0m, in \u001b[0;36mcurve_fit\u001b[0;34m(f, xdata, ydata, p0, sigma, absolute_sigma, check_finite, bounds, method, jac, full_output, nan_policy, **kwargs)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ydata\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m n \u001b[38;5;241m>\u001b[39m ydata\u001b[38;5;241m.\u001b[39msize:\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of func parameters=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    962\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m exceed the number of data points=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mydata\u001b[38;5;241m.\u001b[39msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 963\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mleastsq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDfun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m popt, pcov, infodict, errmsg, ier \u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m    965\u001b[0m ysize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(infodict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfvec\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/scipy/optimize/_minpack_py.py:415\u001b[0m, in \u001b[0;36mleastsq\u001b[0;34m(func, x0, args, Dfun, full_output, col_deriv, ftol, xtol, gtol, maxfev, epsfcn, factor, diag)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    414\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[0;32m--> 415\u001b[0m shape, dtype \u001b[38;5;241m=\u001b[39m \u001b[43m_check_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleastsq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfunc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m m \u001b[38;5;241m=\u001b[39m shape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m>\u001b[39m m:\n",
      "File \u001b[0;32m/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/scipy/optimize/_minpack_py.py:25\u001b[0m, in \u001b[0;36m_check_func\u001b[0;34m(checker, argname, thefunc, x0, args, numinputs, output_shape)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_func\u001b[39m(checker, argname, thefunc, x0, args, numinputs,\n\u001b[1;32m     24\u001b[0m                 output_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 25\u001b[0m     res \u001b[38;5;241m=\u001b[39m atleast_1d(\u001b[43mthefunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mnuminputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (output_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (shape(res) \u001b[38;5;241m!=\u001b[39m output_shape):\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (output_shape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[0;32m/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/scipy/optimize/_minpack_py.py:507\u001b[0m, in \u001b[0;36m_lightweight_memoizer.<locals>._memoized_func\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(_memoized_func\u001b[38;5;241m.\u001b[39mlast_params \u001b[38;5;241m==\u001b[39m params):\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _memoized_func\u001b[38;5;241m.\u001b[39mlast_val\n\u001b[0;32m--> 507\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _memoized_func\u001b[38;5;241m.\u001b[39mlast_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     _memoized_func\u001b[38;5;241m.\u001b[39mlast_params \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(params)\n",
      "File \u001b[0;32m/fefs/aswg/workspace/juan.jimenez/.conda/envs/lst-dev2/lib/python3.11/site-packages/scipy/optimize/_minpack_py.py:523\u001b[0m, in \u001b[0;36m_wrap_func.<locals>.func_wrapped\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc_wrapped\u001b[39m(params):\n\u001b[0;32m--> 523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mydata\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (70,) (2,) "
     ]
    }
   ],
   "source": [
    "for run_number in total_runs:\n",
    "\n",
    "    dict_results = dict_runs[run_number]\n",
    "\n",
    "    x_fit = np.cumsum(dict_dchecks[run_number][\"time\"][\"srunwise\"][\"telapsed\"])\n",
    "    y_fit = np.array([dict_results[\"final_scaling\"][srun] for srun in np.sort(list(dict_results[\"final_scaling\"].keys()))])\n",
    "\n",
    "    # Performing the fit\n",
    "    params, pcov, info, _, _ = curve_fit(\n",
    "        f     = geom.straight_line,\n",
    "        xdata = x_fit,\n",
    "        ydata = y_fit,\n",
    "        p0    = [1, 0],\n",
    "        full_output = True,\n",
    "    )\n",
    "        \n",
    "    intercept       = params[0]\n",
    "    slope           = params[1]\n",
    "    delta_intercept = np.sqrt(pcov[0, 0])\n",
    "    delta_slope     = np.sqrt(pcov[1, 1])\n",
    "    _chi2           = np.sum(info['fvec'] ** 2)\n",
    "    pvalue          = 1 - chi2.cdf(_chi2, len(x_fit))\n",
    "    \n",
    "    dict_results[\"interpolation\"] = {\n",
    "        \"chi2\" : _chi2,      \n",
    "        \"p_value\" : pvalue,         \n",
    "        \"slope\": slope,      \n",
    "        \"delta_slope\" : delta_slope,     \n",
    "        \"intercept\" : intercept, \n",
    "        \"delta_intercept\" : delta_intercept,\n",
    "    }\n",
    "    \n",
    "    #############################################\n",
    "    #############################################\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 2))\n",
    "    \n",
    "    ax.plot(x_fit/60, ((intercept + x_fit * slope) - 1) * 100, color=\"k\", ls=\"--\", zorder=10, label=\"Interpolation\")\n",
    "    \n",
    "    ax.plot(x_fit/60, (y_fit - 1) * 100, 'r', label='Data')\n",
    "    \n",
    "    ax.set_xlabel(\"Time elapsed [min]\")\n",
    "    ax.set_ylabel(\"Scaling factor [%]\")\n",
    "    ax.legend(ncols=2, frameon=False)\n",
    "    \n",
    "    plt.show()\n",
    "    #############################################\n",
    "    #############################################\n",
    "    \n",
    "    # Setting a interpolated scaling factor\n",
    "    for srun in dict_results[\"final_scaling\"].keys():\n",
    "        \n",
    "        scaling_interpolated = intercept + slope * x_fit[srun]\n",
    "        \n",
    "        dict_results[\"final_scaling_interpolated\"][srun] = scaling_interpolated\n",
    "        dict_results[\"scaled\"][\"final\"][srun]            = scaling_interpolated\n",
    "\n",
    "    dict_fname = root_final_results + f\"results_job_{run_number}.pkl\"\n",
    "    \n",
    "    # Saving the objects\n",
    "    with open(dict_fname, 'wb') as f:\n",
    "        pickle.dump(dict_results, f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
