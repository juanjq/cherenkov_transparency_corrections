{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c06d147-c1a4-4b4f-8395-2b18ed5f4b36",
   "metadata": {},
   "source": [
    "### Import needed packages and scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa942be9-ac39-4184-bd2e-4ff583ab57b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pickle, sys, os, json\n",
    "import astropy.units as u\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "from astropy.coordinates import SkyCoord\n",
    "from matplotlib.dates    import DayLocator, MonthLocator, DateFormatter\n",
    "from regions             import PointSkyRegion\n",
    "from astropy.time        import Time\n",
    "from scipy.stats         import chi2\n",
    "\n",
    "from gammapy.modeling.models import create_crab_spectral_model, SkyModel, LogParabolaSpectralModel\n",
    "from gammapy.estimators      import FluxPointsEstimator, LightCurveEstimator, FluxPoints\n",
    "from gammapy.modeling        import Fit\n",
    "from gammapy.datasets        import Datasets, SpectrumDataset\n",
    "from gammapy.makers          import SpectrumDatasetMaker, WobbleRegionsFinder, ReflectedRegionsBackgroundMaker, SafeMaskMaker\n",
    "from gammapy.maps            import MapAxis, RegionGeom, Map, TimeMapAxis\n",
    "from gammapy.data            import DataStore\n",
    "\n",
    "# import scripts\n",
    "sys.path.insert(0, os.getcwd() + \"/../scripts/\")\n",
    "import auxiliar as aux\n",
    "import plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd29d71c",
   "metadata": {},
   "source": [
    "### Paths to data and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1a0580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root path of this script\n",
    "root = os.getcwd() + \"/\"\n",
    "# Objects directory\n",
    "root_objects = root + \"objects/\"\n",
    "# Data directory\n",
    "root_data = root + \"../../data/\"\n",
    "\n",
    "# Gammapy configuration file\n",
    "config_gammapy = root_objects + \"config_gammapy_analysis.json\"\n",
    "\n",
    "# Path of dl3 data\n",
    "dl3_dir_paper = \"/fefs/aswg/workspace/juan.jimenez/data/tests/crab/dl3_paper/\"\n",
    "#\"/fefs/aswg/workspace/daniel.morcuende/data/real/DL3/Crab_performance/AllSkyMC_v0.9.9/intensity80/all_nodes/gh_eff_0.7_th_cont_0.7/\"\n",
    "dl3_dir   = \"/fefs/aswg/workspace/juan.jimenez/data/cherenkov_transparency_corrections/crab/dl3/\"\n",
    "#root_data + \"cherenkov_transparency_corrections/crab/dl3/\"\n",
    "dl3_dir_s = \"/fefs/aswg/workspace/juan.jimenez/data/cherenkov_transparency_corrections/crab/dl3_scaled/\"\n",
    "#root_data + \"cherenkov_transparency_corrections/crab/dl3_scaled/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a163d520-9469-4c5e-9c0b-c7d0f961153d",
   "metadata": {},
   "source": [
    "### Loading configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "028a1ca5-5eac-471e-85cd-cd16f29a954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the configuration from the gammapy configuration file we created\n",
    "with open(config_gammapy, \"r\") as json_file:\n",
    "    dict = json.load(json_file)\n",
    "\n",
    "# Saving the configuration in variables\n",
    "target_name   = dict[\"target_name\"]\n",
    "n_off_regions = dict[\"n_off_regions\"]\n",
    "_e_reco = dict[\"e_reco\"]\n",
    "_e_true = dict[\"e_true\"]\n",
    "\n",
    "e_reco_min, e_reco_max, e_reco_bin_p_dec = _e_reco[\"min\"], _e_reco[\"max\"], _e_reco[\"bins_p_dec\"]\n",
    "e_true_min, e_true_max, e_true_bin_p_dec = _e_true[\"min\"], _e_true[\"max\"], _e_true[\"bins_p_dec\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa354a1-2aa6-45ea-83dd-9dea4cdedec0",
   "metadata": {},
   "source": [
    "### Loading full datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2d77300-170c-462b-bf2c-7bffdec939bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total livetime of observations 18.93 h\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><i>ObservationTable length=5</i>\n",
       "<table id=\"table139816827395472\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>OBS_ID</th><th>DATE-OBS</th><th>TIME-OBS</th><th>DATE-END</th><th>TIME-END</th><th>RA_PNT</th><th>DEC_PNT</th><th>ZEN_PNT</th><th>ALT_PNT</th><th>AZ_PNT</th><th>RA_OBJ</th><th>DEC_OBJ</th><th>TSTART</th><th>TSTOP</th><th>ONTIME</th><th>TELAPSE</th><th>LIVETIME</th><th>DEADC</th><th>OBJECT</th><th>OBS_MODE</th><th>N_TELS</th><th>TELLIST</th><th>INSTRUME</th></tr></thead>\n",
       "<thead><tr><th></th><th></th><th></th><th></th><th></th><th>deg</th><th>deg</th><th>deg</th><th>deg</th><th>deg</th><th>deg</th><th>deg</th><th>s</th><th>s</th><th>s</th><th>s</th><th>s</th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead>\n",
       "<thead><tr><th>int64</th><th>bytes10</th><th>bytes12</th><th>bytes10</th><th>bytes12</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>bytes4</th><th>bytes8</th><th>int64</th><th>bytes5</th><th>bytes5</th></tr></thead>\n",
       "<tr><td>2914</td><td>2020-11-18</td><td>03:43:34.194</td><td>2020-11-18</td><td>04:02:52.571</td><td>83.28333315577058</td><td>21.78656761995211</td><td>14.827836028194994</td><td>75.172163971805</td><td>245.10365521538614</td><td>83.6287</td><td>22.0147</td><td>67319014.1938901</td><td>67320172.57093215</td><td>1158.378259897232</td><td>1158.37704205513</td><td>1110.7061580196348</td><td>0.9588458247810809</td><td>crab</td><td>POINTING</td><td>1</td><td>LST-1</td><td>LST-1</td></tr>\n",
       "<tr><td>2929</td><td>2020-11-19</td><td>01:41:13.090</td><td>2020-11-19</td><td>01:59:21.736</td><td>83.97361218928079</td><td>22.244784527308934</td><td>15.852691452043658</td><td>74.14730854795634</td><td>110.69510910121441</td><td>83.6287</td><td>22.0147</td><td>67398073.0903821</td><td>67399161.73551226</td><td>1088.6460654735565</td><td>1088.6451301574707</td><td>1006.0309340133695</td><td>0.9241120378051889</td><td>crab</td><td>POINTING</td><td>1</td><td>LST-1</td><td>LST-1</td></tr>\n",
       "<tr><td>2930</td><td>2020-11-19</td><td>02:02:55.113</td><td>2020-11-19</td><td>02:22:43.860</td><td>83.26877581309876</td><td>21.786545814732005</td><td>11.237297662536207</td><td>78.7627023374638</td><td>126.56517001542876</td><td>83.6287</td><td>22.0147</td><td>67399375.11288929</td><td>67400563.8598373</td><td>1187.369912147522</td><td>1188.746948003769</td><td>1106.9165247250533</td><td>0.9322423563209904</td><td>crab</td><td>POINTING</td><td>1</td><td>LST-1</td><td>LST-1</td></tr>\n",
       "<tr><td>2931</td><td>2020-11-19</td><td>02:24:41.947</td><td>2020-11-19</td><td>02:44:15.865</td><td>83.9756101018619</td><td>22.244460939543412</td><td>7.96866394897576</td><td>82.03133605102424</td><td>145.00062108158266</td><td>83.6287</td><td>22.0147</td><td>67400681.94708037</td><td>67401855.86515117</td><td>1172.8647966384888</td><td>1173.9180707931519</td><td>1097.3626797944742</td><td>0.9356258990291048</td><td>crab</td><td>POINTING</td><td>1</td><td>LST-1</td><td>LST-1</td></tr>\n",
       "<tr><td>2932</td><td>2020-11-19</td><td>02:46:09.185</td><td>2020-11-19</td><td>03:05:51.191</td><td>83.27280963639575</td><td>21.776677987260904</td><td>7.155731807508118</td><td>82.84426819249188</td><td>188.473005543687</td><td>83.6287</td><td>22.0147</td><td>67401969.18531418</td><td>67403151.19075465</td><td>1172.2196559906006</td><td>1182.0054404735565</td><td>1094.4444604410246</td><td>0.9336513466976025</td><td>crab</td><td>POINTING</td><td>1</td><td>LST-1</td><td>LST-1</td></tr>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<ObservationTable length=5>\n",
       "OBS_ID  DATE-OBS    TIME-OBS    DATE-END  ... OBS_MODE N_TELS TELLIST INSTRUME\n",
       "                                          ...                                 \n",
       "int64   bytes10     bytes12     bytes10   ...  bytes8  int64   bytes5  bytes5 \n",
       "------ ---------- ------------ ---------- ... -------- ------ ------- --------\n",
       "  2914 2020-11-18 03:43:34.194 2020-11-18 ... POINTING      1   LST-1    LST-1\n",
       "  2929 2020-11-19 01:41:13.090 2020-11-19 ... POINTING      1   LST-1    LST-1\n",
       "  2930 2020-11-19 02:02:55.113 2020-11-19 ... POINTING      1   LST-1    LST-1\n",
       "  2931 2020-11-19 02:24:41.947 2020-11-19 ... POINTING      1   LST-1    LST-1\n",
       "  2932 2020-11-19 02:46:09.185 2020-11-19 ... POINTING      1   LST-1    LST-1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------------------------------\n",
    "# Opening all the dl3 data in a path\n",
    "total_data_store_paper = DataStore.from_dir(dl3_dir_paper)\n",
    "\n",
    "# Taking the obs ids\n",
    "obs_ids_paper = total_data_store_paper.obs_table[\"OBS_ID\"].data\n",
    "obs_ids_paper = obs_ids_paper[:]\n",
    "\n",
    "# Then we get the observation information from the total data store\n",
    "observations_paper = total_data_store_paper.get_observations(\n",
    "    obs_ids_paper,\n",
    "    required_irf=[\"aeff\", \"edisp\", \"rad_max\"]\n",
    ")\n",
    "# -------------------------------------------------------\n",
    "# -------------------------------------------------------\n",
    "# Opening all the dl3 data in a path\n",
    "total_data_store   = DataStore.from_dir(dl3_dir)\n",
    "\n",
    "# Taking the obs ids\n",
    "obs_ids = total_data_store.obs_table[\"OBS_ID\"].data\n",
    "obs_ids = obs_ids[:]\n",
    "\n",
    "# Then we get the observation information from the total data store\n",
    "observations = total_data_store.get_observations(\n",
    "    obs_ids,\n",
    "    required_irf=[\"aeff\", \"edisp\", \"rad_max\"]\n",
    ")\n",
    "# -------------------------------------------------------\n",
    "# -------------------------------------------------------\n",
    "# Opening all the dl3 data in a path\n",
    "total_data_store_s = DataStore.from_dir(dl3_dir_s)\n",
    "\n",
    "# Taking the obs ids\n",
    "obs_ids_s = total_data_store_s.obs_table[\"OBS_ID\"].data\n",
    "obs_ids_s = obs_ids_s[:]\n",
    "\n",
    "# Then we get the observation information from the total data store\n",
    "observations_s = total_data_store_s.get_observations(\n",
    "    obs_ids_s,\n",
    "    required_irf=[\"aeff\", \"edisp\", \"rad_max\"]\n",
    ")\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# Defining target position and ON reion\n",
    "target_position = SkyCoord.from_name(target_name, frame='icrs')\n",
    "on_region = PointSkyRegion(target_position)\n",
    "\n",
    "print(f'Total livetime of observations {total_data_store_paper.obs_table[\"LIVETIME\"].data.sum()/3600:.2f} h')\n",
    "display(total_data_store.obs_table[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e287538-e24e-406a-9c51-80fa90658e50",
   "metadata": {},
   "source": [
    "### Defining all the energy axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55dc148f-7dec-4460-ba3f-15be1daf9d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectral fit will be done in energy edges:\n",
      " [ 0.25        0.38876979  0.60456779  0.94015077  1.46200887  2.27353952\n",
      "  3.53553391  5.49803508  8.54987973 13.29573974 20.67592771 32.15270417\n",
      " 50.        ] TeV\n",
      "\n",
      "LC will be estimated from 0.2 TeV to 50.0 TeV\n"
     ]
    }
   ],
   "source": [
    "# ============================ #\n",
    "# estimated energy axes\n",
    "energy_axis = MapAxis.from_energy_bounds(\n",
    "    e_reco_min, e_reco_max, \n",
    "    nbin=e_reco_bin_p_dec, per_decade=True, \n",
    "    unit=\"TeV\", name=\"energy\"\n",
    ")\n",
    "# ============================ #\n",
    "# estimated energy axes\n",
    "energy_axis_true = MapAxis.from_energy_bounds(\n",
    "    e_true_min, e_true_max, \n",
    "    nbin=e_true_bin_p_dec, per_decade=True, \n",
    "    unit=\"TeV\", name=\"energy_true\"\n",
    ")\n",
    "# ============================ #\n",
    "# Energy for the spectrum\n",
    "e_fit_min = energy_axis.edges[0].value\n",
    "e_fit_max = energy_axis.edges[-1].value\n",
    "e_fit_bin_p_dec = e_reco_bin_p_dec\n",
    "\n",
    "# Just to have a separate MapAxis for spectral fit energy range\n",
    "energy_fit_edges = MapAxis.from_energy_bounds(\n",
    "    e_fit_min, e_fit_max, \n",
    "    nbin=e_fit_bin_p_dec, per_decade=True, \n",
    "    unit=\"TeV\"\n",
    ").edges\n",
    "\n",
    "# ============================ #\n",
    "# Energy for the lightcurve\n",
    "e_lc_min = energy_axis.edges[0]\n",
    "e_lc_max = energy_axis.edges[-1]\n",
    "\n",
    "print(\"Spectral fit will be done in energy edges:\\n\", energy_fit_edges)\n",
    "print(f\"\\nLC will be estimated from {e_lc_min:.1f} to {e_lc_max:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35686077-d04f-4885-ba22-c9d739998641",
   "metadata": {},
   "source": [
    "### We define the geometry regions in te sky and prepare the empty datasets and makers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e4b1f96-2ae4-4ed6-8f30-c5d812aa60ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geometry defining the ON region and SpectrumDataset based on it\n",
    "geom = RegionGeom.create(\n",
    "    region=on_region, \n",
    "    axes=[energy_axis]\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# creating an empty dataset\n",
    "dataset_empty_paper = SpectrumDataset.create(\n",
    "    geom=geom, \n",
    "    energy_axis_true=energy_axis_true\n",
    ")\n",
    "dataset_maker_paper = SpectrumDatasetMaker(\n",
    "    containment_correction=False,\n",
    "    selection=[\"counts\", \"exposure\", \"edisp\"]\n",
    ")\n",
    "# -------------------------------------------------------\n",
    "# -------------------------------------------------------\n",
    "# creating an empty dataset\n",
    "dataset_empty = SpectrumDataset.create(\n",
    "    geom=geom, \n",
    "    energy_axis_true=energy_axis_true\n",
    ")\n",
    "dataset_maker = SpectrumDatasetMaker(\n",
    "    containment_correction=False,\n",
    "    selection=[\"counts\", \"exposure\", \"edisp\"]\n",
    ")\n",
    "# -------------------------------------------------------\n",
    "# -------------------------------------------------------\n",
    "dataset_empty_s = SpectrumDataset.create(\n",
    "    geom=geom, \n",
    "    energy_axis_true=energy_axis_true\n",
    ")\n",
    "dataset_maker_s = SpectrumDatasetMaker(\n",
    "    containment_correction=False,\n",
    "    selection=[\"counts\", \"exposure\", \"edisp\"]\n",
    ")\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# tell the background maker to use the WobbleRegionsFinder\n",
    "region_finder = WobbleRegionsFinder(n_off_regions=n_off_regions)\n",
    "bkg_maker = ReflectedRegionsBackgroundMaker(region_finder=region_finder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d22616-6e73-4e0f-a95b-482bee1176c8",
   "metadata": {},
   "source": [
    "### Now we analize the ON and OFF regions in the dataset and we store them in `datasets`, then the datasets can be stacked in a unique one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a23ba7c-9963-4aeb-b833-eccc15664fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpectrumDatasetOnOff\n",
      "--------------------\n",
      "\n",
      "  Name                            : Y8ulGUCx \n",
      "\n",
      "  Total counts                    : 23030 \n",
      "  Total background counts         : 5318.00\n",
      "  Total excess counts             : 17712.00\n",
      "\n",
      "  Predicted counts                : 14174.00\n",
      "  Predicted background counts     : 14174.00\n",
      "  Predicted excess counts         : nan\n",
      "\n",
      "  Exposure min                    : 2.47e+07 m2 s\n",
      "  Exposure max                    : 1.36e+10 m2 s\n",
      "\n",
      "  Number of total bins            : 12 \n",
      "  Number of fit bins              : 12 \n",
      "\n",
      "  Fit statistic type              : wstat\n",
      "  Fit statistic value (-2 log(L)) : 12884.73\n",
      "\n",
      "  Number of models                : 0 \n",
      "  Number of parameters            : 0\n",
      "  Number of free parameters       : 0\n",
      "\n",
      "  Total counts_off                : 5318 \n",
      "  Acceptance                      : 12 \n",
      "  Acceptance off                  : 12 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# -------------------------------------------------------\n",
    "# The final object will be stored as a Datasets object\n",
    "datasets_paper = Datasets()\n",
    "for observation_paper in observations_paper:\n",
    "    dataset_paper = dataset_maker_paper.run(\n",
    "        dataset=dataset_empty_paper.copy(name=str(observation_paper.obs_id)),\n",
    "        observation=observation_paper\n",
    "    )\n",
    "    dataset_on_off_paper = bkg_maker.run(\n",
    "        dataset=dataset_paper, \n",
    "        observation=observation_paper\n",
    "    )\n",
    "    datasets_paper.append(dataset_on_off_paper) \n",
    "\n",
    "# Stacking all the datasets in one\n",
    "stacked_dataset_paper = Datasets(datasets_paper).stack_reduce()\n",
    "print(stacked_dataset_paper)\n",
    "# -------------------------------------------------------\n",
    "# -------------------------------------------------------\n",
    "# The final object will be stored as a Datasets object\n",
    "datasets = Datasets()\n",
    "for observation in observations:\n",
    "    dataset = dataset_maker.run(\n",
    "        dataset=dataset_empty.copy(name=str(observation.obs_id)),\n",
    "        observation=observation\n",
    "    )\n",
    "    dataset_on_off = bkg_maker.run(\n",
    "        dataset=dataset, \n",
    "        observation=observation\n",
    "    )\n",
    "    datasets.append(dataset_on_off) \n",
    "\n",
    "# Stacking all the datasets in one\n",
    "stacked_dataset = Datasets(datasets).stack_reduce()\n",
    "print(stacked_dataset)\n",
    "# -------------------------------------------------------\n",
    "# -------------------------------------------------------\n",
    "# The final object will be stored as a Datasets object\n",
    "datasets_s = Datasets()\n",
    "for observation_s in observations_s:\n",
    "    dataset_s=dataset_maker_s.run(\n",
    "        dataset=dataset_empty_s.copy(name=str(observation_s.obs_id)),\n",
    "        observation=observation_s\n",
    "    )\n",
    "    dataset_on_off_s = bkg_maker.run(\n",
    "        dataset=dataset_s, \n",
    "        observation=observation_s\n",
    "    )\n",
    "    datasets_s.append(dataset_on_off_s)\n",
    "\n",
    "# Stacking all the datasets in one\n",
    "stacked_dataset_s = Datasets(datasets_s).stack_reduce()\n",
    "print(stacked_dataset_s)\n",
    "# -------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8407286a-7e1a-41f9-a2d6-d7d798e5538b",
   "metadata": {},
   "source": [
    "### Then we define the model and set inside the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0bca34-b9e7-43bb-a3ac-56b65fc52bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the model we want to fit and the starting values\n",
    "spectral_model = LogParabolaSpectralModel(\n",
    "    amplitude=1e-12 * u.Unit(\"cm-2 s-1 TeV-1\"),\n",
    "    alpha=2,\n",
    "    beta=0.1,\n",
    "    reference=1 * u.TeV,\n",
    ")\n",
    "# -------------------------------------------------------\n",
    "# we will use the crab model in general\n",
    "model_paper = SkyModel(\n",
    "    spectral_model=spectral_model, \n",
    "    name=\"crab\"\n",
    ")\n",
    "# We set the model of all datasets to log parabola\n",
    "stacked_dataset_paper.models = model_paper\n",
    "# -------------------------------------------------------\n",
    "# -------------------------------------------------------\n",
    "# we will use the crab model in general\n",
    "model = SkyModel(\n",
    "    spectral_model=spectral_model, \n",
    "    name=\"crab\"\n",
    ")\n",
    "# We set the model of all datasets to log parabola\n",
    "stacked_dataset.models = model\n",
    "# -------------------------------------------------------\n",
    "# -------------------------------------------------------\n",
    "# we will use the crab model in general\n",
    "model_s = SkyModel(\n",
    "    spectral_model=spectral_model, \n",
    "    name=\"crab\"\n",
    ")\n",
    "# We set the model of all datasets to log parabola\n",
    "stacked_dataset_s.models = model_s\n",
    "# -------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e5e56b",
   "metadata": {},
   "source": [
    "### We fit the model with the stacked dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695148b7-e285-4fac-a9ff-7094988b47db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# -------------------------------------------------------\n",
    "# Now we run the fit to extract the parameters of the model\n",
    "fit_paper = Fit()\n",
    "result_paper = fit_paper.run(datasets=stacked_dataset_paper)\n",
    "best_fit_model_paper = model_paper.copy()\n",
    "display(stacked_dataset_paper.models.to_parameters_table())\n",
    "# -------------------------------------------------------\n",
    "# -------------------------------------------------------\n",
    "# Now we run the fit to extract the parameters of the model\n",
    "fit = Fit()\n",
    "result = fit.run(datasets=stacked_dataset)\n",
    "best_fit_model = model.copy()\n",
    "display(stacked_dataset.models.to_parameters_table())\n",
    "# -------------------------------------------------------\n",
    "# -------------------------------------------------------\n",
    "# Now we run the fit to extract the parameters of the model\n",
    "fit_s = Fit()\n",
    "result_s = fit_s.run(datasets=stacked_dataset_s)\n",
    "best_fit_model_s = model_s.copy()\n",
    "display(stacked_dataset_s.models.to_parameters_table())\n",
    "# -------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9b97bc",
   "metadata": {},
   "source": [
    "### Then from the model and the data we can extract the flux points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b1fd7e-2228-4ccd-ae99-c0992bf28228",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# then extracting the flux points from the data\n",
    "fpe = FluxPointsEstimator(\n",
    "    energy_edges=energy_fit_edges, \n",
    "    source=target_name, \n",
    "    selection_optional=\"all\"\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# We apply the flux point estiation from the datasets\n",
    "flux_points_paper = fpe.run(datasets=stacked_dataset_paper)\n",
    "display(flux_points_paper.to_table(sed_type=\"dnde\", formatted=True)[:3])\n",
    "# -------------------------------------------------------\n",
    "# -------------------------------------------------------\n",
    "# We apply the flux point estiation from the datasets\n",
    "flux_points = fpe.run(datasets=stacked_dataset)\n",
    "display(flux_points.to_table(sed_type=\"dnde\", formatted=True)[:3])\n",
    "# -------------------------------------------------------\n",
    "# -------------------------------------------------------\n",
    "# We apply the flux point estiation from the datasets\n",
    "flux_points_s = fpe.run(datasets=stacked_dataset_s)\n",
    "display(flux_points_s.to_table(sed_type=\"dnde\", formatted=True)[:3])\n",
    "# -------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51809436-e823-4321-8094-ecdb47e0c9c0",
   "metadata": {},
   "source": [
    "### Then we can plot the SED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1305bcb-6268-45ed-954b-12ceeca5f3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kwargs = {\n",
    "    \"energy_bounds\": [0.08, 100] * u.TeV,\n",
    "    \"sed_type\": \"e2dnde\",\n",
    "    \"yunits\": u.Unit(\"TeV cm-2 s-1\"),\n",
    "    \"xunits\": u.TeV,\n",
    "}\n",
    "cmap = plotting.create_cmap_from_colors([\"w\", \"w\", \"cornflowerblue\"])\n",
    "\n",
    "# Crab models\n",
    "crab_magic_100 = create_crab_spectral_model(\"magic_lp\")\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "best_fit_model_paper.spectral_model.plot_error(\n",
    "    ax=ax, facecolor=\"k\", alpha=0.2, **plot_kwargs,\n",
    ")\n",
    "best_fit_model.spectral_model.plot_error(\n",
    "    ax=ax, facecolor=\"r\", alpha=0.2, **plot_kwargs\n",
    ")\n",
    "best_fit_model_s.spectral_model.plot_error(\n",
    "    ax=ax, facecolor=\"b\", alpha=0.2, **plot_kwargs\n",
    ")\n",
    "best_fit_model_s.spectral_model.plot_error(\n",
    "    ax=ax, facecolor=\"mediumblue\", alpha=0.2, **plot_kwargs\n",
    ")\n",
    "\n",
    "crab_magic_100.plot(\n",
    "    ax=ax, ls=\"--\", lw=1.5, color=\"crimson\", label=\"MAGIC reference (Aleksić et al. 2015)\", **plot_kwargs\n",
    ")\n",
    "\n",
    "flux_points_paper.plot(sed_type=\"e2dnde\", color=\"k\", label=\"Flux points performance paper\")\n",
    "flux_points.plot(sed_type=\"e2dnde\", color=\"r\", label=\"Flux points original data\")\n",
    "flux_points_s.plot(sed_type=\"e2dnde\", color=\"b\", marker=\"o\", label=\"Flux points scaled data\")\n",
    "# flux_points_s.plot_ts_profiles(sed_type=\"e2dnde\", cmap=cmap)\n",
    "\n",
    "ax.legend(loc=3, frameon=False)\n",
    "ax.set_ylim([1e-13, 1e-10])\n",
    "ax.grid(which=\"both\", alpha=0.5)\n",
    "ax.set_ylabel(\"$E^2\\\\frac{dN}{dE}$ [TeVs${}^{-1}$cm${}^{-2}$]\")\n",
    "\n",
    "plt.savefig(f\"plots/total.png\", bbox_inches=\"tight\", dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc25646-b623-44ff-b85a-3bcf11e21a36",
   "metadata": {},
   "source": [
    "### Then once we have found the SED model we fix the alpha and beta parameters and let the amplitude as a free parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a8d703-6c1f-4725-8f09-988867d28070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LC Estimator for each run\n",
    "lc_maker_1d = LightCurveEstimator(\n",
    "    energy_edges=[e_lc_min, e_lc_max], \n",
    "    reoptimize=False, # Re-optimizing other free model parameters (not belonging to the source)\n",
    "    source=\"crab\", \n",
    "    selection_optional=\"all\" # Estimates asymmetric errors, upper limits and fit statistic profiles\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "model_paper.parameters[\"alpha\"].frozen = True\n",
    "model_paper.parameters[\"beta\"].frozen  = True\n",
    "# Assigning the fixed parameters model to each dataset\n",
    "for data in datasets_paper:\n",
    "    data.models = model_paper\n",
    "# -------------------------------------------------------\n",
    "# -------------------------------------------------------\n",
    "model.parameters[\"alpha\"].frozen = True\n",
    "model.parameters[\"beta\"].frozen  = True\n",
    "# Assigning the fixed parameters model to each dataset\n",
    "for data in datasets:\n",
    "    data.models = model\n",
    "# -------------------------------------------------------\n",
    "# -------------------------------------------------------\n",
    "model_s.parameters[\"alpha\"].frozen = True\n",
    "model_s.parameters[\"beta\"].frozen  = True\n",
    "# Assigning the fixed parameters model to each dataset\n",
    "for data in datasets_s:\n",
    "    data.models = model_s\n",
    "# -------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780728b6-381a-41d4-985b-0d1cf391d36d",
   "metadata": {},
   "source": [
    "### Then we run the lightcurve maker run-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c871fec-70be-4b0e-99f0-d41b6c963be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(f\"LC will be estimated from {e_lc_min:.1f} to {e_lc_max:.1f}\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "lc_runwise_paper = lc_maker_1d.run(datasets_paper)\n",
    "lightcurve_paper = lc_runwise_paper.to_table(sed_type=\"flux\", format=\"lightcurve\")\n",
    "# -------------------------------------------------------\n",
    "# -------------------------------------------------------\n",
    "lc_runwise = lc_maker_1d.run(datasets)\n",
    "lightcurve = lc_runwise.to_table(sed_type=\"flux\", format=\"lightcurve\")\n",
    "# -------------------------------------------------------\n",
    "# -------------------------------------------------------\n",
    "lc_runwise_s = lc_maker_1d.run(datasets_s)\n",
    "lightcurve_s = lc_runwise_s.to_table(sed_type=\"flux\", format=\"lightcurve\")\n",
    "# -------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c0309",
   "metadata": {},
   "source": [
    "### We calculate the mean flux and the statistical error\n",
    "### Calculating the means, errors, $\\chi^2$ and $p$ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3c80d0-5ba2-4971-a7a6-ee0c0c016176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(table, sys_error=0):\n",
    "    val = table[\"flux\"]\n",
    "    uncertainty = np.sqrt((sys_error * table[\"flux\"])**2 + table[\"flux_err\"]**2)\n",
    "    return (val/uncertainty**2).sum() / (1/uncertainty**2).sum(), np.sqrt(1/np.sum(1/uncertainty**2))\n",
    "\n",
    "def calculate_chi2_pvalue(table, sys_error=0):\n",
    "    uncertainty = np.sqrt((sys_error * table[\"flux\"])**2 + table[\"flux_err\"]**2)\n",
    "    flux = table[\"flux\"]\n",
    "    mean_flux = (flux/uncertainty**2).sum() / (1/uncertainty**2).sum()\n",
    "    mean_flux_err = np.sqrt(1/np.sum(1/uncertainty**2))\n",
    "    print(f\"Weighted mean flux: {mean_flux:.3e} +/- {mean_flux_err:.3e} cm-2 s-1\")\n",
    "    \n",
    "    chi2_value = np.sum((table[\"flux\"] - mean_flux)**2/uncertainty**2)\n",
    "    ndf = len(table[\"flux\"]) - 1\n",
    "    pvalue = chi2.sf(x=chi2_value, df=ndf)\n",
    "    print(f\"Chi2: {chi2_value:.1f}, ndf: {ndf}, P-value: {pvalue:.2e}\")\n",
    "    return chi2_value, ndf, pvalue\n",
    "\n",
    "# -------------------------------------------------------\n",
    "mean_flux_paper, mean_flux_err_paper    = weighted_average(lightcurve_paper)\n",
    "chi2_val_paper, ndf_paper, pvalue_paper = calculate_chi2_pvalue(lightcurve_paper, sys_error=0.0)\n",
    "# -------------------------------------------------------\n",
    "# -------------------------------------------------------\n",
    "mean_flux,   mean_flux_err   = weighted_average(lightcurve)\n",
    "chi2_val,   ndf,   pvalue   = calculate_chi2_pvalue(lightcurve, sys_error=0.0)\n",
    "# -------------------------------------------------------\n",
    "# -------------------------------------------------------\n",
    "mean_flux_s, mean_flux_err_s = weighted_average(lightcurve_s)\n",
    "chi2_val_s, ndf_s, pvalue_s  = calculate_chi2_pvalue(lightcurve_s, sys_error=0.0)\n",
    "# -------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f74bf8",
   "metadata": {},
   "source": [
    "### Extracting the data from the table as arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db199fb7-3cd2-4e47-9c7b-847732a6eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "time_min_paper = Time(np.hstack(lightcurve_paper[\"time_min\"]), format='mjd').datetime\n",
    "time_max_paper = Time(np.hstack(lightcurve_paper[\"time_max\"]), format='mjd').datetime\n",
    "delta_time_paper  = time_max_paper - time_min_paper\n",
    "time_center_paper = time_min_paper + delta_time_paper / 2\n",
    "# Flux and flux error\n",
    "flux_lst1_paper = np.hstack(lightcurve_paper[\"flux\"])\n",
    "flux_stat_err_lst1_paper = np.hstack(lightcurve_paper[\"flux_err\"])\n",
    "# run numbers\n",
    "run_num_paper = [int(n) for n in observations_paper.ids]\n",
    "# -------------------------------------------------------\n",
    "# -------------------------------------------------------\n",
    "time_min = Time(np.hstack(lightcurve[\"time_min\"]), format='mjd').datetime\n",
    "time_max = Time(np.hstack(lightcurve[\"time_max\"]), format='mjd').datetime\n",
    "delta_time  = time_max - time_min\n",
    "time_center = time_min + delta_time / 2\n",
    "# Flux and flux error\n",
    "flux_lst1 = np.hstack(lightcurve[\"flux\"])\n",
    "flux_stat_err_lst1 = np.hstack(lightcurve[\"flux_err\"])\n",
    "# run numbers\n",
    "run_num = [int(n) for n in observations.ids]\n",
    "# -------------------------------------------------------\n",
    "# -------------------------------------------------------\n",
    "time_min_s = Time(np.hstack(lightcurve_s[\"time_min\"]), format='mjd').datetime\n",
    "time_max_s = Time(np.hstack(lightcurve_s[\"time_max\"]), format='mjd').datetime\n",
    "delta_time_s  = time_max_s - time_min_s\n",
    "time_center_s = time_min_s + delta_time_s / 2\n",
    "# Flux and flux error\n",
    "flux_lst1_s = np.hstack(lightcurve_s[\"flux\"])\n",
    "flux_stat_err_lst1_s = np.hstack(lightcurve_s[\"flux_err\"])\n",
    "# run numbers\n",
    "run_num_s = [int(n) for n in observations_s.ids]\n",
    "# -------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497a6f3f",
   "metadata": {},
   "source": [
    "### The Crab Nebula reference from MAGIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11909347-a4dc-42ed-802e-4b0a22665232",
   "metadata": {},
   "outputs": [],
   "source": [
    "crab = create_crab_spectral_model(\"magic_lp\")\n",
    "\n",
    "crab.amplitude.error = 0.03e-11 * u.Unit(\"cm-2 s-1 TeV-1\")\n",
    "crab.alpha.error = 0.01\n",
    "crab.beta.error = 0.01/np.log(10)\n",
    "\n",
    "flux_crab = crab.integral(e_lc_min, e_lc_max)\n",
    "flux_crab_error = flux_crab * 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85290b3",
   "metadata": {},
   "source": [
    "### Plotting the LC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cacabbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "# Plotting the Ligt Curve\n",
    "ax.errorbar(time_center_paper, flux_lst1_paper, yerr=flux_stat_err_lst1_paper, color=\"k\", ls=\"\", marker=\".\", ms=10, label=\"LST-1 (src-independent) original\")\n",
    "ax.errorbar(time_center, flux_lst1, yerr=flux_stat_err_lst1, color=\"r\", ls=\"\", marker=\"x\", ms=10, label=\"LST-1 (src-independent) scaled\")\n",
    "ax.errorbar(time_center_s, flux_lst1_s, yerr=flux_stat_err_lst1_s, color=\"b\", ls=\"\", marker=\".\", ms=10, label=\"LST-1 (src-independent) scaled\")\n",
    "    \n",
    "# Mean flux + error\n",
    "ax.axhline(mean_flux_paper, ls=\"--\", color=\"k\", zorder=-1, label=\"Mean flux\")\n",
    "ax.axhspan(mean_flux_paper - mean_flux_err_paper, mean_flux_paper + mean_flux_err_paper, color=\"k\", alpha=0.4, zorder=-1)\n",
    "ax.axhline(mean_flux, ls=\"--\", color=\"r\", zorder=-1, label=\"Mean flux\")\n",
    "ax.axhspan(mean_flux - mean_flux_err, mean_flux + mean_flux_err, color=\"r\", alpha=0.4, zorder=-1)\n",
    "ax.axhline(mean_flux_s, ls=\"--\", color=\"b\", zorder=-1, label=\"Mean flux\")\n",
    "ax.axhspan(mean_flux_s - mean_flux_err_s, mean_flux_s + mean_flux_err_s, color=\"b\", alpha=0.4, zorder=-1)\n",
    "\n",
    "# MAGIC reference\n",
    "ax.axhline(flux_crab.value, ls=\"-.\", color=\"crimson\", zorder=-1, label=\"MAGIC (Aleksić et al. 2015)\")\n",
    "\n",
    "energy_range = f\"{e_lc_min:.2f} < $E$ < {e_lc_max:.0f}\"\n",
    "ax.set_title(f\"Light curve of {target_name} ({energy_range})\")\n",
    "ax.set_ylabel(\"Flux [cm$^{-2}$ s$^{-1}$]\")\n",
    "ax.set_xlabel(\"Time [date]\")\n",
    "# ax.grid()\n",
    "ax.legend(loc=4)\n",
    "# ax.set_xlim(18876, 18882.5)\n",
    "ax.xaxis.set_major_locator(MonthLocator(interval=2))\n",
    "ax.xaxis.set_major_formatter(DateFormatter(\"%d-%m-%Y\"))\n",
    "plt.xticks(rotation=45)\n",
    "ax.set_ylim(0)\n",
    "\n",
    "# plt.savefig(f\"plots/total.png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbab3e8f",
   "metadata": {},
   "source": [
    "### Plotting the LC run-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677ed1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(17,3))\n",
    "\n",
    "# Plotting the Ligt Curve\n",
    "ax.errorbar(np.arange(len(flux_lst1_paper)), flux_lst1_paper, yerr=flux_stat_err_lst1_paper, color=\"k\", ls=\"\", marker=\".\", ms=10, label=\"LST-1 (src-independent)\")  \n",
    "ax.errorbar(np.arange(len(flux_lst1)), flux_lst1, yerr=flux_stat_err_lst1, color=\"r\", ls=\"\", marker=\".\", ms=10, label=\"LST-1 (src-independent)\")  \n",
    "ax.errorbar(np.arange(len(flux_lst1_s)), flux_lst1_s, yerr=flux_stat_err_lst1_s, color=\"b\", ls=\"\", marker=\".\", ms=10, label=\"LST-1 (src-independent)\")  \n",
    "\n",
    "# Mean flux + error\n",
    "ax.axhline(mean_flux_paper, ls=\"--\", color=\"k\", zorder=-1, label=\"Mean flux\")\n",
    "ax.axhspan(mean_flux_paper - mean_flux_err_paper, mean_flux_paper + mean_flux_err_paper, color=\"k\", alpha=0.4, zorder=-1)\n",
    "ax.axhline(mean_flux, ls=\"--\", color=\"r\", zorder=-1, label=\"Mean flux\")\n",
    "ax.axhspan(mean_flux - mean_flux_err, mean_flux + mean_flux_err, color=\"r\", alpha=0.4, zorder=-1)\n",
    "ax.axhline(mean_flux_s, ls=\"--\", color=\"b\", zorder=-1, label=\"Mean flux\")\n",
    "ax.axhspan(mean_flux_s - mean_flux_err_s, mean_flux_s + mean_flux_err_s, color=\"b\", alpha=0.4, zorder=-1)\n",
    "# MAGIC reference\n",
    "ax.axhline(flux_crab.value, ls=\"-.\", color=\"crimson\", zorder=-1, label=\"MAGIC (Aleksić et al. 2015)\")\n",
    "\n",
    "# ax.legend()\n",
    "ax.grid()\n",
    "ax.set_xlabel(f\"Run #\")\n",
    "ax.set_ylabel(\"Flux [cm$^{-2}$ s$^{-1}$]\")\n",
    "energy_range = f\"{e_lc_min:.1f} < $E$ < {e_lc_max:.1f}\"\n",
    "ax.set_title(f\"Light curve of {target_name} ({energy_range})\")\n",
    "# plt.savefig(f\"plots/total.png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdd5aad-e07a-4d15-8ac9-cc39bb1499ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc63625e-578a-476c-aeda-4ce02277f9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(flux_lst1_paper), np.std(flux_lst1), np.std(flux_lst1_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29e48d9",
   "metadata": {},
   "source": [
    "### Now we put all inside a dict and we store it in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea9780",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_total = {\n",
    "    \n",
    "    \"dict_model\" : best_fit_model.to_dict(), # SkyModel.from_dict(<>)\n",
    "    \n",
    "    \"table_sed\"  : flux_points.to_table(),   # FluxPoints.from_table(<>)\n",
    "\n",
    "    \"lightcurve\" : {\n",
    "\n",
    "        \"run_number\" : run_num,\n",
    "        \"t_start\"    : time_min,\n",
    "        \"t_stop\"     : time_max,\n",
    "        \"timedelta\"  : delta_time,\n",
    "        \"flux\"       : flux_lst1,\n",
    "        \"e_flux\"     : flux_stat_err_lst1,\n",
    "        \n",
    "        \"global\" : {\n",
    "            \"e_min\"               : e_lc_min,\n",
    "            \"e_max\"               : e_lc_max,\n",
    "            \"n_off_regions\"       : n_off_regions,\n",
    "            \"target_name\"         : target_name,\n",
    "            \"crab_reference_flux\" : flux_crab,\n",
    "            \"chi2\"                : chi2_val,\n",
    "            \"pvalue\"              : pvalue\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Saving the object\n",
    "# with open(fname_dict, 'wb') as f:\n",
    "#     pickle.dump(dict_total, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6be7a9-5a07-4aa6-b366-48750e259a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664f73a2-3fa9-4cbc-94dd-a4699021ccfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
